{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-30 16:09:44,844 : INFO : WTI index loaded!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sys.path = list(set(['..', '../3_text_analysis']) - set(sys.path)) + sys.path\n",
    "\n",
    "#import bokeh, bokeh.plotting, bokeh.models, \n",
    "import matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import common.treaty_utility as treaty_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import treaty_corpus\n",
    "#import types, glob\n",
    "import textacy.keyterms\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER = '../data'\n",
    "PATTERN = '*.txt'\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "TREATY_TIME_GROUPINGS = WTI_INDEX.get_treaty_time_groupings()\n",
    "\n",
    "%matplotlib inline\n",
    "# set_matplotlib_formats('svg')   \n",
    "#bokeh.plotting.output_notebook()\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c94a1f4dc134780b5eb85bb4eef2489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(Dropdown(description='C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, WTI_INDEX, container)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>DESCRIBE</span> List of Most Frequent Words<span style='color: blue; float: right'>OPTIONAL</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bde17cd2364806b89718b30c441d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, layout=Layout(width='98%'), max=5), HBox(children=(VBox(children=(Dropdown…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "import collections\n",
    "from spacy import attrs\n",
    "\n",
    "def textacy_doc_to_bow(doc, target='lemma', weighting='count', as_strings=False, include=None):\n",
    "\n",
    "    spacy_store = doc.vocab.strings\n",
    "    \n",
    "    weighing_keys = { 'count', 'freq' }\n",
    "    target_keys = { 'lemma': attrs.LEMMA, 'lower': attrs.LOWER, 'orth': attrs.ORTH }\n",
    "    \n",
    "    default_exclude = lambda x: x.is_stop or x.is_punct or x.is_space\n",
    "    exclude = default_exclude if include is None else lambda x: x.is_stop or x.is_punct or x.is_space or not include(x)\n",
    "    \n",
    "    assert weighting in weighing_keys\n",
    "    assert target in target_keys\n",
    "\n",
    "    target_weights = doc.count_by(target_keys[target], exclude=exclude)\n",
    "    n_tokens = doc._.n_tokens\n",
    "\n",
    "    if weighting == 'count':\n",
    "        n_tokens = 1\n",
    "        \n",
    "    if as_strings:\n",
    "        bow = {\n",
    "            spacy_store[word_id]: count / n_tokens for word_id, count in target_weights.items()\n",
    "        }\n",
    "        if target == 'lemma':\n",
    "            lower_cased_word_counts = collections.Counter()\n",
    "            for k, v in bow.items():\n",
    "                lower_cased_word_counts.update({ k.lower(): v })\n",
    "            bow = lower_cased_word_counts\n",
    "    else:\n",
    "        bow = target_weights\n",
    "        \n",
    "    return bow\n",
    "\n",
    "def compute_list_of_most_frequent_words(\n",
    "    corpus,\n",
    "    gui,\n",
    "    group_by_column='signed_year',\n",
    "    parties=None,\n",
    "    target='lemma',\n",
    "    weighting='count',\n",
    "    include_pos=None,\n",
    "    stop_words=None,\n",
    "    display_score=False\n",
    "):\n",
    "    stop_words = stop_words or set()\n",
    "    \n",
    "    def include(token):\n",
    "        flag = True\n",
    "        if not include_pos is None:\n",
    "             flag = flag and token.pos_ in include_pos\n",
    "        flag = flag and token.lemma_ not in stop_words\n",
    "        return flag\n",
    "    \n",
    "    gui.progress.max = len(corpus)\n",
    "    \n",
    "    df_freqs = pd.DataFrame({ 'treaty_id': [], 'signed_year': [], 'token': [], 'score': [] })\n",
    "    \n",
    "    parties_set = set(parties or [])\n",
    "    \n",
    "    docs = corpus if len(parties_set) == 0 \\\n",
    "        else ( x for x in corpus if len(set((x._.meta['party1'], x._.meta['party2'])) & parties_set) > 0 )\n",
    "                                                   \n",
    "    for doc in docs:\n",
    "        \n",
    "        doc_freqs = textacy_doc_to_bow(doc, target=target, weighting=weighting, as_strings=True, include=include)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'treaty_id': doc._.meta['treaty_id'],\n",
    "            'signed_year': int(doc._.meta['signed_year']),\n",
    "            'token': list(doc_freqs.keys()),\n",
    "            'score': list(doc_freqs.values())\n",
    "        })\n",
    "        \n",
    "        # print('Added {}: {} words'.format(doc._.meta['treaty_id'], len(df)))\n",
    "        \n",
    "        df_freqs = df_freqs.append(df)\n",
    "        gui.progress.value = gui.progress.value + 1\n",
    "        \n",
    "    df_freqs['signed_year'] = df_freqs.signed_year.astype(int)\n",
    "    \n",
    "    for key, group in TREATY_TIME_GROUPINGS.items():\n",
    "        if key in df_freqs.columns:\n",
    "            continue\n",
    "        df_freqs[key] = (group['fx'])(df_freqs)\n",
    "        \n",
    "    df_freqs['term'] = df_freqs.token # if True else df_freqs.token\n",
    "    \n",
    "    df_freqs = df_freqs.groupby([group_by_column, 'term']).sum().reset_index()[[group_by_column, 'term', 'score']]\n",
    "    \n",
    "    if display_score is True:\n",
    "        df_freqs['term'] = df_freqs.term + '*' + (df_freqs.score.apply('{:,.3f}'.format) if weighting == 'freq' else df_freqs.score.astype(str))\n",
    "        \n",
    "    df_freqs['position'] = df_freqs.sort_values(by=[group_by_column, 'score'], ascending=False).groupby([group_by_column]).cumcount() + 1\n",
    "    \n",
    "    gui.progress.value = 0\n",
    "    \n",
    "    return df_freqs\n",
    "    \n",
    "def display_list_of_most_frequent_words(gui, df):\n",
    "    if gui.output_type.value == 'table':\n",
    "        display(df)\n",
    "    elif gui.output_type.value == 'rank':\n",
    "        group_by_column = gui.group_by_column.value\n",
    "        df = df[df.position <= gui.n_tokens.value]\n",
    "        df_unstacked_freqs = df[[group_by_column, 'position', 'term']].set_index([group_by_column, 'position']).unstack()\n",
    "        display(df_unstacked_freqs)\n",
    "    else:\n",
    "        filename = '../data/word_trend_data.xlsx'\n",
    "        df.to_excel(filename)\n",
    "        print('Excel written: ' + filename)\n",
    "        \n",
    "def word_frequency_gui(wti_index, corpus, compute_callback, display_callback):\n",
    "    \n",
    "    lw = lambda w: widgets.Layout(width=w)\n",
    "    \n",
    "    include_pos_tags = [ 'ADJ', 'VERB', 'NUM', 'ADV', 'NOUN', 'PROPN' ]\n",
    "    weighting_options = { 'Count': 'count', 'Frequency': 'freq' }\n",
    "    normalize_options = { '':  False, 'Lemma': 'lemma', 'Lower': 'lower' }\n",
    "    #pos_tags = DF_TAGSET[DF_TAGSET.POS.isin(include_pos_tags)].groupby(['POS'])['DESCRIPTION'].apply(list).apply(lambda x: ', '.join(x[:1])).to_dict()\n",
    "    #pos_options = { k + ' (' + v + ')': k for k,v in pos_tags.items() }\n",
    "    pos_options = include_pos_tags\n",
    "    \n",
    "    default_include_pos = ['NOUN', 'PROPN']\n",
    "    frequent_words = [ x[0] for x in textacy_utility.get_most_frequent_words(corpus, 100, include_pos=default_include_pos) ]\n",
    "\n",
    "    group_by_options = { TREATY_TIME_GROUPINGS[k]['title']: k for k in TREATY_TIME_GROUPINGS }\n",
    "    output_type_options = [ ( 'List', 'table' ), ( 'Rank', 'rank' ), ( 'Excel', 'excel' ), ]\n",
    "    ngrams_options = { '-': None, '1': [1], '1,2': [1,2], '1,2,3': [1,2,3]}\n",
    "    party_preset_options = wti_index.get_party_preset_options()\n",
    "    parties_options = [ x for x in wti_index.get_countries_list() if x != 'ALL OTHER' ]\n",
    "    gui = types.SimpleNamespace(\n",
    "        progress=widgets.IntProgress(value=0, min=0, max=5, step=1, description='', layout=lw('98%')),\n",
    "        parties=widgets.SelectMultiple(description='Parties', options=parties_options, value=[], rows=7, layout=lw('200px')),\n",
    "        party_preset=widgets_config.dropdown('Presets', party_preset_options, None, layout=lw('200px')),\n",
    "        ngrams=widgets.Dropdown(description='n-grams', options=ngrams_options, value=None, layout=lw('200px')),\n",
    "        min_word=widgets.Dropdown(description='Min length', options=[1,2,3,4], value=1, layout=lw('200px')),\n",
    "        normalize=widgets.Dropdown(description='Normalize', options=normalize_options, value='lemma', layout=lw('200px')),\n",
    "        weighting=widgets.Dropdown(description='Weighting', options=weighting_options, value='freq', layout=lw('200px')),\n",
    "        include_pos=widgets.SelectMultiple(description='POS', options=pos_options, value=default_include_pos, rows=7, layout=lw('150px')),\n",
    "        stop_words=widgets.SelectMultiple(description='STOP', options=frequent_words, value=list([]), rows=7, layout=lw('200px')),\n",
    "        group_by_column=widgets.Dropdown(description='Group by', value='signed_year', options=group_by_options, layout=lw('200px')),\n",
    "        output_type=widgets.Dropdown(description='Output', value='rank', options=output_type_options, layout=lw('200px')),\n",
    "        n_tokens=widgets.IntSlider(description='#tokens', value=25, min=3, max=500, layout=lw('250px')),\n",
    "        compute=widgets.Button(description='Compute', button_style='Success', layout=lw('120px')),\n",
    "        display_score=widgets.ToggleButton(description='Display score', icon='check', value=False, layout=lw('120px')),\n",
    "        output=widgets.Output(layout={'border': '1px solid black'})\n",
    "    )\n",
    "    \n",
    "    boxes = widgets.VBox([\n",
    "        gui.progress,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.normalize,\n",
    "                gui.ngrams,\n",
    "                gui.weighting,\n",
    "                gui.group_by_column,\n",
    "                gui.output_type,\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.parties,\n",
    "                gui.party_preset,\n",
    "            ]),\n",
    "            gui.include_pos,\n",
    "            gui.stop_words,\n",
    "            widgets.VBox([\n",
    "                gui.n_tokens,\n",
    "                gui.display_score,\n",
    "                gui.compute,\n",
    "            ], layout=widgets.Layout(align_items='flex-end')),\n",
    "        ]),\n",
    "        gui.output\n",
    "    ])\n",
    "    \n",
    "    display(boxes)\n",
    "    \n",
    "    def on_party_preset_change(change):  # pylint: disable=W0613\n",
    "        if gui.party_preset.value is None:\n",
    "            return\n",
    "        gui.parties.value = gui.parties.options if 'ALL' in gui.party_preset.value else gui.party_preset.value\n",
    "            \n",
    "    gui.party_preset.observe(on_party_preset_change, names='value')\n",
    "    \n",
    "    def pos_change_handler(*args):\n",
    "        with gui.output:\n",
    "            gui.compute.disabled = True\n",
    "            selected = set(gui.stop_words.value)\n",
    "            frequent_words = [\n",
    "                x[0] for x in textacy_utility.get_most_frequent_words(\n",
    "                    corpus,\n",
    "                    100,\n",
    "                    normalize=gui.normalize.value,\n",
    "                    include_pos=gui.include_pos.value,\n",
    "                    weighting=gui.weighting.value\n",
    "                )\n",
    "            ]\n",
    "            gui.stop_words.options = frequent_words\n",
    "            selected = selected & set(gui.stop_words.options)\n",
    "            gui.stop_words.value = list(selected)\n",
    "            gui.compute.disabled = False\n",
    "        \n",
    "    gui.include_pos.observe(pos_change_handler, 'value')    \n",
    "    gui.weighting.observe(pos_change_handler, 'value')    \n",
    "    \n",
    "    def compute_callback_handler(*_args):\n",
    "        gui.output.clear_output()\n",
    "        with gui.output:\n",
    "            #try:\n",
    "                gui.compute.disabled = True\n",
    "                df_freqs = compute_callback(\n",
    "                    corpus=corpus,\n",
    "                    gui=gui,\n",
    "                    target=gui.normalize.value,\n",
    "                    group_by_column=gui.group_by_column.value,\n",
    "                    parties=gui.parties.value,\n",
    "                    weighting=gui.weighting.value,\n",
    "                    include_pos=gui.include_pos.value,\n",
    "                    stop_words=set(gui.stop_words.value),\n",
    "                    display_score=gui.display_score.value\n",
    "                )\n",
    "                display_callback(gui, df_freqs)\n",
    "            #finally:\n",
    "            #    gui.compute.disabled = False\n",
    "\n",
    "    gui.compute.on_click(compute_callback_handler)\n",
    "    return gui\n",
    "                \n",
    "try:\n",
    "    word_frequency_gui(\n",
    "        WTI_INDEX,\n",
    "        current_corpus(),\n",
    "        compute_callback=compute_list_of_most_frequent_words,\n",
    "        display_callback=display_list_of_most_frequent_words\n",
    "    )\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n",
    "    raise\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
