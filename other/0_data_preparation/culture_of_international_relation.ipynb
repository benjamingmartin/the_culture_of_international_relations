{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:red'>THIS NOTEBOOK IS DEPRECATED!</span>\n",
    "\n",
    "### The Culture of International Relations\n",
    "\n",
    "#### About this project\n",
    "Cultural treaties are the bi-lateral and multilateral agreements among states that promote and regulate cooperation and exchange in the fields of life generally call cultural or intellectual. Although it was only invented in the early twentieth century, this treaty type came to be the fourth most common bilateral treaty in the period 1900-1980 (Poast et al., 2010). In this project, we seek to use several (mostly European) states’ cultural treaties as a historical source with which to explore the emergence of a global concept of culture in the twentieth century. Specifically, the project will investigate the hypothesis that the culture concept, in contrast to earlier ideas of civilization, played a key role in the consolidation of the post-World War II international order.\n",
    "\n",
    "The central questions that interest me here can be divided into two groups: \n",
    "- First, what is the story of the cultural treaty, as a specific tool of international relations, in the twentieth century? What was the historical curve of cultural treaty-making? For example, in which political or ideological constellations do we find (the most) use of cultural treaties? Among which countries, in which historical periods? What networks of relations were thereby created, reinforced, or challenged? \n",
    "- Second, what is the \"culture\" addressed in these treaties? That is, what do the two signatories seem to mean by \"culture\" in these documents, and what does that tell us about the role that concept played in the international system? How can quantitative work on this dataset advance research questions about the history of concepts?\n",
    "\n",
    "In this notebook, we deal with these treaties in three ways:\n",
    "1) quantitative analysis of \"metadata\" about all bilateral cultural treaties signed betweeen 1919 and 1972, as found in the World Treaty Index or WTI (Poast et al., 2010).\n",
    "    For more on how exactly we define a \"cultural treaty\" here, and on other principles of selection, see... [add this, using text now in \"WTI quality assurance\"].\n",
    "2) network analysis of the system of international relationships created by these treaties (using data from WTI, as above).\n",
    "3) Text analysis of the complete texts of selected treaties. \n",
    "\n",
    "After some set-up sections, the discussion of the material begins at \"Part 1,\" below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Instructions on Jupyter Notebooks\n",
    "Please see [this tutorial](https://www.youtube.com/watch?v=h9S4kN4l5Is) for an introduction on what Jupyter notebooks are and how to use them. There are lots of other Jupyter tutorials on YouTube (and elsewhere) as well. In short, a notebook is a document with embedded executable code presented in a simple and easy to use web interface. Most important things to note are:\n",
    "- Click on the menu Help -> User Interface Tour for an overview of the Jupyter Notebook App user interface.\n",
    "- The **code cells** contains the script code (Python in this case, but can be other languages are also suported) and are the sections marked by **In [x]** in the left margin. It is marked as **In []** if it hasn't been executed, and as **In [n]** when it has been executed(n is an integer). A cell marked as **In [\\*]** is either executing, or waiting to be executed (i.e. other cells are executing).\n",
    "- The **current cell** is highlighted with a blue (or green if in \"edit\" mode) border. You make a cell current by clicking on it,\n",
    "- Code cells aren't executed automatically. Instead you execute the current cell by either pressing **shift+enter** or the **play** button in the toolbar. The output (or result) of a cell's execution is presented directly below the cell prefixed by **Out[n]**.\n",
    "- The next cell will automatically be selected (made current) after a cell has been executed. Repeatadly pressing **shift+enter** or the play button hence executes the cells in sequence.\n",
    "- You can run the entire notebook in a single step by clicking on the menu Cell -> Run All. Note that this can take some time to finish. You can see how cells are executed in sequence via the indicator in the margin (i.e. \"In [\\*]\" changes to \"In [n]\" where n is an integer).\n",
    "- The cells can be edited if they are double-clicked, in which case the cell border turns green. Use the ESC key to escape edit mode (or click on any other cell).\n",
    "\n",
    "To restart the kernel (i.e. the computational engine assigned to your session), click on the menu Kernel -> Restart. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:green'>**Optional Prepare Step**</span>: Update WTI data from Google Drive\n",
    "The statistics computed on this page is dependent on a recent verison of the WTI treaties master list. This file is stored on Google Drive, and the script \"./google_drive.py\" can be used to download and update the data. Please note that the load script below reads CSV-files, with specific names, so a manual download of the master list must be followed by saving each sheet as an CSV. The script ./google_drive.py does this automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e24726ed1c749b882bfeccfcbcee1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='File:', options={'WTI Master Index': {'sheets': ['Treaties…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code: Update WTI master data from Google Drive\n",
    "%run ./common/google_drive\n",
    "%run ./common/widgets_utility\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "files_to_download = {\n",
    "    'WTI Master Index': {\n",
    "        'file_id': '1V8KPeghLQ2iOMWkbPqff480zDSLa5YDX',\n",
    "        'destination': './data/Treaties_Master_List.xlsx',\n",
    "        'sheets': [ 'Treaties' ]\n",
    "    },\n",
    "    'Curated Parties': {\n",
    "        'file_id': '1k4dOPuqR7oi4K8SazoGN6R40jOBWOdWp',\n",
    "        'destination': './data/parties_curated.xlsx',\n",
    "        'sheets': ['parties', 'group', 'continent']\n",
    "    },\n",
    "    'Country & Continent': {\n",
    "        'file_id': '19lEmVPu7hNmr1MaMpU0VvKL7muu-OKg9',\n",
    "        'destination': './data/country_continent.csv',\n",
    "        'sheets': [ ]\n",
    "    }\n",
    "}\n",
    "\n",
    "def update_file(file, confirm):\n",
    "    global upw\n",
    "    if file is None:\n",
    "        return\n",
    "    if confirm is False:\n",
    "        print('Please confirm update by checking the CONFIRM button!')\n",
    "        return\n",
    "    upw.confirm.value = False\n",
    "    print('Updatating Google file with ID: {}'.format(file['file_id']))\n",
    "    process_file(file, overwrite=confirm)\n",
    "    \n",
    "upw = BaseWidgetUtility(\n",
    "    file=widgets.Dropdown(\n",
    "        options=files_to_download,\n",
    "        value=None,\n",
    "        description='File:',\n",
    "    ),\n",
    "    confirm=widgets.ToggleButton(\n",
    "        description='Confirm',\n",
    "        button_style='',\n",
    "        icon='check',\n",
    "        value=False\n",
    "    ),)\n",
    "iupw = widgets.interactive(update_file, file=upw.file, confirm=upw.confirm)\n",
    "display(widgets.VBox([widgets.HBox([upw.file, upw.confirm]), iupw.children[-1]]))\n",
    "# iupw.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".jupyter-widgets {\n",
       "    font-size: 8pt;\n",
       "}\n",
       ".widget-label {\n",
       "    font-size: 8pt;\n",
       "}\n",
       ".widget-dropdown > select {\n",
       "    font-size: 8pt;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".jupyter-widgets {\n",
    "    font-size: 8pt;\n",
    "}\n",
    ".widget-label {\n",
    "    font-size: 8pt;\n",
    "}\n",
    ".widget-dropdown > select {\n",
    "    font-size: 8pt;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>**Mandatory Prepare Step**</span>: Setup Notebook\n",
    "The following code cell must to be executed once for each user session. The step loads utility Python code stored in separate files, and imports dependencies to external libraries. The following external libraries are used:\n",
    "<table>\n",
    "    <tr><td>[NLTK](https://www.nltk.org/)</td><td>NLP framework. *Natural Language Toolkit*</td><td>*Bird, Steven, Edward Loper and Ewan Klein (2009),<br/>Natural Language Processing with Python.<br/>O’Reilly Media Inc.*</td><td></td></tr>\n",
    "    <tr><td> [gensim](https://radimrehurek.com/gensim/index.html)</td><td>NLP framework. *Topic Modelling for Humans*</td><td>[Google scholar](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9vG_kV0AAAAJ&citation_for_view=9vG_kV0AAAAJ:NaGl4SEjCO4C)</td><td></td></tr>\n",
    "    <tr><td>[python_louvain](https://github.com/taynaud/python-louvain)</td><td>Louvain Community Detection</td><td> https://python-louvain.readthedocs.io/</td><td></td></tr>\n",
    "    <tr><td>[wordcloud](https://github.com/amueller/word_cloud)</td><td>Wordcloud generator</td><td>https://github.com/amueller/word_cloud</td><td></td></tr>\n",
    "    <tr><td>[Graphviz](https://www.graphviz.org/)</td><td>Graphviz is an open source graph visualization software.</td><td>https://www.graphviz.org/</td><td></td></tr>\n",
    "    <tr><td>[bokeh](http://bokeh.pydata.org/en/latest/)</td><td>Bokeh is an interactive visualization library.</td><td>http://bokeh.pydata.org/en/latest/</td><td></td></tr>\n",
    "    <tr><td>[pandas](https://pandas.pydata.org/)</td><td>Data structures and data analysis tools</td><td>https://pandas.pydata.org/</td><td></td></tr>\n",
    "    <tr><td>[NetworkX](https://networkx.github.io/)</td><td>Package for the creation, manipulation, and study of complex networks.</td><td>https://networkx.github.io/</td><td></td></tr>\n",
    "    <tr><td>[graph_tool](https://graph-tool.skewed.de/)</td><td>Module for manipulation and statistical analysis of graphs networks.</td><td>https://graph-tool.skewed.de/</td><td></td></tr>\n",
    "    <tr><td>[PyTables](http://www.pytables.org/)</td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>[scipy](http://www.scipy.org/), [numpy](http://www.numpy.org/)</td><td></td><td></td><td></td></tr>\n",
    "</table>\n",
    "\n",
    "Pandas, bokeh, Jupyter, numpy and PyTables are all sponsored by [NumFOCUS](https://www.numfocus.org/sponsored-projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"227c33b3-7f79-46d8-ada0-e49795fa9dbb\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"227c33b3-7f79-46d8-ada0-e49795fa9dbb\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"227c33b3-7f79-46d8-ada0-e49795fa9dbb\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '227c33b3-7f79-46d8-ada0-e49795fa9dbb' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"227c33b3-7f79-46d8-ada0-e49795fa9dbb\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"227c33b3-7f79-46d8-ada0-e49795fa9dbb\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"227c33b3-7f79-46d8-ada0-e49795fa9dbb\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '227c33b3-7f79-46d8-ada0-e49795fa9dbb' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"227c33b3-7f79-46d8-ada0-e49795fa9dbb\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup\n",
    "%run ./common/file_utility\n",
    "%run ./common/network_utility\n",
    "%run ./common/widgets_utility\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import logging\n",
    "import fnmatch\n",
    "import datetime\n",
    "import wordcloud\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import bokeh.plotting as bp\n",
    "import bokeh.palettes\n",
    "import bokeh.models as bm\n",
    "import bokeh.io\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import zipfile\n",
    "import nltk.tokenize\n",
    "import nltk.corpus\n",
    "import gensim.models\n",
    "\n",
    "from pivottablejs import pivot_ui\n",
    "from math import sqrt\n",
    "from bokeh.io import push_notebook\n",
    "from gensim.corpora.textcorpus import TextCorpus\n",
    "\n",
    "from IPython.display import display, HTML #, clear_output, IFrame\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.ERROR)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,hover,previewsave\"\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "warnings.filterwarnings('ignore')\n",
    "bp.output_notebook()\n",
    "\n",
    "%run ./common/utility\n",
    "#%autosave 120\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "matplotlib_plot_styles =[\n",
    "    'ggplot',\n",
    "    'bmh',\n",
    "    'seaborn-notebook',\n",
    "    'seaborn-whitegrid',\n",
    "    '_classic_test',\n",
    "    'seaborn',\n",
    "    'fivethirtyeight',\n",
    "    'seaborn-white',\n",
    "    'seaborn-dark',\n",
    "    'seaborn-talk',\n",
    "    'seaborn-colorblind',\n",
    "    'seaborn-ticks',\n",
    "    'seaborn-poster',\n",
    "    'seaborn-pastel',\n",
    "    'fast',\n",
    "    'seaborn-darkgrid',\n",
    "    'seaborn-bright',\n",
    "    'Solarize_Light2',\n",
    "    'seaborn-dark-palette',\n",
    "    'grayscale',\n",
    "    'seaborn-muted',\n",
    "    'dark_background',\n",
    "    'seaborn-deep',\n",
    "    'seaborn-paper',\n",
    "    'classic'\n",
    "]\n",
    "\n",
    "output_formats = {\n",
    "    'Plot vertical bar': 'plot_bar',\n",
    "    'Plot horisontal bar': 'plot_barh',\n",
    "    'Plot vertical bar, stacked': 'plot_bar_stacked',\n",
    "    'Plot horisontal bar, stacked': 'plot_barh_stacked',\n",
    "    'Plot line': 'plot_line',\n",
    "    'Plot stacked line': 'plot_line_stacked',\n",
    "    # 'Chart ': 'chart',\n",
    "    'Table': 'table',\n",
    "    'Pivot': 'pivot'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "toggle_style = dict(icon='', layout=widgets.Layout(width='100px', left='0'))\n",
    "drop_style = dict(layout=widgets.Layout(width='260px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>**Mandatory Prepare Step**</span>: Configuration elements\n",
    "The following code cell must to be executed once for each user session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "period_divisions = [\n",
    "    [ (1919, 1939), (1940, 1944), (1945, 1955), (1956, 1966), (1967, 1972) ],\n",
    "    [ (1919, 1944), (1945, 1955), (1956, 1966), (1967, 1972) ]\n",
    "]\n",
    "\n",
    "parties_of_interest = ['FRANCE', 'GERMU', 'ITALY', 'GERMAN', 'UK', 'GERME', 'GERMW', 'INDIA', 'GERMA' ]\n",
    "\n",
    "\n",
    "period_group_options = {\n",
    "    'Year': 'signed_year',\n",
    "    'Default division': 'signed_period',\n",
    "    'Alt. division': 'signed_period_alt'\n",
    "}\n",
    "\n",
    "default_party_options = {\n",
    "    'Top #n parties': None,\n",
    "    'PartyOf5': parties_of_interest,\n",
    "    'France': [ 'FRANCE' ],\n",
    "    'France vs UK': [ 'FRANCE', 'UK' ],\n",
    "    'France vs ALL': [ 'FRANCE', None ],\n",
    "    'Italy': [ 'ITALY' ],\n",
    "    'UK': [ 'UK' ],\n",
    "    'India': [ 'INDIA' ],\n",
    "    'Germany, after 1991': [ 'GERMU' ],\n",
    "    'Germany, before 1945': [ 'GERMAN' ],\n",
    "    'East Germany': [ 'GERME' ],\n",
    "    'West Germany': [ 'GERMW' ],\n",
    "    'Germany, allied occupation': [ 'GERMA' ],\n",
    "    'Germany (all)': [ 'GERMU', 'GERMAN', 'GERME', 'GERMW', 'GERMA' ],\n",
    "    'China': [ 'CHINA' ]\n",
    "}\n",
    "\n",
    "category_group_settings = {\n",
    "    '7CULT, 7SCIEN, and 7EDUC': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '7SCIEN': ['7SCIEN'],\n",
    "        '7EDUC': ['7EDUC']\n",
    "    },\n",
    "    '7CULT, 7SCI, and 7EDUC+4EDUC': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '7SCIEN': ['7SCIEN'],\n",
    "        '7EDUC+4EDUC': ['7EDUC', '4EDUC']\n",
    "    },\n",
    "    '7CULT + 1AMITY': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '1AMITY': ['1AMITY']\n",
    "    },\n",
    "    '7CULT + 1ALLY': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '1ALLY': ['1ALLY']\n",
    "    },\n",
    "    '7CULT + 1DIPLOMACY': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        'DIPLOMACY': ['1ALLY', '1AMITY', '1ARMCO', '1CHART', '1DISPU', '1ESTAB', '1HEAD', '1OCCUP', '1OPTC', '1PEACE', '1RECOG', '1REPAR', '1STATU', '1TERRI', '1TRUST']\n",
    "    },\n",
    "    '7CULT + 2WELFARE': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '2WELFARE': [ '2HEW', '2HUMAN','2LABOR', '2NARK', '2REFUG', '2SANIT', '2SECUR', '2WOMEN' ]\n",
    "    },\n",
    "    '7CULT + 3ECONOMIC': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        'ECONOMIC': ['3CLAIM', '3COMMO', '3CUSTO', '3ECON', '3INDUS', '3INVES', '3MOSTF', '3PATEN', '3PAYMT', '3PROD', '3TAXAT', '3TECH', '3TOUR','3TRADE','3TRAPA']\n",
    "    },\n",
    "    '7CULT + 4AID': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '4AID': ['4AGRIC','4AID', '4ATOM', '4EDUC', '4LOAN', '4MEDIC', '4MILIT', '4PCOR', '4RESOU', '4TECA', '4UNICE']\n",
    "    },\n",
    "    '7CULT + 5TRANSPORT': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '5TRANSPORT': ['5AIR', '5LAND', '5TRANS', '5WATER']\n",
    "    },\n",
    "    '7CULT + 6COMMUNICATIONS': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '6COMMUNICATIONS': ['6COMMU', '6MEDIA', '6POST', '6TELCO']\n",
    "    },\n",
    "    '7CULTURE': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '7EDUC': ['7EDUC'],\n",
    "        '7RELIG': ['7RELIG'],\n",
    "        '7SCIEN': ['7SCIEN'],\n",
    "        '7SEMIN': ['7SEMIN'],\n",
    "        '7SPACE': ['7SPACE']\n",
    "    },\n",
    "    '7CULT': {\n",
    "        '7CULT': ['7CULT']\n",
    "    },\n",
    "    '7CULT + 8RESOURCES': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '8RESOURCES': ['8AGRIC', '8CATTL', '8ENERG', '8ENVIR', '8FISH', '8METAL', '8WATER', '8WOOD']\n",
    "    },\n",
    "    '7CULT + 9ADMINISTRATION': {\n",
    "        '7CULT': ['7CULT'],\n",
    "        '9ADMINISTRATION': ['9ADMIN', '9BOUND', '9CITIZ', '9CONSU', '9LEGAL', '9MILIT', '9MILMI', '9PRIVI', '9VISAS', '9XTRAD' ]\n",
    "    },\n",
    "}\n",
    "\n",
    "category_group_maps = { \n",
    "    category_name: { v: k for k in category_group_settings[category_name].keys() for v in category_group_settings[category_name][k]  }\n",
    "        for category_name in category_group_settings.keys()\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue'>**Mandatory Prepare Step**</span>: Load and Process Treaty Master Index\n",
    "The following code cell to be executed once for each user session. The code loads the WTI master index (and some related data files), and prepares the data for subsequent use.\n",
    "\n",
    "The treaty data is processed as follows:\n",
    "- All the treaty data are loaded.Extract year treaty was signed as seperate fields\n",
    "- Add new fields for specified signed period divisions\n",
    "- Fields 'group1' and 'group2' are ignored (many missing values). Instead group are fetched via party code from encoding found in the \"groups\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load and process treaties master index\n",
    "\n",
    "%run ./common/treaty_state\n",
    "\n",
    "def load_treaty_state():\n",
    "    global state\n",
    "    try:\n",
    "        state = TreatyState()\n",
    "        print(\"Data loaded!\")\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        print('Load failed! Have you run setup cell above?')\n",
    "\n",
    "load_treaty_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Sanity Checks: Per field (pair) value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     25
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05efa8f7c45b4d16884c25e2810797c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Field 1:', options=('', 'is_cultural_yesno', 'source', 'pa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "\n",
    "treaty_fields = [\n",
    "    '', 'is_cultural_yesno', 'source', 'party1', 'party2', 'laterality',\n",
    "    'headnote', 'topic', 'topic1', 'topic2', 'title', 'signed_year', 'signed_period', 'signed_period_alt', 'is_cultural'\n",
    "]    \n",
    "\n",
    "def display_variable_stats(field1, field2, crosstab):\n",
    "    \n",
    "    columns = [ x for x in set([field1, field2 ]) if x != '' ]\n",
    "    \n",
    "    if len(columns) > 0:\n",
    "        df = state.treaties.groupby(columns).size().reset_index()\\\n",
    "            .rename(columns={0: 'Count'})\\\n",
    "            .sort_values(['Count'], ascending=False)\n",
    "            \n",
    "        if crosstab is True:\n",
    "            if len(columns) == 2:\n",
    "                display(pd.crosstab(df[field1], df[field1]))\n",
    "            else:\n",
    "                print('Both fields are needed for crosstab')\n",
    "        else:\n",
    "            display(df)\n",
    "        # df.set_index(columns).plot.bar(figsize=(16,8))\n",
    "\n",
    "def sanity_check_main():\n",
    "    \n",
    "    sw = BaseWidgetUtility(\n",
    "        field1=wf.create_select_widget('Field 1:', treaty_fields, default=''),\n",
    "        field2=wf.create_select_widget('Field 2:', treaty_fields, default=''),\n",
    "        crosstab=widgets.ToggleButton(\n",
    "            description='Crosstab',\n",
    "            button_style='',\n",
    "            icon='check'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    isw = widgets.interactive(display_variable_stats, field1=sw.field1, field2=sw.field2, crosstab=sw.crosstab)\n",
    "\n",
    "    display(widgets.VBox([widgets.HBox([sw.field1, sw.field2, sw.crosstab]), isw.children[-1]]))\n",
    "\n",
    "    isw.update()\n",
    "    \n",
    "sanity_check_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart: Treaty Quantities by Selected Parties \n",
    "This chart displays the number of treaties per party, or group of parties, and year or period divisions. The default division has periods 1919-1944, 1945-1955, 1956-1966, and 1967-1972, and the alternative division  has 1940-1944 as an additional period. Use the \"Top #n\" slider to select how many parties to display for each period ordered by how many treaties the parties signed. The \"Only Cultural\" button filters out treaties having field \"is_cultural\" set to \"yes\".\n",
    "\n",
    "<span style='color: green;'>**DONE: Top #n should show only n countries per selected period **</span><br>\n",
    "<span style='color: green;'>**DONE: Labels for each year not visible when \"Plot line\" is selected **</span><br>\n",
    "<span style='color: green;'>**DONE: Add Country vs Country selection ALL included - (two listboxes) **</span>\n",
    "<span style='color: green;'>**BUG: Review 7CULT recode **</span><br>\n",
    "<span style='color: green;'>**BUG: ALL doesn't work! (France vs ALL) **</span><br>\n",
    "<span style='color: green;'>**TODO: Add ALL as separate baseline (inclusive and exclusive made selection) **</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     35
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879072895fb141fc9abe34169fd0e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Period:', layout=Layout(width='250px'), opt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#colors = hsv(np.linspace(0, 1.0, 16))\n",
    "colors = bokeh.palettes.Category20[20] #plt.get_cmap('jet')(np.linspace(0, 1.0, 16))\n",
    "\n",
    "def plot_treaties_per_period(data, output_format, plot_style, figsize=(12,6), xlabel='', ylabel='', xticks=None):\n",
    "    matplotlib.style.use(plot_style)\n",
    "    stacked = 'stacked' in output_format\n",
    "    kind = output_format.split('_')[1]\n",
    "    ax = data.plot(kind=kind, stacked=stacked, figsize=figsize, color=colors)\n",
    "    \n",
    "    if xticks is not None:\n",
    "        ax.set_xticks(xticks)\n",
    "    \n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    \n",
    "    # Put a legend to the right of the current axis\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "\n",
    "    for tick in ax.get_xticklabels():        \n",
    "        tick.set_rotation(45)\n",
    "    \n",
    "def get_top_parties(data, period, party_name, n_top=5):\n",
    "    xd = data.groupby([period, party_name]).size().rename('TopCount').reset_index()\n",
    "    top_list = xd.groupby([period]).apply(lambda x: x.nlargest(n_top, 'TopCount'))\\\n",
    "        .reset_index(level=0, drop=True)\\\n",
    "        .set_index([period, party_name])\n",
    "    return top_list\n",
    "\n",
    "def display_treaties_per_period(\n",
    "    period,\n",
    "    party_name,\n",
    "    parties_selection,\n",
    "    only_is_cultural=False,\n",
    "    normalize_values=False,\n",
    "    output_format='chart',\n",
    "    plot_style='classic',\n",
    "    top_n_parties=5\n",
    "    ):\n",
    "        \n",
    "    try:\n",
    "\n",
    "        data = state.stacked_treaties.copy()\n",
    "                        \n",
    "        # if only_within_period_of_interest:\n",
    "        data = data.loc[(data.signed_period!='other')]\n",
    "\n",
    "        if only_is_cultural:\n",
    "            data = data.loc[(data.is_cultural==True)]\n",
    "\n",
    "        if isinstance(parties_selection, list):\n",
    "            data = data.loc[(data.party.isin(parties_selection))]\n",
    "                            \n",
    "        data = data.merge(state.parties, how='left', left_on='party', right_index=True)\n",
    "        \n",
    "        n_top_list = get_top_parties(data, period, party_name, n_top=top_n_parties)\n",
    "               \n",
    "        data = data.groupby([period, party_name])\\\n",
    "                .size()\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns={ period: 'Period', party_name: 'Party', 0: 'Count' })\n",
    "\n",
    "        if parties_selection is None:\n",
    "            data = data.merge(n_top_list, how='inner', left_on=['Period', 'Party'], right_index=True)\n",
    "\n",
    "        pivot = pd.pivot_table(data, index=['Period'], values=[\"Count\"], columns=['Party'], fill_value=0)\n",
    "        pivot.columns = [ x[-1] for x in pivot.columns ]\n",
    "    \n",
    "        if period == 'signed_year':\n",
    "            missing_years = [ x for x in range(data.Period.min(), data.Period.max() + 1) if x not in pivot.index ]\n",
    "            for year in missing_years:\n",
    "                pivot.loc[year] = len(pivot.columns) * [0]\n",
    "            pivot.sort_index(axis=0, inplace=True)\n",
    "    \n",
    "        if normalize_values is True:\n",
    "            pivot = pivot.div(0.01 * pivot.sum(1), axis=0)\n",
    "\n",
    "        if output_format.startswith('plot'):\n",
    "\n",
    "            label = 'Number of treaties' if not normalize_values else 'Share%'\n",
    "\n",
    "            ylabel = label if 'barh' not in output_format else ''\n",
    "            xlabel = label if 'barh' in output_format else ''\n",
    "\n",
    "            height = 10 if 'barh' in output_format and period == 'signed_year' else 6\n",
    "\n",
    "            xticks = list(range(data.Period.min(), data.Period.max() + 1)) if 'line' in output_format and period == 'signed_year' else None\n",
    "            \n",
    "            plot_treaties_per_period(pivot, output_format, plot_style, figsize=(18, height), xlabel=xlabel, ylabel=ylabel, xticks=xticks)\n",
    "\n",
    "        elif output_format == 'table':\n",
    "            display(data)\n",
    "            # display(HTML(data.to_html()))\n",
    "        else:\n",
    "            display(pivot)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        raise\n",
    "\n",
    "def treaty_quantities_by_selected_parties_main():\n",
    "    \n",
    "    tw = BaseWidgetUtility(\n",
    "        period=widgets.Dropdown(\n",
    "            options=period_group_options,\n",
    "            value='signed_period',\n",
    "            description='Period:',\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        ),\n",
    "        party_name=widgets.Dropdown(\n",
    "            options={\n",
    "                'WTI Code': 'party',\n",
    "                'WTI Name': 'party_name',\n",
    "                'WTI Short': 'short_name',\n",
    "                'Country': 'party_country'\n",
    "            },\n",
    "            value='party_name',\n",
    "            description='Name:',\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        ),\n",
    "        country1=widgets.Dropdown(\n",
    "            options=[None] + state.get_countries_list(),\n",
    "            value=None,\n",
    "            description='Country#1:',\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        ),\n",
    "        country2=widgets.Dropdown(\n",
    "            options=[None] + state.get_countries_list(),\n",
    "            value=None,\n",
    "            description='Country#2:',\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        ),\n",
    "        parties_selection=widgets.Dropdown(\n",
    "            options=default_party_options,  # https://stackoverflow.com/questions/35023744/how-to-order-entries-in-ipywidgets-dropdown-or-select\n",
    "            value=default_party_options['PartyOf5'],\n",
    "            description='Parties:',\n",
    "            layout=widgets.Layout(width='250px')\n",
    "        ),\n",
    "\n",
    "        only_is_cultural=widgets.ToggleButton(\n",
    "            description='Only Cultural', value=True, **toggle_style\n",
    "        ),\n",
    "        normalize_values=widgets.ToggleButton(\n",
    "            description='Share%', **toggle_style\n",
    "        ),\n",
    "        output_format=widgets.Dropdown(\n",
    "            description='Output', options=output_formats, value='plot_bar_stacked', layout=widgets.Layout(width='300px')\n",
    "        ),\n",
    "        plot_style=widgets.Dropdown(\n",
    "            options=matplotlib_plot_styles, value='seaborn-pastel',\n",
    "            description='Style:', layout=widgets.Layout(width='300px')\n",
    "        ),\n",
    "        top_n_parties=widgets.IntSlider(\n",
    "            value=3, min=1, max=10, step=1,\n",
    "            description='Top #:',\n",
    "            continuous_update=True,\n",
    "            layout=widgets.Layout(width='220px')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_treaties_per_period,\n",
    "        period=tw.period,\n",
    "        party_name=tw.party_name,\n",
    "        parties_selection=tw.parties_selection,\n",
    "        only_is_cultural=tw.only_is_cultural,\n",
    "        normalize_values=tw.normalize_values,\n",
    "        output_format=tw.output_format,\n",
    "        plot_style=tw.plot_style,\n",
    "        top_n_parties=tw.top_n_parties\n",
    "    )\n",
    "\n",
    "    def on_country_change(change):\n",
    "        if tw.country1.value is None or tw.country2.value is None:\n",
    "            return\n",
    "        name = (tw.country1.value or '') + ' vs ' + (tw.country2.value or '')\n",
    "        if name not in tw.parties_selection.options.keys():\n",
    "            tw.parties_selection.index = None\n",
    "            new_options = extend(dict(tw.parties_selection.options), {name: [tw.country1.value, tw.country2.value]})\n",
    "            tw.parties_selection.options = new_options\n",
    "        tw.parties_selection.value = new_options[name]\n",
    "\n",
    "    tw.country1.observe(on_country_change, names='value')\n",
    "    tw.country2.observe(on_country_change, names='value')\n",
    "\n",
    "    def on_parties_change(change):\n",
    "        try:\n",
    "            # print(change)\n",
    "            # tw.top_n_parties.value = 0\n",
    "            tw.top_n_parties.disabled = change['new'] is not None\n",
    "        except Exception as ex:\n",
    "            logger.info(ex)\n",
    "\n",
    "    tw.parties_selection.observe(on_parties_change, names='value')\n",
    "\n",
    "    first_column_box = widgets.VBox([tw.period, tw.party_name,])\n",
    "    second_column_box = widgets.VBox([ tw.parties_selection, tw.top_n_parties ])\n",
    "    another_column_box = widgets.VBox([ tw.country1, tw.country2 ])\n",
    "    third_column_box = widgets.VBox([ tw.only_is_cultural, tw.normalize_values])\n",
    "    fourth_column_box = widgets.VBox([ tw.output_format, tw.plot_style ])\n",
    "    boxes = widgets.HBox([first_column_box, second_column_box, another_column_box, third_column_box, fourth_column_box ])\n",
    "    display(widgets.VBox([boxes, itw.children[-1]]))\n",
    "    itw.update()\n",
    "\n",
    "treaty_quantities_by_selected_parties_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Chart: Treaty Quantities by Selected Topics\n",
    "This report displays the number of treaties per division periodand WTI category, or groups of categories. The \"recode 7CULT\" flag sets all treaties having \"is_cultural\" true to 7CULT, other \"7NOCULT\". Note that treaties categorized as 7CULT is always included, even if \"is_cultural\" is false. When the \"+other\" flag is checked, *all* other treaties are included recoded as an \"OTHER\" category.\n",
    "\n",
    "Note that currently the grouping is only based on \"topic1\" i.e. \"topic2\" is ignored in this report.\n",
    "\n",
    "** TODO: Recode 7CULT funkar inte som den ska. Man skall kunna välja WTI's ursprungsindelning ELLER Ben's omkodning enligt is_cultural (då omdöpt 7CORR). ** <br>\n",
    "** TODO: Try multiselect of countries** <br>\n",
    "** DONE: Add way to display a chart per country ** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     109
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88efa87e66fe4b32b38de2d34432b5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Period:', layout=Layout(width='300px'), opt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_display_quantity_of_topics(pivot, kind, stacked, xlabel='', ylabel='', plot_style='classic', figsize=(12,10), **kwargs):\n",
    "\n",
    "    matplotlib.style.use(plot_style)\n",
    "    \n",
    "    ax = pivot.plot(kind=kind, stacked=stacked, figsize=figsize, **kwargs)\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    # legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4)\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "    \n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)    \n",
    "\n",
    "def display_quantity_of_topics(\n",
    "    period,\n",
    "    category_map_name,\n",
    "    recode_7cult=False,\n",
    "    normalize_values=False,\n",
    "    include_other=False,\n",
    "    output_format='chart',\n",
    "    plot_style='classic',\n",
    "    parties_selection=None\n",
    "    ):\n",
    "    try:\n",
    "        \n",
    "        data = state.treaties.copy()\n",
    "        data = data.loc[(data.signed_period!='other')]\n",
    "\n",
    "        if isinstance(parties_selection, list):\n",
    "            data = data.loc[(data.party1.isin(parties_selection)|(data.party2.isin(parties_selection)))]\n",
    "           \n",
    "        category_map = category_group_maps[category_map_name]\n",
    "        \n",
    "        if not include_other:\n",
    "            data = data.loc[(data.topic1.isin(category_map.keys())) | (data.topic2.isin(category_map.keys()))]\n",
    "\n",
    "        if data.shape[0] == 0:\n",
    "            print('No data for: ' + ','.join(parties_selection))\n",
    "            return\n",
    "            \n",
    "        data['category'] = data.apply(lambda x: category_map.get(x['topic1'], category_map.get(x['topic2'], 'OTHER')), axis=1)\n",
    "        data = data\\\n",
    "                .groupby([period, 'category'])\\\n",
    "                .size()\\\n",
    "                .reset_index()\\\n",
    "                .rename(columns={ period: 'Period', 'category': 'Category', 0: 'Count' })\n",
    "\n",
    "        pivot = pd.pivot_table(data, index=['Period'], values=[\"Count\"], columns=['Category'], fill_value=0)\n",
    "        pivot.columns = [ x[-1] for x in pivot.columns ]\n",
    "        \n",
    "        if period == 'signed_year':\n",
    "            missing_years = [ x for x in range(data.Period.min(), data.Period.max() + 1) if x not in pivot.index ]\n",
    "            for year in missing_years:\n",
    "                pivot.loc[year] = len(pivot.columns) * [0]\n",
    "            pivot.sort_index(axis=0, inplace=True)\n",
    "\n",
    "        if normalize_values is True:\n",
    "            pivot = pivot.div(0.01 * pivot.sum(1), axis=0)\n",
    "\n",
    "        if output_format.startswith('plot'):\n",
    "\n",
    "            label1 = 'Number of treaties' if not normalize_values else 'Share%'\n",
    "            title = 'Parties ' + ', '.join(parties_selection)\n",
    "            \n",
    "            ylabel = label1 if 'barh' not in output_format else '' \n",
    "            xlabel = label1 if 'barh' in output_format else ''\n",
    "\n",
    "            stacked = 'stacked' in output_format\n",
    "            kind = output_format.split('_')[1]\n",
    "            height = 10 if 'barh' in output_format and period == 'signed_year' else 6\n",
    "\n",
    "            plot_display_quantity_of_topics(\n",
    "                pivot, kind=kind, stacked=stacked, xlabel=xlabel, ylabel=ylabel, plot_style=plot_style, figsize=(16,height), title=title\n",
    "            )\n",
    "\n",
    "        elif output_format == 'chart':\n",
    "            print('bokeh plot not implemented')\n",
    "            data.plot.line(figsize=(12,8))\n",
    "        elif output_format == 'table':\n",
    "            display(data)\n",
    "            #display(HTML(data.to_html()))\n",
    "        else:\n",
    "            display(pivot)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        # raise\n",
    "        \n",
    "def treaty_quantities_by_selected_topics_main():\n",
    "\n",
    "    party_options = {\n",
    "        'PartyOf5': parties_of_interest,\n",
    "        'China': [ 'CHINA' ],\n",
    "        'France': [ 'FRANCE' ],\n",
    "        'Italy': [ 'ITALY' ],\n",
    "        'UK': [ 'UK' ],\n",
    "        'India': [ 'INDIA' ],\n",
    "        'Germany, after 1991': [ 'GERMU' ],\n",
    "        'Germany, before 1945': [ 'GERMAN' ],\n",
    "        'East Germany': [ 'GERME' ],\n",
    "        'West Germany': [ 'GERMW' ],\n",
    "        'Germany, allied occupation': [ 'GERMA' ],\n",
    "        'Germany (all)': [ 'GERMU', 'GERMAN', 'GERME', 'GERMW', 'GERMA' ]\n",
    "    }\n",
    "    \n",
    "    widget_container = BaseWidgetUtility(\n",
    "        period=widgets.Dropdown(\n",
    "            options=period_group_options,\n",
    "            value='signed_period',\n",
    "            description='Period:', layout=widgets.Layout(width='300px')\n",
    "        ),\n",
    "        category_map_name=widgets.Dropdown(\n",
    "            options=category_group_maps.keys(),\n",
    "            value='7CULTURE',\n",
    "            description='Category:', layout=widgets.Layout(width='300px')\n",
    "        ),\n",
    "        recode_7cult=widgets.ToggleButton(\n",
    "            description='Recode 7CULT',\n",
    "            tooltip='Treat all treaties with cultural=yes as 7CULT',\n",
    "            value=False, layout=widgets.Layout(width='120px')\n",
    "        ),\n",
    "        normalize_values=widgets.ToggleButton(\n",
    "            description='Normalize%',\n",
    "            tooltip='Display shares per category instead of count', layout=widgets.Layout(width='120px')\n",
    "        ),\n",
    "        include_other=widgets.ToggleButton(\n",
    "            description='+Other', value=False,  layout=widgets.Layout(width='120px')\n",
    "        ),\n",
    "        output_format=widgets.Dropdown(\n",
    "            description='Output',\n",
    "            value='plot_bar_stacked',\n",
    "            options=output_formats, **drop_style\n",
    "        ),\n",
    "        plot_style=widgets.Dropdown(\n",
    "            options=matplotlib_plot_styles,\n",
    "            value='seaborn-pastel',\n",
    "            description='Style:', **drop_style\n",
    "        ),   \n",
    "        parties_selection=widgets.Dropdown(\n",
    "            options=party_options,\n",
    "            value=party_options['PartyOf5'],\n",
    "            description='Parties:',\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        ),\n",
    "        chart_per_party=widgets.ToggleButton(\n",
    "            description='Chart per party',\n",
    "            tooltip='Display one chart per party', layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def display_quantity_of_topics_proxy(\n",
    "        period,\n",
    "        category_map_name,\n",
    "        recode_7cult=False,\n",
    "        normalize_values=False,\n",
    "        include_other=False,\n",
    "        output_format='chart',\n",
    "        plot_style='classic',\n",
    "        parties_selection=None,\n",
    "        chart_per_party=False\n",
    "    ):\n",
    "        # clear_output()\n",
    "        party_groups = [ [ x ] for x in parties_selection ] if chart_per_party else [ parties_selection ]\n",
    "        for party_group in party_groups:\n",
    "            display_quantity_of_topics(period, category_map_name, recode_7cult, normalize_values,\n",
    "                                       include_other, output_format, plot_style, party_group)\n",
    "        \n",
    "    itw = widgets.interactive(\n",
    "        display_quantity_of_topics_proxy,\n",
    "        period=widget_container.period,\n",
    "        category_map_name=widget_container.category_map_name,\n",
    "        recode_7cult=widget_container.recode_7cult,\n",
    "        normalize_values=widget_container.normalize_values,\n",
    "        include_other=widget_container.include_other,\n",
    "        output_format=widget_container.output_format,\n",
    "        plot_style=widget_container.plot_style,\n",
    "        parties_selection=widget_container.parties_selection,\n",
    "        chart_per_party=widget_container.chart_per_party\n",
    "    )\n",
    "\n",
    "    boxes = widgets.HBox(\n",
    "        [\n",
    "            widgets.VBox([ widget_container.period, widget_container.category_map_name, widget_container.parties_selection]),        \n",
    "            widgets.VBox([ widget_container.recode_7cult, widget_container.normalize_values, widget_container.chart_per_party]),\n",
    "            widgets.VBox([ widget_container.include_other]),\n",
    "            widgets.VBox([ widget_container.output_format, widget_container.plot_style ])\n",
    "        ]\n",
    "    )\n",
    "    display(widgets.VBox([boxes, itw.children[-1]]))\n",
    "    itw.update()\n",
    "\n",
    "treaty_quantities_by_selected_topics_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Headnote word toplist and word-pair co-occurence toplist\n",
    "This report displays headnote toplists either single word occurrance or word-word co-occurrance toplists depending on whether or not the \"Co-occurrance\" is checked. The result is grouped by selected division's periods or by year.\n",
    "\n",
    "The word co-occurrance is defined as the number of times a pair of words co-occur in the same headnote. The length of headnotes is ignored in the computation (all pairs have equal weight). Multiple occurance of a word in a headnote is taken into account i.e \"cultural exchange cultural\" is counted as two co-occurances, and \"cultural exchange exchange cultural\" is four co-occurrances. Stopwords are removed if \"Remove stopwords\" are checked.\n",
    "\n",
    "Stopwords are always removed from the co-occurrance computation, whilst they are removed from single word occurrance toplist if the \"Remove stopwords\" flag is checked. The removal is based on NLTK's list of english stopwords (run ```nltk.corpus.stopwords.words('english')``` to display all stopwords).\n",
    "\n",
    "The toplist can be filtered so that only treaties involving any or one of the five parties of interest are included, and words can be excluded based on character length. Each resulting group can also be restricted by both a maximum number of pairs to display per group, as well as a min co-occurrance count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11a95984c7e48b8af002369cf9b1315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Period:', index=1, layout=Layout(width='260…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import qgrid\n",
    "\n",
    "toggle_style = dict(icon='', layout=widgets.Layout(width='140px', left='0'))\n",
    "\n",
    "class HeadnoteTokenServiceOLD():\n",
    "\n",
    "    def __init__(self, tokenizer, stopwords=None, lemmatizer=None, min_word_size=2):\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenizer,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_word_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)) \n",
    "        ]\n",
    "        \n",
    "        if stopwords is not None:\n",
    "            self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        if lemmatizer is not None:\n",
    "            self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "\n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def parse_headnotes(self, treaties):\n",
    "        \n",
    "        headnotes = treaties['headnote']\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        #tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compute_stacked(self, treaties):\n",
    "        \n",
    "        df = self.parse_headnotes(treaties)\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "    def compute_co_occurrence(self, treaties, pos_tags, only_cultural_treaties=False):\n",
    "\n",
    "        # Filter out tags based on treaties of interest\n",
    "        pos_tags = pos_tags.merge(treaties, how='inner', left_on='treaty_id', right_index=True)[[]]\n",
    "        \n",
    "        if only_cultural_treaties:\n",
    "            df_pos_tags = df_pos_tags[(df_pos_tags.is_cultural.str.contains('yes',na=False))]\n",
    "\n",
    "        # Self join of words within same treaty\n",
    "        df_co_occurrence = pd.merge(df_pos_tags, df_pos_tags, how='inner', left_on='treaty_id', right_on='treaty_id')\n",
    "        # Only consider a specific poir once\n",
    "        df_co_occurrence = df_co_occurrence[(df_co_occurrence.wid_x < df_co_occurrence.wid_y)]\n",
    "        # Reduce number of returned columns\n",
    "        df_co_occurrence = df_co_occurrence[['treaty_id', 'year_x', 'is_cultural_x', 'lemma_x', 'lemma_y' ]]\n",
    "        # Rename columns\n",
    "        df_co_occurrence.columns = ['treaty_id', 'year', 'is_cultural', 'lemma_x', 'lemma_y' ]\n",
    "\n",
    "        # Sort token pair so smallest always comes first\n",
    "        lemma_x = df_co_occurrence[['lemma_x', 'lemma_y']].min(axis=1)\n",
    "        lemma_y = df_co_occurrence[['lemma_x', 'lemma_y']].max(axis=1)\n",
    "        df_co_occurrence['lemma_x'] = lemma_x\n",
    "        df_co_occurrence['lemma_y'] = lemma_y\n",
    "\n",
    "        return df_co_occurrence\n",
    "\n",
    "class HeadnoteTokenCorpus():\n",
    "\n",
    "    def __init__(self, treaties, tokenize=None, stopwords=None, lemmatize=None, min_size=2):\n",
    "        \n",
    "        tokenize = tokenize or nltk.tokenize.word_tokenize\n",
    "        lemmatize = lemmatize or WordNetLemmatizer().lemmatize\n",
    "        stopwords = stopwords or nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenize,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)),\n",
    "            lambda ws: list(set(ws)) \n",
    "        ]\n",
    "        \n",
    "        #if stopwords is not None:\n",
    "        #    self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        #if lemmatizer is not None:\n",
    "        #    self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "        \n",
    "        treaty_tokens = self._compute_stacked(treaties)\n",
    "        vocabulary = treaty_tokens.token.unique()\n",
    "        lemmas = list(map(lemmatize, vocabulary))\n",
    "        lemma_map = { w: l for (w, l) in zip(*(vocabulary, lemmas)) if w != l }\n",
    "        stopwords_map = { s : True for s in stopwords }\n",
    "        treaty_tokens['lemma'] = treaty_tokens.token.apply(lambda x: lemma_map.get(x, x))\n",
    "        treaty_tokens['is_stopword'] = treaty_tokens.token.apply(lambda x: stopwords_map.get(x, False))\n",
    "\n",
    "        self.treaty_tokens = treaty_tokens.set_index(['treaty_id', 'sequence_id'])\n",
    "        \n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def _parse_headnotes(self, treaties):\n",
    "        \n",
    "        headnotes = treaties['headnote']\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _compute_stacked(self, treaties):\n",
    "        \n",
    "        df = self._parse_headnotes(treaties)\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "def compute_co_occurrance(treaties):\n",
    "    \n",
    "    treaty_tokens = state.treaty_headnote_corpus.treaty_tokens\n",
    "    i1 = treaties.index\n",
    "    # i2 = treaty_tokens.reset_index().set_index('treaty_id').index\n",
    "    i2 = treaty_tokens.index.get_level_values(0)\n",
    "    treaty_tokens = treaty_tokens[i2.isin(i1)]\n",
    "    \n",
    "    treaty_tokens = treaty_tokens.loc[treaty_tokens.is_stopword==False]\n",
    "    treaty_tokens = treaty_tokens.reset_index().drop(['is_stopword', 'sequence_id'], axis=1).set_index('treaty_id')\n",
    "\n",
    "    co_occurrance = treaty_tokens.merge(treaty_tokens, how='inner', left_index=True, right_index=True)\n",
    "    co_occurrance = co_occurrance.loc[(co_occurrance['token_x'] < co_occurrance['token_y'])]\n",
    "    #co_occurrance['token'] = co_occurrance.apply(lambda row: row[groupby_pair[0]] + ' - ' + row[groupby_pair[1]], axis=1)\n",
    "    co_occurrance['token'] = co_occurrance.apply(lambda row: ' - '.join([row['token_x'].upper(), row['token_y'].upper()]), axis=1)\n",
    "    co_occurrance['lemma'] = co_occurrance.apply(lambda row: ' - '.join([row['lemma_x'].upper(), row['lemma_y'].upper()]), axis=1)\n",
    "    co_occurrance = co_occurrance.assign(is_stopword=False, sequence_id=0)[['sequence_id', 'token', 'lemma', 'is_stopword']]\n",
    "    \n",
    "    return co_occurrance\n",
    "\n",
    "def create_bigram_transformer(documents):\n",
    "    import gensim.models.phrases\n",
    "    bigram = gensim.models.phrases.Phrases(map(nltk.tokenize.word_tokenize, documents))\n",
    "    return lambda ws: bigram[ws]\n",
    "\n",
    "def remove_snake_case(snake_str):\n",
    "    return ' '.join(x.title() for x in snake_str.split('_'))\n",
    "\n",
    "def get_top_partiesssss(data, period, party_name, n_top=5):\n",
    "    xd = data.groupby([period, party_name]).size().rename('TopCount').reset_index()\n",
    "    top_list = xd.groupby([period]).apply(lambda x: x.nlargest(n_top, 'TopCount'))\\\n",
    "        .reset_index(level=0, drop=True)\\\n",
    "        .set_index([period, party_name])\n",
    "    return top_list\n",
    "\n",
    "result=None\n",
    "def display_headnote_toplist(\n",
    "    period=None,\n",
    "    parties=None,\n",
    "    extra_groupbys=None,\n",
    "    only_is_cultural=True,\n",
    "    use_lemma=False,\n",
    "    compute_co_occurance=False,\n",
    "    remove_stopwords=True,\n",
    "    min_word_size=2,\n",
    "    n_min_count=1,\n",
    "    output_format='table',\n",
    "    n_top=50\n",
    "    # plot_style=tw.plot_style\n",
    "):\n",
    "    global ihnw, result\n",
    "    \n",
    "    try:\n",
    "        hnw.progress.value = 1    \n",
    "        treaties = state.treaties.loc[state.treaties.signed_period != 'other']\n",
    "\n",
    "        if state.treaty_headnote_corpus is None:\n",
    "            print('Preparing headnote corpus for first time use')\n",
    "            state.treaty_headnote_corpus = HeadnoteTokenCorpus(treaties=treaties)\n",
    "\n",
    "        if only_is_cultural:\n",
    "            treaties = treaties.loc[(state.treaties.is_cultural)]\n",
    "\n",
    "        if parties is not None:\n",
    "            ids = state.stacked_treaties.loc[(state.stacked_treaties.party.isin(parties))].index\n",
    "            treaties = treaties.loc[ids]\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        if compute_co_occurance:\n",
    "\n",
    "            treaty_tokens = compute_co_occurrance(treaties)\n",
    "\n",
    "        else:\n",
    "\n",
    "            treaty_tokens = state.treaty_headnote_corpus.treaty_tokens\n",
    "\n",
    "            if remove_stopwords is True:\n",
    "                treaty_tokens = treaty_tokens.loc[treaty_tokens.is_stopword==False]\n",
    "\n",
    "            treaty_tokens = treaty_tokens.reset_index().set_index('treaty_id')\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        treaty_tokens = treaty_tokens\\\n",
    "            .merge(treaties, how='inner', left_index=True, right_index=True)\\\n",
    "            .drop(['sequence', 'is_cultural_yesno', 'source', 'signed', 'headnote', 'is_cultural',\n",
    "                   'topic1', 'topic2', 'title'], axis=1)\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        token_or_lemma = 'token' if not use_lemma else 'lemma'\n",
    "\n",
    "        groupbys  = []\n",
    "        groupbys += [ period ] if not period is None else []\n",
    "        groupbys += (extra_groupbys or [])\n",
    "        groupbys += [ token_or_lemma ]\n",
    "\n",
    "        result = treaty_tokens.groupby(groupbys).size().reset_index().rename(columns={0: 'Count'})\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        ''' Filter out the n_top most frequent words from each group '''\n",
    "        result = result.groupby(groupbys[-1]).apply(lambda x: x.nlargest(n_top, 'Count'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            # .set_index(groupbys)\n",
    "\n",
    "        if min_word_size > 0:\n",
    "            result = result.loc[result[token_or_lemma].str.len() >= min_word_size]\n",
    "\n",
    "        if n_min_count > 1:\n",
    "            result = result.loc[result.Count >= n_min_count]\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        result = result.sort_values(groupbys[:-1] + ['Count'], ascending=len(groupbys[:-1])*[True] + [False])\n",
    "\n",
    "        hnw.progress.value += 1\n",
    "\n",
    "        if output_format in ('table', 'qgrid'):\n",
    "            result.columns = [ remove_snake_case(x) for x in result.columns ]\n",
    "            if output_format == 'table':\n",
    "                display(HTML(result.to_html()))\n",
    "            else:\n",
    "                qgrid_widget = qgrid.show_grid(result, show_toolbar=True)\n",
    "                qgrid_widget\n",
    "        elif output_format == 'unstack':\n",
    "            result = result.set_index(groupbys).unstack(level=0).fillna(0).astype('int32')\n",
    "            result.columns = [ x[1] for x in result.columns ]\n",
    "            display(HTML(result.to_html()))\n",
    "        elif output_format == 'unstack_plot':\n",
    "            result = result.set_index(list(reversed(groupbys))).unstack(level=0).fillna(0).astype('int32')\n",
    "            result.columns = [ x[1] for x in result.columns ]\n",
    "            result.plot(kind='bar', figsize=(16,8))\n",
    "\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    hnw.progress.value += 1\n",
    "    hnw.progress.value = 0\n",
    "\n",
    "hnw = BaseWidgetUtility(\n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    parties=widgets.Dropdown(\n",
    "        options=default_party_options,\n",
    "        value=None,\n",
    "        description='Parties:', **drop_style\n",
    "    ),\n",
    "    use_lemma=widgets.ToggleButton(\n",
    "        description='Use lemma', value=False,\n",
    "        tooltip='Use WordNet lemma', **toggle_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords', **toggle_style\n",
    "    ),\n",
    "    extra_groupbys=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Topic': [ 'Topic' ],\n",
    "        },\n",
    "        value=None,\n",
    "        description='Groupbys:', **drop_style\n",
    "    ),\n",
    "    min_word_size=widgets.BoundedIntText(\n",
    "        value=2, min=0, max=5, step=1,\n",
    "        description='Min word:', layout=widgets.Layout(width='140px')\n",
    "    ),\n",
    "    only_is_cultural=widgets.ToggleButton(\n",
    "        description='Only Cultural', value=True,\n",
    "        tooltip='Display only \"is_cultural\" treaties', **toggle_style\n",
    "    ),\n",
    "    compute_co_occurance=widgets.ToggleButton(\n",
    "        description='Cooccurrence', value=True,\n",
    "        tooltip='Compute Cooccurrence', **toggle_style\n",
    "    ),\n",
    "    output_format=widgets.Dropdown(\n",
    "        description='Output', value='table',\n",
    "        options={\n",
    "            'Table': 'table',\n",
    "            'Qgrid': 'qgrid',\n",
    "            'Unstack': 'unstack',\n",
    "            'Unstack plot': 'unstack_plot'\n",
    "        }, **drop_style\n",
    "    ),\n",
    "    plot_style=widgets.Dropdown(\n",
    "        options=matplotlib_plot_styles,\n",
    "        value='seaborn-pastel',\n",
    "        description='Style:', **drop_style\n",
    "    ),\n",
    "    n_top=widgets.IntSlider(\n",
    "        value=25, min=2, max=100, step=10,\n",
    "        description='Top/grp #:', # continuous_update=False,\n",
    "    ),\n",
    "    n_min_count=widgets.IntSlider(\n",
    "        value=2, min=1, max=10, step=1,\n",
    "        tooltip='Filter out words with count less than specified value',\n",
    "        description='Min count:', # continuous_update=False,\n",
    "    ),\n",
    "    progress=wf.create_int_progress_widget(min=0, max=10, step=1, value=0, layout=widgets.Layout(width='99%')),\n",
    ")\n",
    "\n",
    "ihnw = widgets.interactive(\n",
    "    display_headnote_toplist,\n",
    "    period=hnw.period,\n",
    "    parties=hnw.parties,\n",
    "    extra_groupbys=hnw.extra_groupbys,\n",
    "    only_is_cultural=hnw.only_is_cultural,\n",
    "    n_min_count=hnw.n_min_count,\n",
    "    n_top=hnw.n_top,\n",
    "    min_word_size=hnw.min_word_size,\n",
    "    use_lemma=hnw.use_lemma,\n",
    "    compute_co_occurance=hnw.compute_co_occurance,\n",
    "    remove_stopwords=hnw.remove_stopwords,\n",
    "    output_format=hnw.output_format,\n",
    "    # plot_style=tw.plot_style\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([ hnw.period, hnw.parties, hnw.min_word_size ]),\n",
    "        widgets.VBox([ hnw.extra_groupbys, hnw.n_top, hnw.n_min_count]),\n",
    "        widgets.VBox([ hnw.only_is_cultural, hnw.use_lemma, hnw.remove_stopwords, hnw.compute_co_occurance]),\n",
    "        widgets.VBox([ hnw.output_format, hnw.progress ])\n",
    "    ]\n",
    ")\n",
    "display(widgets.VBox([boxes, ihnw.children[-1]]))\n",
    "ihnw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style='color:blue'>**Mandatory Step**</span>: Prepare Treaty Text Corpora\n",
    "\n",
    "This code cell is a mandatory step for subsequent text corpus statistics. \n",
    "\n",
    "This step processes the treaty text for from given compressed archive (ZIP-file), each language , and stores in an efficient Market-Matrix (MM) corpus format. The corpora is only stored if it is not previously stored, or the \"Force Update\" is specified. Note that an update MUST be forced whenever the treaty archive is updated - otherwise the text in the new archive is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2a3c1db5874d4f99bc7d0613b74872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Corpus:', options=('./data/treaty_corpus_20180821.zip',), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "\n",
    "sort_chained = lambda x, f: list(x).sort(key=f) or x\n",
    "    \n",
    "def ls_sorted(path):\n",
    "    return sort_chained(list(filter(os.path.isfile, glob.glob(path))), os.path.getmtime)\n",
    "       \n",
    "class CompressedFileReader(object):\n",
    "\n",
    "    def __init__(self, archive_pattern, filename_pattern='*.txt'):\n",
    "        self.archive_pattern = archive_pattern\n",
    "        self.filename_pattern = filename_pattern\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for zip_path in glob.glob(self.archive_pattern):\n",
    "            with zipfile.ZipFile(zip_path) as zip_file:\n",
    "                filenames = [ name for name in zip_file.namelist() if fnmatch.fnmatch(name, self.filename_pattern) ]\n",
    "                for filename in filenames:\n",
    "                    try:\n",
    "                        with zip_file.open(filename, 'rU') as text_file:\n",
    "                            content = text_file.read()\n",
    "                            content = gensim.utils.to_unicode(content, 'utf8', errors='ignore')\n",
    "                            content = content.replace('-\\r\\n', '').replace('-\\n', '')\n",
    "                            yield os.path.basename(filename), content\n",
    "                    except:\n",
    "                        print('Unicode error: {}'.format(filename))\n",
    "                        raise\n",
    "                        \n",
    "class TreatyCorpus(TextCorpus):\n",
    "\n",
    "    def __init__(self, content_iterator, dictionary=None, metadata=False, character_filters=None,\n",
    "                 tokenizer=None, token_filters=None, bigram_transform=False\n",
    "    ):\n",
    "        self.content_iterator = content_iterator\n",
    "        \n",
    "        token_filters = [\n",
    "           (lambda tokens: [ x.lower() for x in tokens ]),\n",
    "           (lambda tokens: [ x for x in tokens if any(map(lambda x: x.isalpha(), x)) ])\n",
    "        ] + (token_filters or [])\n",
    "        \n",
    "        #if bigram_transform is True:\n",
    "        #    train_corpus = TreatyCorpus(content_iterator, token_filters=[ x.lower() for x in tokens ])\n",
    "        #    phrases = gensim.models.phrases.Phrases(train_corpus)\n",
    "        #    bigram = gensim.models.phrases.Phraser(phrases)\n",
    "        #    token_filters.append(\n",
    "        #        lambda tokens: bigram[tokens]\n",
    "        #    )           \n",
    "        \n",
    "        super(TreatyCorpus, self).__init__(\n",
    "            input=True,\n",
    "            dictionary=dictionary,\n",
    "            metadata=metadata,\n",
    "            character_filters=character_filters,\n",
    "            tokenizer=tokenizer,\n",
    "            token_filters=token_filters\n",
    "        )\n",
    "        \n",
    "    def getstream(self):\n",
    "        \"\"\"Generate documents from the underlying plain text collection (of one or more files).\n",
    "        Yields\n",
    "        ------\n",
    "        str\n",
    "            Document read from plain-text file.\n",
    "        Notes\n",
    "        -----\n",
    "        After generator end - initialize self.length attribute.\n",
    "        \"\"\"\n",
    "        filenames = []\n",
    "        num_texts = 0\n",
    "        for filename, content in self.content_iterator:\n",
    "            yield content\n",
    "            filenames.append(filename)\n",
    "        self.length = num_texts\n",
    "        self.filenames = filenames\n",
    "        self.document_names = self._compile_document_names()\n",
    "                 \n",
    "    def get_texts(self):\n",
    "        '''\n",
    "        This is mandatory method from gensim.corpora.TextCorpus. Returns stream of documents.\n",
    "        '''\n",
    "        for document in self.getstream():\n",
    "            yield self.preprocess_text(document)\n",
    "            \n",
    "    def preprocess_text(self, text):\n",
    "            \"\"\"Apply `self.character_filters`, `self.tokenizer`, `self.token_filters` to a single text document.\n",
    "            Parameters\n",
    "            ---------\n",
    "            text : str\n",
    "                Document read from plain-text file.\n",
    "            Return\n",
    "            ------\n",
    "            list of str\n",
    "                List of tokens extracted from `text`.\n",
    "            \"\"\"\n",
    "            for character_filter in self.character_filters:\n",
    "                text = character_filter(text)\n",
    "\n",
    "            tokens = self.tokenizer(text)\n",
    "            for token_filter in self.token_filters:\n",
    "                tokens = token_filter(tokens)\n",
    "\n",
    "            return tokens\n",
    "        \n",
    "    def _compile_document_names(self):\n",
    "        \n",
    "        document_names = pd.DataFrame(dict(\n",
    "            document_name=self.filenames,\n",
    "            treaty_id=[ x.split('_')[0] for x in self.filenames ]\n",
    "        )).reset_index().rename(columns={'index': 'document_id'})\n",
    "        \n",
    "        document_names = document_names.set_index('document_id')   \n",
    "        dupes = document_names.groupby('treaty_id').size().loc[lambda x: x > 1]\n",
    "        \n",
    "        if len(dupes) > 0:\n",
    "            logger.critical('Warning! Duplicate treaties found in corpus: {}'.format(' '.join(list(dupes.index))))\n",
    "            \n",
    "        return document_names\n",
    "\n",
    "class MmCorpusStatisticsService():\n",
    "    \n",
    "    def __init__(self, corpus, dictionary, language):\n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        self.stopwords = nltk.corpus.stopwords.words(language[1])\n",
    "        _ = dictionary[0]\n",
    "        \n",
    "    def get_total_token_frequencies(self):\n",
    "        dictionary = self.corpus.dictionary\n",
    "        freqencies = np.zeros(len(dictionary.id2token))\n",
    "        document_stats = []\n",
    "        for document in corpus:\n",
    "            for i, f in document:\n",
    "                freqencies[i] += f\n",
    "        return freqencies\n",
    "\n",
    "    def get_document_token_frequencies(self):\n",
    "        from itertools import chain\n",
    "        '''\n",
    "        Returns a DataFrame with per document token frequencies i.e. \"melts\" doc-term matrix\n",
    "        '''\n",
    "        data = ((document_id, x[0], x[1]) for document_id, values in enumerate(self.corpus) for x in values )\n",
    "        pd = pd.DataFrame(list(zip(*data)), columns=['document_id', 'token_id', 'count'])\n",
    "        pd = pd.merge(self.corpus.document_names, left_on='document_id', right_index=True)\n",
    "\n",
    "        return pd\n",
    "\n",
    "    def compute_word_frequencies(self, remove_stopwords):\n",
    "        id2token = self.dictionary.id2token\n",
    "        term_freqencies = np.zeros(len(id2token))\n",
    "        document_stats = []\n",
    "        for document in self.corpus:\n",
    "            for i, f in document:\n",
    "                term_freqencies[i] += f\n",
    "        stopwords = set(self.stopwords).intersection(set(id2token.values()))\n",
    "        df = pd.DataFrame({\n",
    "            'token_id': list(id2token.keys()),\n",
    "            'token': list(id2token.values()),\n",
    "            'frequency': term_freqencies,\n",
    "            'dfs':  list(self.dictionary.dfs.values())\n",
    "        })\n",
    "        df['is_stopword'] = df.token.apply(lambda x: x in stopwords)\n",
    "        if remove_stopwords is True:\n",
    "            df = df.loc[(df.is_stopword==False)]\n",
    "        df['frequency'] = df.frequency.astype(np.int64)\n",
    "        df = df[['token_id', 'token', 'frequency', 'dfs', 'is_stopword']].sort_values('frequency', ascending=False)\n",
    "        return df.set_index('token_id')\n",
    "\n",
    "    def compute_document_stats(self):\n",
    "        id2token = self.dictionary.id2token\n",
    "        stopwords = set(self.stopwords).intersection(set(id2token.values()))\n",
    "        df = pd.DataFrame({\n",
    "            'document_id': self.corpus.index,\n",
    "            'document_name': self.corpus.document_names.document_name,\n",
    "            'treaty_id': self.corpus.document_names.treaty_id,\n",
    "            'size': [ sum(list(zip(*document))[1]) for document in self.corpus],\n",
    "            'stopwords': [ sum([ v for (i,v) in document if id2token[i] in self.stopwords]) for document in self.corpus],\n",
    "        }).set_index('document_name')\n",
    "        df[['size', 'stopwords']] = df[['size', 'stopwords']].astype('int')\n",
    "        return df\n",
    "\n",
    "    def compute_word_stats(self):\n",
    "        df = self.compute_document_stats()[['size', 'stopwords']]\n",
    "        df_agg = df.agg(['count', 'mean', 'std', 'min', 'median', 'max', 'sum']).reset_index()\n",
    "        legend_map = {\n",
    "            'count': 'Documents',\n",
    "            'mean': 'Mean words',\n",
    "            'std': 'Std',\n",
    "            'min': 'Min',\n",
    "            'median': 'Median',\n",
    "            'max': 'Max',\n",
    "            'sum': 'Sum words'\n",
    "        }\n",
    "        df_agg['index'] = df_agg['index'].apply(lambda x: legend_map[x]).astype('str')\n",
    "        df_agg = df_agg.set_index('index')\n",
    "        df_agg[df_agg.columns] = df_agg[df_agg.columns].astype('int')\n",
    "        return df_agg.reset_index()\n",
    "    \n",
    "#@staticmethod\n",
    "\n",
    "class ExtMmCorpus(gensim.corpora.MmCorpus):\n",
    "    \"\"\"Extension of MmCorpus that allow TF normalization based on document length.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def norm_tf_by_D(doc):\n",
    "        D = sum([x[1] for x in doc])\n",
    "        return doc if D == 0 else map(lambda tf: (tf[0], tf[1]/D), doc)\n",
    "\n",
    "    def __init__(self, fname):\n",
    "        gensim.corpora.MmCorpus.__init__(self, fname)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in gensim.corpora.MmCorpus.__iter__(self):\n",
    "            yield self.norm_tf_by_D(doc)\n",
    "\n",
    "    def __getitem__(self, docno):\n",
    "        return self.norm_tf_by_D(gensim.corpora.MmCorpus.__getitem__(self, docno))\n",
    "\n",
    "class TreatyCorpusSaveLoad():\n",
    "\n",
    "    def __init__(self, source_folder, lang):\n",
    "        \n",
    "        self.mm_filename = os.path.join(source_folder, 'corpus_{}.mm'.format(lang))\n",
    "        self.dict_filename = os.path.join(source_folder, 'corpus_{}.dict.gz'.format(lang))\n",
    "        self.document_index = os.path.join(source_folder, 'corpus_{}_documents.csv'.format(lang))\n",
    "        \n",
    "    def store_as_mm_corpus(self, treaty_corpus):\n",
    "        \n",
    "        gensim.corpora.MmCorpus.serialize(self.mm_filename, treaty_corpus, id2word=treaty_corpus.dictionary.id2token)\n",
    "        treaty_corpus.dictionary.save(self.dict_filename)\n",
    "        treaty_corpus.document_names.to_csv(self.document_index, sep='\\t')\n",
    "\n",
    "    def load_mm_corpus(self, normalize_by_D=False):\n",
    "    \n",
    "        corpus_type = ExtMmCorpus if normalize_by_D else gensim.corpora.MmCorpus\n",
    "        corpus = corpus_type(self.mm_filename)\n",
    "        corpus.dictionary = gensim.corpora.Dictionary.load(self.dict_filename)\n",
    "        corpus.document_names = pd.read_csv(self.document_index, sep='\\t').set_index('document_id')  \n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    def exists(self):\n",
    "        return os.path.isfile(self.mm_filename) and \\\n",
    "            os.path.isfile(self.dict_filename) and \\\n",
    "            os.path.isfile(self.document_index)\n",
    "\n",
    "def store_mm_corpora(source_path, force, languages):\n",
    "    \n",
    "    try:\n",
    "        print('Current archive:{}'.format(source_path))\n",
    "        tokenizer = nltk.tokenize.word_tokenize\n",
    "        source_folder = os.path.split(source_path)[0]\n",
    "        for language in languages.split(','):\n",
    "            loader = TreatyCorpusSaveLoad(source_folder, language)\n",
    "            if not loader.exists() or force:\n",
    "                print('Processing: {}'.format(language))\n",
    "                stream = CompressedFileReader(source_path, filename_pattern='*_{}*.txt'.format(language))\n",
    "                treaty_corpus = TreatyCorpus(stream, tokenizer=tokenizer)        \n",
    "                loader.store_as_mm_corpus(treaty_corpus)\n",
    "        print('Corpus is up-to-date!')\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "current_archives = (ls_sorted('./data/*.zip') or [])\n",
    "\n",
    "cuw = BaseWidgetUtility(\n",
    "    source_path=widgets.Dropdown(\n",
    "        options=current_archives,\n",
    "        value=current_archives[-1] if len(current_archives) else None,\n",
    "        description='Corpus:' #, **drop_style\n",
    "    ),\n",
    "    force_corpus_update=widgets.ToggleButton(\n",
    "        description='Force Update',\n",
    "        tooltip='Force refresh saved corpus cache (a performance feature). Use when ZIP-archive has been updated.',\n",
    "        value=False #, **toggle_style\n",
    "    )\n",
    ")\n",
    "\n",
    "icuw = widgets.interactive(\n",
    "    store_mm_corpora,\n",
    "    source_path=cuw.source_path,\n",
    "    force=cuw.force_corpus_update,\n",
    "    languages='en,it,fr,de'\n",
    ")\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([cuw.source_path, cuw.force_corpus_update]), icuw.children[-1]]))\n",
    "\n",
    "icuw.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all checked en/fr/de files in WTI exist in corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Basic Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4898f72f85c4d04bb0d639861b2a3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Language:', index=3, layout=Layout(width='260px'), options…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code \n",
    "\n",
    "corpus = None\n",
    "def display_token_toplist(source_folder, language, statistics='', remove_stopwords=False):\n",
    "    global tlw, corpus\n",
    "    try:\n",
    "        \n",
    "        tlw.progress.value = 1\n",
    "\n",
    "        corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0]).load_mm_corpus()\n",
    "\n",
    "        tlw.progress.value = 2\n",
    "        service = MmCorpusStatisticsService(corpus, dictionary=corpus.dictionary, language=language)\n",
    "\n",
    "        print(\"Corpus consists of {} documents, {} words in total and a vocabulary size of {} tokens.\"\\\n",
    "                  .format(len(corpus), corpus.dictionary.num_pos, len(corpus.dictionary)))\n",
    "\n",
    "        tlw.progress.value = 3\n",
    "        if statistics == 'word_freqs':\n",
    "            display(service.compute_word_frequencies(remove_stopwords))\n",
    "        elif statistics == 'documents':\n",
    "            display(service.compute_document_stats())\n",
    "        elif statistics == 'word_count':\n",
    "            display(service.compute_word_stats())\n",
    "        else:\n",
    "            print('Unknown: ' + statistics)\n",
    "            \n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    tlw.progress.value = 5\n",
    "    tlw.progress.value = 0\n",
    "    \n",
    "tlw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    statistics=widgets.Dropdown(\n",
    "        options={\n",
    "            'Word freqs': 'word_freqs',\n",
    "            'Documents': 'documents',\n",
    "            'Word count': 'word_count'\n",
    "        },\n",
    "        value='word_count',\n",
    "        description='Statistics:', **drop_style\n",
    "    ),    \n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    progress=wf.create_int_progress_widget(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itlw = widgets.interactive(\n",
    "    display_token_toplist,\n",
    "    source_folder='./data',\n",
    "    language=tlw.language,\n",
    "    statistics=tlw.statistics,\n",
    "    remove_stopwords=tlw.remove_stopwords\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        tlw.language, tlw.statistics, tlw.remove_stopwords, tlw.progress\n",
    "    ]\n",
    ")\n",
    "display(widgets.VBox([boxes, itlw.children[-1]]))\n",
    "itlw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Network Visualization of Signed Treaties\n",
    "<table>\n",
    "    <tr><th>Layout algorithm</th><th>C</th><th>K</th><th>p</th><th>Note</th></tr>\n",
    "    <tr><td>NetworkX ([fruchterman_reingold](https://networkx.github.io/documentation/networkx-1.11/reference/generated/networkx.drawing.layout.fruchterman_reingold_layout.html))</td><td></td><td></td><td>Optimal distance between nodes.</td><td>Fruchterman-Reingold force-directed algorithm.</td></tr>\n",
    "    <tr><td>NetworkX ([Spectral](https://networkx.github.io/documentation/networkx-1.11/reference/generated/networkx.drawing.nx_pylab.draw_spectral.html#networkx.drawing.nx_pylab.draw_spectral))</td><td></td><td></td><td></td><td>Position nodes using the eigenvectors of the graph Laplacian.</td></tr>\n",
    "    <tr><td>NetworkX (Circular)</td><td></td><td></td><td></td><td>Position nodes on a circle.</td></tr>\n",
    "    <tr><td>NetworkX ([Shell](https://networkx.github.io/documentation/networkx-1.11/reference/generated/networkx.drawing.nx_pylab.draw_shell.html#networkx.drawing.nx_pylab.draw_shell))</td><td></td><td></td><td></td><td>Position nodes in a bipartite graph in concentric circles.</td></tr>\n",
    "    <tr><td>NetworkX (Kamada-Kawai)</td><td></td><td></td><td></td></tr>\n",
    "    <tr><td>graph-tool ([arf](https://graph-tool.skewed.de/static/doc/draw.html#graph_tool.draw.arf_layout))</td><td>Attracting force between adjacent vertices (a).</td><td>Opposing force between vertices (d).</td><td></td></tr>\n",
    "    <tr><td>graph-tool ([sfdp](https://graph-tool.skewed.de/static/doc/draw.html#graph_tool.draw.sfdp_layout))</td><td>Relative strength of repulsive forces (C/100).</td><td>Optimal edge length.</td><td>Strength of the attractive force between connected components (gamma).</td></tr>\n",
    "    <tr><td>graph-tool ([fruchterman_reingold](https://graph-tool.skewed.de/static/doc/draw.html#graph_tool.draw.fruchterman_reingold_layout))</td><td>Repulsive force between vertices (a=2*N*K).</td><td>Attracting force between adjacent vertices (r = 2\\*C).</td><td></td><td>Fruchterman-Reingold force-directed algorithm.</td></tr>\n",
    "    <tr><td>graphviz ([neato](https://graphviz.gitlab.io/_pages/pdf/neatoguide.pdf))</td><td></td><td>K=K</td><td></td></tr>\n",
    "    <tr><td>graphviz ([dot](https://graphviz.gitlab.io/_pages/pdf/dotguide.pdf))</td><td></td><td>K=K</td><td></td></tr>\n",
    "    <tr><td>graphviz (circo)</td><td></td><td>K=K</td><td></td></tr>\n",
    "    <tr><td>graphviz (fdp)</td><td></td><td>K=K</td><td></td></tr>\n",
    "    <tr><td>graphviz (sfdp)</td><td></td><td>K=K</td><td></td></tr>\n",
    "</table>\n",
    "\n",
    "*N = Number of nodes in graph*\n",
    "        \n",
    "- **TODO: Button \"Recode 7CULT\" button**<br>\n",
    "- **TODO: Add Category and year as edge labels **<br>\n",
    "- **TODO: Split result into one graph per category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cc4b9d6e794877833f8661bc6ca9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(HBox(children=(Dropdown(description='Parties:', index=9, layout=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea9c9ed8a774c66aaeb6a6e866d9582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Label(value=''), IntRangeSlider(value=(0, 19), continuous_update=False, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize treaties\n",
    "import bokeh.palettes as pals\n",
    "import community\n",
    "import types\n",
    "\n",
    "%run ./common/network_utility\n",
    "%run ./common/plot_utility\n",
    "    \n",
    "periods_division = [\n",
    "    (1919, 1939), (1940, 1944), (1919, 1944), (1945, 1955), (1956, 1966), (1967, 1972)\n",
    "    ]\n",
    "\n",
    "label_text_opts=dict(\n",
    "    x_offset=0, #y_offset=5,\n",
    "    level='overlay',\n",
    "    text_align='center',\n",
    "    text_baseline='bottom',\n",
    "    render_mode='canvas',\n",
    "    text_font=\"Tahoma\",\n",
    "    text_font_size=\"9pt\",\n",
    "    text_color='black'\n",
    "    )\n",
    "\n",
    "network_plot_opts = dict(\n",
    "    x_axis_type=None,\n",
    "    y_axis_type=None,\n",
    "    background_fill_color='white',\n",
    "    line_opts=dict(color='green', alpha=0.5 ),\n",
    "    node_opts=dict(color=None, level='overlay', alpha=1.0),\n",
    "    )\n",
    "\n",
    "def network_edges_to_dicts(network, layout):\n",
    "    LD = [ extend(dict(source=u,target=v,xs=[layout[u][0], layout[v][0]], ys=[layout[u][1], layout[v][1]]), d) for u, v, d in G.edges(data=True) ]\n",
    "    LD.sort(key=lambda x: x['signed'])\n",
    "    edges = dict(zip(LD[0],zip(*[d.values() for d in LD])))\n",
    "    return edges\n",
    "\n",
    "def pandas_to_network_edges(data):\n",
    "    return [ (x[0], x[1], { y: x[j] for j, y in enumerate(data.columns)}) for i, x in data.iterrows() ]\n",
    "    \n",
    "def get_party_network_data(\n",
    "    parties,\n",
    "    period,\n",
    "    only_is_cultural=True,\n",
    "    party_name='party'\n",
    "    ):\n",
    "    \n",
    "    global state\n",
    "    \n",
    "    data = state.stacked_treaties.copy()\n",
    "\n",
    "    data = data.loc[(data.signed_period!='other')]\n",
    "\n",
    "    if only_is_cultural:\n",
    "        data = data.loc[(data.is_cultural==True)]\n",
    "\n",
    "    if isinstance(parties, list):\n",
    "        data = data.loc[(data.party.isin(parties))]\n",
    "    else:\n",
    "        data = data.loc[(data.reversed==False)]\n",
    "\n",
    "    data = data.loc[(data.signed_period != period)]\n",
    "    data = data.loc[(data.signed_year.between(period[0], period[1]))]\n",
    "    data = data.sort_values('signed')\n",
    "\n",
    "    # data = data.groupby(['party', 'party_other']).size().reset_index().rename(columns={0: 'weight'})\n",
    "    \n",
    "    data = data[[ 'party', 'party_other', 'signed', 'topic', 'headnote']]\n",
    "\n",
    "    if party_name != 'party':\n",
    "        for column in ['party', 'party_other']:\n",
    "            data[column] = data[column].apply(lambda x: state.get_party_name(x, party_name))\n",
    "\n",
    "    data['weight'] = 1.0\n",
    "            \n",
    "    return data\n",
    "\n",
    "def create_party_network(data, K, node_partition, palette): #, multigraph=True):\n",
    "\n",
    "    #if multigraph:\n",
    "    \n",
    "    edges_data = pandas_to_network_edges(data)\n",
    "\n",
    "    G = nx.MultiGraph(K=K)\n",
    "    G.add_edges_from(edges_data)\n",
    "    #else:\n",
    "    #    edges_data = [ tuple(x) for x in data.values ]\n",
    "    #    print(edges_data)\n",
    "    #    G = nx.Graph(K=K)\n",
    "    #    G.add_weighted_edges_from(edges_data)\n",
    "\n",
    "    if node_partition is not None:\n",
    "        partition = community.best_partition(G)\n",
    "        partition_color = { n: palette[p % len(palette)] for n, p in partition.items() }\n",
    "        nx.set_node_attributes(G, partition, 'community')\n",
    "        nx.set_node_attributes(G, partition_color, 'fill_color')\n",
    "    else:\n",
    "        #nx.set_node_attributes(G, 0, 'community')\n",
    "        nx.set_node_attributes(G, palette[0], 'fill_color')\n",
    "\n",
    "    nx.set_node_attributes(G, dict(G.degree()), name='degree')\n",
    "    nx.set_node_attributes(G, dict(nx.betweenness_centrality(G, weight=None)), name='betweenness')\n",
    "    nx.set_node_attributes(G, dict(nx.closeness_centrality(G)), name='closeness')\n",
    "    \n",
    "    # if not multigraph:\n",
    "    #    nx.set_node_attributes(G, dict(nx.eigenvector_centrality(G, weight=None)), name='eigenvector')\n",
    "        \n",
    "    # nx.set_node_attributes(G, dict(nx.communicability_betweenness_centrality(G)), name='communicability_betweenness')\n",
    "    \n",
    "    return G\n",
    "\n",
    "def setup_node_size(nodes, node_size, node_size_range):\n",
    "\n",
    "    if node_size in nodes.keys() and node_size_range is not None:\n",
    "        nodes['clamped_size'] = clamp_values(nodes[node_size], node_size_range)\n",
    "        node_size = 'clamped_size'\n",
    "    return node_size\n",
    "    \n",
    "def setup_label_y_offset(nodes, node_size):\n",
    "\n",
    "    label_y_offset = 'y_offset' if node_size in nodes.keys() else node_size + 5\n",
    "    if label_y_offset == 'y_offset':\n",
    "        nodes['y_offset'] = [ y + r for (y, r) in zip(nodes['y'], [ r / 2.0 + 5 for r in nodes[node_size] ]) ]\n",
    "    return label_y_offset\n",
    "\n",
    "def plot_party_network(\n",
    "    nodes,\n",
    "    edges,\n",
    "    node_description=None,\n",
    "    node_size=5,\n",
    "    node_opts=None,\n",
    "    line_opts=None,\n",
    "    text_opts=None,\n",
    "    element_id='nx_id3',\n",
    "    figsize=(900, 900),\n",
    "    tools=None,\n",
    "    **figkwargs\n",
    "    ):\n",
    "    \n",
    "    edges_source = bm.ColumnDataSource(edges)\n",
    "    nodes_source = bm.ColumnDataSource(nodes)\n",
    "\n",
    "    node_opts = extend(DFLT_NODE_OPTS, node_opts or {})\n",
    "    line_opts = extend(DFLT_EDGE_OPTS, line_opts or {})\n",
    "\n",
    "    p = figure(plot_width=figsize[0], plot_height=figsize[1], tools=tools or TOOLS, **figkwargs)\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    \n",
    "    if 'line_color' in edges.keys():\n",
    "        line_opts = extend(line_opts, { 'line_color': 'line_color', 'alpha': 1.0})\n",
    "\n",
    "    r_lines = p.multi_line('xs', 'ys', line_width='weight', source=edges_source, **line_opts)\n",
    "    r_nodes = p.circle('x', 'y', size=node_size, source=nodes_source, **node_opts)\n",
    "\n",
    "    if 'fill_color' in nodes.keys():\n",
    "        r_nodes.glyph.fill_color = 'fill_color'\n",
    "\n",
    "    if node_description is not None:\n",
    "        p.add_tools(bm.HoverTool(renderers=[r_nodes], tooltips=None, callback=WidgetUtility.\\\n",
    "            glyph_hover_callback(nodes_source, 'node_id', text_ids=node_description.index, \\\n",
    "                                 text=node_description, element_id=element_id))\n",
    "        )\n",
    "\n",
    "    label_opts = extend(DFLT_TEXT_OPTS, text_opts or {})\n",
    "\n",
    "    p.add_layout(bm.LabelSet(source=nodes_source, **label_opts))\n",
    "\n",
    "    # return p\n",
    "    handle = bp.show(p, notebook_handle=True)\n",
    "    return types.SimpleNamespace(\n",
    "        handle=handle,\n",
    "        edges_source=edges_source,\n",
    "        nodes_source=nodes_source,\n",
    "        nodes=nodes,\n",
    "        edges=edges,\n",
    "    )\n",
    "\n",
    "handle_data = None\n",
    "\n",
    "def display_party_network(\n",
    "    parties,\n",
    "    period,\n",
    "    only_is_cultural=True,\n",
    "    layout_algorithm='',\n",
    "    C=1.0,\n",
    "    K=0.10,\n",
    "    p1=0.10,\n",
    "    output='network_bokeh',\n",
    "    party_name='party',\n",
    "    node_size_range=[40,60],\n",
    "    refresh=False,\n",
    "    palette_name=None,\n",
    "    width=900,\n",
    "    height=900,\n",
    "    node_size=None,\n",
    "    node_partition=None,\n",
    "    weight_threshold=0.0,\n",
    "    weight_scale=1.0,\n",
    "    weight_normalize=True,\n",
    "    category_map_name=None\n",
    "    ):\n",
    "    \n",
    "    global state, zn, G, layout, handle_data\n",
    "    \n",
    "    try:\n",
    "        #multigraph = False\n",
    "        figsize=(width, height)\n",
    "        palette_id = max(pals.all_palettes[palette_name].keys())\n",
    "        palette = pals.RdYlBu[11] if palette_name is None else pals.all_palettes[palette_name][palette_id]\n",
    "\n",
    "        zn.refresh.value = False\n",
    "        zn.progress.value = 1\n",
    "        \n",
    "        data = get_party_network_data(parties, period, only_is_cultural, party_name)\n",
    "        \n",
    "        if category_map_name is not None:\n",
    "            category_map = category_group_maps[category_map_name]\n",
    "            data = data.loc[(data.topic.isin(category_map.keys()))]\n",
    "            data['category'] = data.apply(lambda x: category_map.get(x['topic'], 'OTHER'), axis=1)\n",
    "            \n",
    "            line_palette = pals.Set1[8]\n",
    "            group_keys = category_group_settings[category_map_name].keys()\n",
    "            line_palette_map = { k: i % len(line_palette) for i, k in enumerate(group_keys) }\n",
    "            # print(line_palette_map)\n",
    "            data['line_color'] = data.category.apply(lambda x: line_palette[line_palette_map[x]])\n",
    "\n",
    "        else:\n",
    "            data['category'] = data.topic\n",
    "            \n",
    "        zn.progress.value = 2\n",
    "        \n",
    "        #if not multigraph:\n",
    "        #    data = data.groupby(['party', 'party_other']).size().reset_index().rename(columns={0: 'weight'})\n",
    "        \n",
    "        G = create_party_network(data, K, node_partition, palette) #, multigraph)\n",
    "        \n",
    "        zn.progress.value = 3\n",
    "        \n",
    "        if output == 'bokeh_plot':\n",
    "            \n",
    "            if weight_threshold > 0:\n",
    "                G = filter_by_weight(G, weight_threshold)\n",
    "            \n",
    "            layout = layout_network(G, layout_algorithm, **dict(scale=1.0, K=K, C=C, p=p1))\n",
    "            zn.progress.value = 4\n",
    "            \n",
    "            edges = network_edges_to_dicts(G, layout)\n",
    "            nodes = NetworkUtility.get_node_attributes(G, layout)\n",
    "            \n",
    "            edges = { k: list(edges[k]) for k in edges }\n",
    "            nodes = { k: list(nodes[k]) for k in nodes }\n",
    "    \n",
    "            node_size = node_size if not node_size is None else node_size_range[0]\n",
    "            node_size = setup_node_size(nodes, node_size, node_size_range)\n",
    "            \n",
    "            y_offset = setup_label_y_offset(nodes, node_size)\n",
    "            text_opts = extend(label_text_opts, dict(y_offset=y_offset, x_offset=0))\n",
    "            zn.progress.value = 5 \n",
    "        \n",
    "            handle_data = plot_party_network(\n",
    "                nodes=nodes,\n",
    "                edges=edges,\n",
    "                figsize=figsize,\n",
    "                node_size=node_size,\n",
    "                text_opts=text_opts,\n",
    "                **network_plot_opts\n",
    "            )\n",
    "            \n",
    "            zn.progress.value = 6\n",
    "            zn.time_travel_range.max = len(edges['source'])\n",
    "            zn.time_travel_range.value = [0, len(edges['source'])]\n",
    "            \n",
    "            #bp.show(p)\n",
    "\n",
    "        elif output == 'table':\n",
    "            display(data)\n",
    "        else:\n",
    "            display(pivot_ui(data))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        raise\n",
    "    finally:\n",
    "        zn.progress.value = 0\n",
    "\n",
    "zn = BaseWidgetUtility(\n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '{} to {}'.format(x[0], x[1]): x for x in list(set(period_divisions[0] + period_divisions[1]))\n",
    "        },\n",
    "        value=period_divisions[0][0],\n",
    "        description='Period:', layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    category_map_name=widgets.Dropdown(\n",
    "        options=category_group_maps.keys(),\n",
    "        description='Category:', layout=widgets.Layout(width='300px')\n",
    "    ),\n",
    "    parties=widgets.Dropdown(\n",
    "        description='Parties:',\n",
    "        options=default_party_options,\n",
    "        value=default_party_options['PartyOf5'],\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    party_name=widgets.Dropdown(\n",
    "        description='Name:',\n",
    "        options={\n",
    "            'WTI Code': 'party',\n",
    "            'WTI Name': 'party_name',\n",
    "            'WTI Short': 'short_name',\n",
    "            'CC': 'country_code',\n",
    "            'Country': 'party_country'\n",
    "        },\n",
    "        value='short_name',\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    node_size=widgets.Dropdown(\n",
    "        description='Node size:',\n",
    "        options={\n",
    "            '(default)': None,\n",
    "            'Degree centrality': 'degree',\n",
    "            'Closeness centrality': 'closeness',\n",
    "            'Betweenness centrality': 'betweenness',\n",
    "            'Eigenvector centrality': 'eigenvector'            \n",
    "            #'communicability_betweenness': 'communicability_betweenness'\n",
    "        },\n",
    "        value=None,\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    palette=widgets.Dropdown(\n",
    "        description='Color:',\n",
    "        options={\n",
    "            palette_name: palette_name\n",
    "                    for palette_name in bokeh.palettes.all_palettes.keys()\n",
    "                        if any([ len(x) > 7 for x in bokeh.palettes.all_palettes[palette_name].values()])\n",
    "        },\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    C=widgets.IntSlider(\n",
    "        description='C', min=0, max=100, step=1, value=1,\n",
    "        continuous_update=False, layout=widgets.Layout(width='240px', height='30px')  # , orientation='vertical'\n",
    "    ),\n",
    "    K=widgets.FloatSlider(\n",
    "        description='K', min=0.01, max=1.0, step=0.01, value=0.10,\n",
    "        continuous_update=False, layout=widgets.Layout(width='240px', height='30px')  # , orientation='vertical'\n",
    "    ),\n",
    "    p=widgets.FloatSlider(\n",
    "        description='p', min=0.01, max=2.0, step=0.01, value=1.10,\n",
    "        continuous_update=False, layout=widgets.Layout(width='240px', height='30px')  # , orientation='vertical', \n",
    "    ),\n",
    "    node_size_range=widgets.IntRangeSlider(\n",
    "        description='Node size range',\n",
    "        value=[20, 40], min=5, max=100, step=1,\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='240px', height='30px'),  # , orientation='vertical'\n",
    "        style={'font-size': '9pt' }\n",
    "    ),\n",
    "    fig_width=widgets.IntSlider(\n",
    "        description='Width', min=600, max=1600, step=100, value=1000,\n",
    "        continuous_update=False, layout=widgets.Layout(width='240px', height='30px')  # , orientation='vertical'\n",
    "    ),\n",
    "    fig_height=widgets.IntSlider(\n",
    "        description='Height', min=600, max=1600, step=100, value=700,\n",
    "        continuous_update=False, layout=widgets.Layout(width='240px', height='30px')  # , orientation='vertical'\n",
    "    ),\n",
    "    only_is_cultural=widgets.ToggleButton(\n",
    "        description='Only Cultural', value=True,\n",
    "        tooltip='Display only \"is_cultural\" treaties', layout=widgets.Layout(width='100px')\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        description='Output:',\n",
    "        options={ 'Plot': 'bokeh_plot', 'List': 'table' }, # ,'Framework': 'framework_plot' },\n",
    "        value='bokeh_plot',\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    layout_algorithm=widgets.Dropdown(\n",
    "        description='Layout',\n",
    "        options=layout_function_name,\n",
    "        value='graphtool_sfdp',\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    progress=wf.create_int_progress_widget(min=0, max=4, step=1, value=0, layout=widgets.Layout(width=\"50%\")),\n",
    "    refresh=widgets.ToggleButton(\n",
    "        description='Refresh', value=False,\n",
    "        tooltip='Update plot', layout=widgets.Layout(width='100px')\n",
    "    ),\n",
    "    node_partition=widgets.Dropdown(\n",
    "        description='Partition:',\n",
    "        options={\n",
    "            '(default)': None,\n",
    "            'Louvain': 'louvain',\n",
    "        },\n",
    "        value=None,\n",
    "        layout=widgets.Layout(width='220px')\n",
    "    ),\n",
    "    simple_mode=widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Simple',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='150px')\n",
    "    ),\n",
    "    time_travel_range=widgets.IntRangeSlider(\n",
    "        value=[0, 100],\n",
    "        min=0,\n",
    "        max=100,\n",
    "        step=1,\n",
    "        description='Time travel',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    ),\n",
    "    time_travel_label=widgets.Label(value=\"\")\n",
    ") \n",
    "\n",
    "def on_value_change(change):\n",
    "    display_mode = 'none' if change['new'] == True else ''\n",
    "    zn.node_partition.layout.display = display_mode\n",
    "    zn.node_size.layout.display = display_mode\n",
    "    zn.node_size_range.layout.display = display_mode\n",
    "    zn.layout_algorithm.layout.display = display_mode\n",
    "    zn.C.layout.display = display_mode\n",
    "    zn.K.layout.display = display_mode\n",
    "    zn.p.layout.display = display_mode\n",
    "    zn.fig_width.layout.display = display_mode\n",
    "    zn.fig_height.layout.display = display_mode\n",
    "    zn.palette.layout.display = display_mode\n",
    "\n",
    "zn.simple_mode.observe(on_value_change, names='value')\n",
    "zn.simple_mode.value = True\n",
    "\n",
    "wn = widgets.interactive(\n",
    "    display_party_network,\n",
    "    parties=zn.parties,\n",
    "    period=zn.period,\n",
    "    only_is_cultural=zn.only_is_cultural,\n",
    "    layout_algorithm=zn.layout_algorithm,\n",
    "    C=zn.C,\n",
    "    K=zn.K,\n",
    "    p1=zn.p,\n",
    "    output=zn.output,\n",
    "    party_name=zn.party_name,\n",
    "    node_size_range=zn.node_size_range,\n",
    "    refresh=zn.refresh,\n",
    "    palette_name=zn.palette,\n",
    "    width=zn.fig_width,\n",
    "    height=zn.fig_height,\n",
    "    node_size=zn.node_size,\n",
    "    node_partition=zn.node_partition,\n",
    "    category_map_name=zn.category_map_name\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox([\n",
    "    widgets.VBox([widgets.HBox([zn.parties, zn.only_is_cultural]), widgets.HBox([zn.period, zn.refresh]),\n",
    "                  widgets.HBox([zn.category_map_name]),\n",
    "                  widgets.HBox([zn.simple_mode, zn.progress])\n",
    "                 ]),\n",
    "    widgets.VBox([zn.layout_algorithm, zn.party_name, zn.output, zn.palette]),\n",
    "    widgets.VBox([zn.K, zn.C, zn.p, zn.node_partition]),\n",
    "    widgets.VBox([zn.fig_width, zn.fig_height, zn.node_size, zn.node_size_range]),\n",
    "])\n",
    "\n",
    "display(widgets.VBox([boxes, wn.children[-1]]))\n",
    "\n",
    "wn.update()\n",
    "\n",
    "#Code\n",
    "def display_partial_party_network(range=[1,100]):\n",
    "    global handle_data    \n",
    "    edge_count = len(handle_data.edges['source'])\n",
    "    edges = { k: handle_data.edges[k][range[0]:range[1]] for k in handle_data.edges } \n",
    "    nodes_indices = set(edges['source'] + edges['target'])\n",
    "    df = pd.DataFrame({k:handle_data.nodes[k] for k in handle_data.nodes if isinstance(handle_data.nodes[k], list) }).set_index('id')\n",
    "    nodes = df.loc[nodes_indices].reset_index().to_dict(orient='list')\n",
    "    nodes = extend(nodes, {k:handle_data.nodes[k] for k in handle_data.nodes if not isinstance(handle_data.nodes[k], list) })\n",
    "    handle_data.edges_source.data.update(edges)    \n",
    "    handle_data.nodes_source.data.update(nodes)\n",
    "    min_year, max_year = min(handle_data.edges_source.data['signed']).year, max(handle_data.edges_source.data['signed']).year\n",
    "    zn.time_travel_range.description = '{}-{}'.format(min_year, max_year)\n",
    "    bokeh.io.push_notebook(handle=handle_data.handle)\n",
    "    \n",
    "iw_time_travel = widgets.interactive(\n",
    "    display_partial_party_network,\n",
    "    range=zn.time_travel_range\n",
    ")\n",
    "time_travel_box = widgets.VBox([widgets.VBox([zn.time_travel_label, zn.time_travel_range]), iw_time_travel.children[-1]])\n",
    "\n",
    "display(time_travel_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Treaty Keyword Extraction (using TF-IDF weighing)\n",
    "- [ML Wiki.org](http://mlwiki.org/index.php/TF-IDF)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- Spärck Jones, K. (1972). \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\".\n",
    "- Manning, C.D.; Raghavan, P.; Schutze, H. (2008). \"Scoring, term weighting, and the vector space model\". ([PDF](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf))\n",
    "- https://markroxor.github.io/blog/tfidf-pivoted_norm/\n",
    "$\\frac{tf-idf}{\\sqrt(rowSums( tf-idf^2 ) )}$\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/pivoted-normalized-document-length-1.html\n",
    "\n",
    "Neural Network Methods in Natural Language Processing, Yoav Goldberg:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dfcda5a6bb4909b15496f93d10b17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Dropdown(description='Language:', index=3, layout=Layout(width='2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code\n",
    "from scipy.sparse import csr_matrix\n",
    "%timeit\n",
    "\n",
    "    \n",
    "def get_top_tfidf_words(data, n_top=5):\n",
    "    top_list = data.groupby(['treaty_id'])\\\n",
    "        .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "        .reset_index(level=0, drop=True)\n",
    "    return top_list\n",
    "\n",
    "def compute_tfidf_scores(corpus, dictionary, smartirs='ntc'):\n",
    "    #model = gensim.models.logentropy_model.LogEntropyModel(corpus, normalize=True)\n",
    "    model = gensim.models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary, normalize=True) #, smartirs=smartirs)\n",
    "    rows, cols, scores = [], [], []\n",
    "    for r, document in enumerate(corpus): \n",
    "        vector = model[document]\n",
    "        c, v = zip(*vector)\n",
    "        rows += (len(c) * [ int(r) ])\n",
    "        cols += c\n",
    "        scores += v\n",
    "        \n",
    "    return csr_matrix((scores, (rows, cols)))\n",
    "    \n",
    "if True: #'tfidf_cache' not in globals():\n",
    "    tfidf_cache = {\n",
    "    }\n",
    "    \n",
    "def display_tfidf_scores(source_folder, language, period, n_top=5, threshold=0.001):\n",
    "    \n",
    "    global state, tfw, tfidf_cache\n",
    "    \n",
    "    try:\n",
    "        treaties = state.treaties\n",
    "\n",
    "        tfw.progress.value = 0\n",
    "        tfw.progress.value += 1\n",
    "        if language[0] not in tfidf_cache.keys():\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0])\\\n",
    "                .load_mm_corpus(normalize_by_D=True)\n",
    "            document_names = corpus.document_names\n",
    "            dictionary = corpus.dictionary\n",
    "            _ = dictionary[0]\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            A = compute_tfidf_scores(corpus, dictionary)\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            scores = pd.DataFrame(\n",
    "                [ (i, j, dictionary.id2token[j], A[i, j]) for i, j in zip(*A.nonzero())],\n",
    "                columns=['document_id', 'token_id', 'token', 'score']\n",
    "            )\n",
    "            tfw.progress.value += 1\n",
    "            scores = scores.merge(document_names, how='inner', left_on='document_id', right_index=True)\\\n",
    "                .drop(['document_id', 'token_id', 'document_name'], axis=1)\n",
    "\n",
    "            scores = scores[['treaty_id', 'token', 'score']]\\\n",
    "                .sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "            tfidf_cache[language[0]] = scores\n",
    "\n",
    "        scores = tfidf_cache[language[0]]\n",
    "        if threshold > 0:\n",
    "            scores = scores.loc[scores.score >= threshold]\n",
    "\n",
    "        tfw.progress.value += 1\n",
    "\n",
    "        #scores = get_top_tfidf_words(scores, n_top=5)\n",
    "        #scores = scores.groupby(['treaty_id']).sum() \n",
    "\n",
    "        scores = scores.groupby(['treaty_id'])\\\n",
    "            .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            .set_index('treaty_id')\n",
    "\n",
    "        if period is not None:\n",
    "            periods = state.treaties[period]\n",
    "            scores = scores.merge(periods.to_frame(), left_index=True, right_index=True, how='inner')\\\n",
    "                .groupby([period, 'token']).score.agg([np.mean])\\\n",
    "                .reset_index().rename(columns={0:'score'}) #.sort_values('token')\n",
    "\n",
    "        #['token'].apply(' '.join)\n",
    "\n",
    "        print(scores)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        raise\n",
    "        \n",
    "    tfw.progress.value = 0\n",
    "\n",
    "#if 'tfidf_scores' not in globals():\n",
    "#    tfidf_scores = compute_document_tfidf(corpus, corpus.dictionary, state.treaties)\n",
    "#    tfidf_scores = tfidf_scores.sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "tfw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    n_top=widgets.IntSlider(\n",
    "        value=5, min=1, max=25, step=1,\n",
    "        description='Top #:',\n",
    "        continuous_update=False\n",
    "    ),\n",
    "    threshold=widgets.FloatSlider(\n",
    "        value=0.001, min=0.0, max=0.5, step=0.01,\n",
    "        description='Threshold:',\n",
    "        tooltip='Word having a TF-IDF score below this value is filtered out',\n",
    "        continuous_update=False,\n",
    "        readout_format='.3f',\n",
    "    ), \n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Output:', **drop_style\n",
    "    ),\n",
    "    progress=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itfw = widgets.interactive(\n",
    "    display_tfidf_scores,\n",
    "    source_folder='./data',\n",
    "    language=tfw.language,\n",
    "    n_top=tfw.n_top,\n",
    "    threshold=tfw.threshold,\n",
    "    period=tfw.period\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([tfw.language, tfw.period]),\n",
    "        widgets.VBox([tfw.n_top, tfw.threshold]),\n",
    "        widgets.VBox([tfw.progress, tfw.output])\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(widgets.VBox([boxes, itfw.children[-1]]))\n",
    "itfw.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red'>IGNORE EVERYTHING BELOW</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network info and statistics\n",
    "\n",
    "__START HERE__\n",
    "\n",
    "[set_node_attributes](https://networkx.github.io/documentation/stable/reference/generated/networkx.classes.function.set_node_attributes.html#networkx.classes.function.set_node_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Code\n",
    "def display_partial_party_network(position=1):\n",
    "    global handle_data\n",
    "    edge_count = len(handle_data.edges['source'])\n",
    "    edges = { k: handle_data.edges[k][:position] for k in handle_data.edges } \n",
    "    nodes_indices = set(edges['source'] + edges['target'])\n",
    "    df = pd.DataFrame({k:handle_data.nodes[k] for k in handle_data.nodes if isinstance(handle_data.nodes[k], list) }).set_index('id')\n",
    "    nodes = df.loc[nodes_indices].reset_index().to_dict(orient='list')\n",
    "    nodes = extend(nodes, {k:handle_data.nodes[k] for k in handle_data.nodes if not isinstance(handle_data.nodes[k], list) })\n",
    "    handle_data.edges_source.data.update(edges)\n",
    "    handle_data.nodes_source.data.update(nodes)\n",
    "    bokeh.io.push_notebook(handle=handle_data.handle)\n",
    "    \n",
    "iw_time_travel = widgets.interactive(\n",
    "    display_partial_party_network,\n",
    "    position=zn.time_travel_position\n",
    ")\n",
    "time_travel_box = widgets.VBox([zn.time_travel_position, iw_time_travel.children[-1]])\n",
    "\n",
    "display(time_travel_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class CoOccurrance():\n",
    "\n",
    "    def __init__(self, tokenizer, stopwords=None, lemmatizer=None, min_word_size=2):\n",
    "        \n",
    "        self.transforms = [\n",
    "            tokenizer,\n",
    "            lambda ws: ( x for x in ws if len(x) >= min_word_size ),\n",
    "            lambda ws: ( x for x in ws if any(ch.isalpha() for ch in x)) \n",
    "        ]\n",
    "        \n",
    "        if stopwords is not None:\n",
    "            self.transforms += [ lambda ws: ( x for x in ws if x not in stopwords ) ]\n",
    "            \n",
    "        if lemmatizer is not None:\n",
    "            self.transforms += [ lambda ws: ( lemmatizer(x) for x in ws ) ]\n",
    "\n",
    "    def _apply_transforms(self, ws):\n",
    "        for f in self.transforms:\n",
    "            ws = f(ws)\n",
    "        return list(ws)\n",
    "    \n",
    "    def compute(self, headnotes):\n",
    "        \n",
    "        texts = [ x.lower() for x in list(headnotes) ]\n",
    "        tokens = list(map(self._apply_transforms, texts))\n",
    "        df = pd.DataFrame({'headnote': headnotes, 'tokens': tokens })\n",
    "        \n",
    "        df_stacked = pd.DataFrame(df.tokens.tolist(), index=df.index).stack()\\\n",
    "            .reset_index().rename(columns={'level_1': 'sequence_id', 0: 'token'})\n",
    "            \n",
    "        return df_stacked\n",
    "    \n",
    "    def compute_co_occurrence(self, treaties, pos_tags, only_cultural_treaties=False):\n",
    "\n",
    "        # Filter out tags based on treaties of interest\n",
    "        pos_tags = pos_tags.merge(treaties, how='inner', left_on='treaty_id', right_index=True)[[]]\n",
    "        \n",
    "        if only_cultural_treaties:\n",
    "            df_pos_tags = df_pos_tags[(df_pos_tags.is_cultural.str.contains('yes',na=False))]\n",
    "\n",
    "        # Self join of words within same treaty\n",
    "        df_co_occurrence = pd.merge(df_pos_tags, df_pos_tags, how='inner', left_on='treaty_id', right_on='treaty_id')\n",
    "        # Only consider a specific poir once\n",
    "        df_co_occurrence = df_co_occurrence[(df_co_occurrence.wid_x < df_co_occurrence.wid_y)]\n",
    "        # Reduce number of returned columns\n",
    "        df_co_occurrence = df_co_occurrence[['treaty_id', 'year_x', 'is_cultural_x', 'lemma_x', 'lemma_y' ]]\n",
    "        # Rename columns\n",
    "        df_co_occurrence.columns = ['treaty_id', 'year', 'is_cultural', 'lemma_x', 'lemma_y' ]\n",
    "\n",
    "        # Sort token pair so smallest always comes first\n",
    "        lemma_x = df_co_occurrence[['lemma_x', 'lemma_y']].min(axis=1)\n",
    "        lemma_y = df_co_occurrence[['lemma_x', 'lemma_y']].max(axis=1)\n",
    "        df_co_occurrence['lemma_x'] = lemma_x\n",
    "        df_co_occurrence['lemma_y'] = lemma_y\n",
    "\n",
    "        return df_co_occurrence\n",
    "    \n",
    "def create_bigram_transformer(documents):\n",
    "    import gensim.models.phrases\n",
    "    bigram = gensim.models.phrases.Phrases(map(nltk.tokenize.word_tokenize, documents))\n",
    "    return lambda ws: bigram[ws]\n",
    "\n",
    "treaties = state.treaties.loc[(state.treaties.is_cultural)]\n",
    "headnotes = treaties['headnote']\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokenizer = nltk.tokenize.word_tokenize\n",
    "lemmatizer = WordNetLemmatizer().lemmatize\n",
    "df = CoOccurrance(tokenizer=tokenizer, stopwords=stopwords, lemmatizer=lemmatizer, min_word_size=2).compute(headnotes)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
