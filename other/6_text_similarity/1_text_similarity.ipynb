{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations\n",
    "\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "model_id": "46753cd8b144437b90fa59db54b14949",
       "version_minor": 0
      },
      "text/plain": "VBox(children=(Dropdown(description='Load index', layout=Layout(width='300px'), options=(('WTI 7CULT', 'is_cul…"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": "\n    <div class=\"bk-root\">\n        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n        <span id=\"1001\">Loading BokehJS ...</span>\n    </div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  var JS_MIME_TYPE = 'application/javascript';\n  var HTML_MIME_TYPE = 'text/html';\n  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  var CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    var script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    var cell = handle.cell;\n\n    var id = cell.output_area._bokeh_element_id;\n    var server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd, {\n        iopub: {\n          output: function(msg) {\n            var id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    var output_area = handle.output_area;\n    var output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n      return\n    }\n\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      var bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      var script_attrs = bk_div.children[0].attributes;\n      for (var i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      var toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "def project_root():\n",
    "    folder = os.getcwd()\n",
    "    while not os.path.exists(os.path.join(folder, \"common\")):\n",
    "        folder, _ = os.path.split(folder)\n",
    "    return folder\n",
    "\n",
    "sys.path.append(project_root())\n",
    "sys.path.append(os.path.join(project_root(), \"3_text_analysis\"))\n",
    "\n",
    "import glob, re\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import common.utility as utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import common.config as config\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import treaty_corpus\n",
    "\n",
    "# from beakerx.object import beakerx\n",
    "# from beakerx import *\n",
    "\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "#import pyarrow.parquet as pq\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "DF_TAGSET = pd.read_csv(os.path.join(config.DATA_FOLDER, 'tagset.csv'), sep='\\t').fillna('')\n",
    "treaty_repository.load_wti_index_with_gui(data_folder=config.DATA_FOLDER)\n",
    "TREATY_TIME_GROUPINGS = treaty_repository.current_wti_index().get_treaty_time_groupings()\n",
    "GPE_FILENAME = os.path.join(config.DATA_FOLDER, 'gpe_substitutions.txt')\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n",
    "\n",
    "from bokeh.plotting import output_notebook, show\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "model_id": "25f76ceb7dd845ceaacc3bded8c24627",
       "version_minor": 0
      },
      "text/plain": "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(Dropdown(description='C…"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import textacy_corpus_gui\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(config.DATA_FOLDER, treaty_repository.current_wti_index(), container)\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Compute Similarity Scores <span style='float: right; color: red'>MANDATORY</span>\n",
    "\n",
    "#### gensim LSI or TF-IDF Document (BoW) Similarity\n",
    "See https://radimrehurek.com/gensim/tut3.html and “Indexing by Latent Semantic Analysis, Deerwester et al. (1990)”.\n",
    "Compute cosine similarity between documents based on LSI or TF-IDF models.\n",
    "\n",
    "#### gensim LDA Document Similarity\n",
    "Cosine similarity on document-topic assignments from (gensim) LDA topic models.\n",
    "\n",
    "#### sklearn TF-IDF Document Similarity\n",
    "Cosine similarity on sklearn TF-IDF models.\n",
    "\n",
    "#### biopython Alignment Scores Document Similarity\n",
    "See [Biopython](https://biopython.org/). GLobal sequence alignment using specifically [PairwiseAligner](http://biopython.org/DIST/docs/api/). The biopython package has many other sequence alignment packages. Note that the alignment is _character_ based, which might not be optimal for text similarity.\n",
    "\n",
    "Since this is a very time-consuming computational method, the text are uppercased, and characters not in a predefined alphabet are filtered out. This alphabet consists of letters A-Z but can be configured. Note that all other characters are stripped away, including whitespaces etc. To limit the search space, there is an option to first compute LSI similarity, and then only tries to align the n top n most similar document based on this metric.\n",
    "\n",
    "From [documentation](http://biopython.org/DIST/docs/api/): \"Based on the values of the gap scores, a PairwiseAligner object automatically chooses the appropriate alignment algorithm (the Needleman-Wunsch, Smith-Waterman, Gotoh, or Waterman-Smith-Beyer global or local alignment algorithm).\"\n",
    "\n",
    "#### {NOT IMPLEMENTED} SIMD Smith-Waterman Alignment Scores Document Similarity\n",
    "An SIMD Smith-Waterman C/C++/Python/Java Library for Use in Genomic Applications\n",
    "\n",
    "- [libssw](https://bioconda.github.io/recipes/libssw/README.html)\n",
    "- [github](https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) Citation: https://doi.org/10.1371/journal.pone.0082138\n",
    "\n",
    "#### {NOT IMPLEMENTED} VT-PASSIM\n",
    "See separate tests.\n",
    "\n",
    "#### Python \"python-alignment\" PACKAGE\n",
    "https://github.com/eseraygun/python-alignment\n",
    "```pip install alignment```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "model_id": "07ba4d5950a44cdf8973d49a350c9b26",
       "version_minor": 0
      },
      "text/plain": "HBox(children=(Dropdown(description='Model', index=2, options=(('bag-of-word', 'gensim_bow'), ('Jaccard', 'jac…"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-02 14:28:32,126 : INFO : generic_process_corpus_gui.py.process_corpus_gui() : ...loading term substitution mappings...\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "model_id": "71ecb91d923345c1899fdd7c6b515867",
       "version_minor": 0
      },
      "text/plain": "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(HBox(chi…"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import string, heapq\n",
    "import gensim\n",
    "import generic_process_corpus_gui\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "#from Bio import Align\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "clocks = None\n",
    "def plot_histogram(ax, values, bins=20, **kwargs):\n",
    "    global clocks\n",
    "    clocks = locals()\n",
    "    #subplot = kwargs.get('subplot', None)\n",
    "    figsize = kwargs.get('figsize', None)\n",
    "    title = kwargs.get('title', None)\n",
    "    xlabel, ylabel = kwargs.get('xlabel', None), kwargs.get('ylabel', None)\n",
    "    \n",
    "    avg_length  = sum(values) / float(len(values))\n",
    "\n",
    "    #if subplot is not None:\n",
    "    #    plt.subplot(*subplot)\n",
    "    \n",
    "    #if figsize is not None:\n",
    "    #    plt.rc('figure', figsize=figsize)\n",
    "        \n",
    "    ##plt.rc('font', size=12)\n",
    "    #plt.rc('lines', linewidth=2)\n",
    "\n",
    "    ax.hist(values, bins=bins)\n",
    "    ax.axvline(avg_length, color='#e41a1c')\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "        \n",
    "    if xlabel is not None:\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "    #ax.text(100, 50, 'avg = %.2f' % avg_length)\n",
    "    \n",
    "__current_similarity_model = None\n",
    "def current_similarity_model():\n",
    "    assert __current_similarity_model is not None, 'Please create a similarity model first'\n",
    "    return __current_similarity_model\n",
    "\n",
    "class IdentityModel():\n",
    "    def __getitem__(self, key):\n",
    "        return key\n",
    "    \n",
    "def compute_gensim_similarity_scores(model_type, terms, process_opts, n_features=25):\n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary(terms)\n",
    "    bow_corpus = [ dictionary.doc2bow(tokens) for tokens in terms ]\n",
    "    \n",
    "    if model_type == \"lsi\":\n",
    "        similarity_model  = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=n_features)\n",
    "    elif model_type == \"lda\":\n",
    "        similarity_model  = gensim.models.LdaModel(\n",
    "            bow_corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=n_features,\n",
    "            eval_every=1,\n",
    "            iterations=2000,\n",
    "            passes=4,\n",
    "            alpha='auto'\n",
    "        )\n",
    "    elif model_type == \"bow\":\n",
    "        similarity_model  = IdentityModel()\n",
    "    elif model_type == \"tfidf\":\n",
    "        similarity_model  = gensim.models.TfidfModel(bow_corpus)\n",
    "    else:\n",
    "        assert False, 'Unknown model type'\n",
    "\n",
    "    model_corpus      = similarity_model[bow_corpus]\n",
    "    similarity_index  = gensim.similarities.MatrixSimilarity(model_corpus, corpus_len=len(bow_corpus))\n",
    "    model_scores      = list(enumerate(similarity_index[model_corpus]))\n",
    "    \n",
    "    similarity_scores = [ (i, j, score) for i, scores in model_scores for (j, score) in enumerate(scores) if i < j]\n",
    "    return similarity_scores\n",
    "\n",
    "def get_top_candidates_by_lsi_scores(terms, process_opts, n_largest=500, n_features=25, p_threshold=0.90):\n",
    "    lsi_scores = compute_gensim_similarity_scores('lsi', terms, process_opts, n_features=n_features)\n",
    "    candidates = [ (i, j, score) for i, j, score in lsi_scores if score >= p_threshold]\n",
    "    candidates = heapq.nlargest(n_largest, candidates, key=lambda x: x[2])\n",
    "    return ( (i,j) for i, j, _ in candidates )\n",
    "\n",
    "def compute_tfidf_similarity_scores(model_type, terms, process_opts, n_features=25):\n",
    "    \n",
    "    texts = [ ' '.join(list(x)) for x in terms ]\n",
    "    tfidf = TfidfVectorizer().fit_transform(texts)\n",
    "    m = (tfidf * tfidf.T).tocoo() \n",
    "    \n",
    "    similarity_scores = list(( (i, j, score) for i, j, score in zip(*(m.row, m.col, m.data)) if i < j ))\n",
    "    return similarity_scores\n",
    "\n",
    "def terms_to_text(terms, alphabet=string.ascii_uppercase):\n",
    "    text = ''.join(terms).upper()\n",
    "    return ''.join(c for c in text if c in alphabet)\n",
    "\n",
    "def compute_biopython_alignment_score(model_type, terms, process_opts, threshold=0.5, n_lsi_largest=None):\n",
    "    \n",
    "    tick           = process_opts.get('tick', utility.noop)\n",
    "    match_score    = process_opts.get('match_score', 1)\n",
    "    mismatch_score = process_opts.get('mismatch_score', -1)\n",
    "    alphabet       = string.ascii_uppercase # 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    \n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.mode = 'global'\n",
    "    aligner.match = match_score\n",
    "    aligner.mismatch  = mismatch_score\n",
    "    \n",
    "    terms_corpus  = [ terms_to_text(doc, alphabet) for doc in terms ]\n",
    "    \n",
    "    if n_lsi_largest is not None:\n",
    "        candidates = get_top_candidates_by_lsi_scores(terms, process_opts, n_largest=n_lsi_largest, n_features=25, p_threshold=threshold)\n",
    "        n_candidates = n_lsi_largest\n",
    "    else:\n",
    "        corpus_length = len(terms_corpus)\n",
    "        candidates = itertools.combinations(range(corpus_length), r=2)\n",
    "        n_candidates = (corpus_length ** 2) / 2 - corpus_length\n",
    "\n",
    "    tick(0, n_candidates)\n",
    "    \n",
    "    for i, j in candidates:\n",
    "        score = aligner.score(terms_corpus[i], terms_corpus[j]) / max(len(terms_corpus[i]), len(terms_corpus[j]))\n",
    "        tick()\n",
    "        if score >= threshold:\n",
    "            yield (i, j, score)\n",
    "\n",
    "    tick(0)\n",
    "    \n",
    "def compute_leebird_alignment_score(model_type, terms, process_opts, threshold=0.5):\n",
    "    \n",
    "    from similarity.alignment.alignment import Needleman, Hirschberg, SegmentAlignment\n",
    "    \n",
    "    model_types = {\n",
    "        \"needleman\": Needleman,\n",
    "        \"hirschberg\": Hirschberg,\n",
    "        \"segmentalignment\": SegmentAlignment\n",
    "    }\n",
    "    \n",
    "    aligner = model_types[model_type]()\n",
    "    \n",
    "    if n_lsi_largest is not None:\n",
    "        candidates = get_top_candidates_by_lsi_scores(terms, process_opts, n_largest=n_lsi_largest, n_features=25, p_threshold=threshold)\n",
    "        n_candidates = n_lsi_largest\n",
    "    else:\n",
    "        corpus_length = len(terms)\n",
    "        candidates = itertools.combinations(range(corpus_length), r=2)\n",
    "        n_candidates = (corpus_length ** 2) / 2 - corpus_length\n",
    "\n",
    "    for i, j in candidates:\n",
    "        score = aligner.score(terms_corpus[i], terms_corpus[j]) / max(len(terms_corpus[i]), len(terms_corpus[j]))\n",
    "        if score >= threshold:\n",
    "            yield (i, j, score)\n",
    "\n",
    "def compute_python_alignment_score(model_type, terms, process_opts, threshold=0.7, n_lsi_largest=500):\n",
    "\n",
    "    from alignment.sequence import Sequence\n",
    "    from alignment.vocabulary import Vocabulary\n",
    "    from alignment.sequencealigner import SimpleScoring, GlobalSequenceAligner\n",
    "\n",
    "    tick           = process_opts.get('tick', utility.noop)\n",
    "    match_score    = process_opts.get('match_score', 1)\n",
    "    mismatch_score = process_opts.get('mismatch_score', -1)\n",
    "    gap_score      = process_opts.get('gap_score', -1)\n",
    "\n",
    "    #alphabet=string.ascii_uppercase + ' '\n",
    "    #[a-zA-Z]+|[0-9]+|\\s+|[.,;!\\(\\)]+\n",
    "    #terms_corpus = [ terms_to_text(doc, alphabet=string.ascii_uppercase) for doc in terms ]\n",
    "\n",
    "    terms_corpus = terms\n",
    "    \n",
    "    candidates = [ (i,j) for i,j in get_top_candidates_by_lsi_scores(terms, process_opts, n_largest=n_lsi_largest, n_features=25, p_threshold=threshold) ]\n",
    "\n",
    "    v = Vocabulary()\n",
    "    \n",
    "    scoring = SimpleScoring(match_score, mismatch_score)\n",
    "    aligner = GlobalSequenceAligner(scoring, gap_score)\n",
    "    \n",
    "    tick(0, n_lsi_largest)\n",
    "    \n",
    "    if False:\n",
    "        \n",
    "        from joblib import Parallel, delayed        \n",
    "        \n",
    "        def align(i, j):\n",
    "\n",
    "            a = Sequence(terms_corpus[i])\n",
    "            b = Sequence(terms_corpus[j])\n",
    "\n",
    "            a_encoded, b_encoded = v.encodeSequence(a), v.encodeSequence(b)\n",
    "            score = aligner.align(a_encoded, b_encoded, backtrace=False) / max(len(terms_corpus[i]), len(terms_corpus[j]))\n",
    "            tick()\n",
    "            return (i, j, score)\n",
    "        \n",
    "        data = Parallel(n_jobs=4, prefer=\"threads\")(delayed(align)(i,j) for i,j in candidates)\n",
    "        return data\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        for i, j in candidates:\n",
    "\n",
    "            a = Sequence(terms_corpus[i])\n",
    "            b = Sequence(terms_corpus[j])\n",
    "\n",
    "            a_encoded, b_encoded = v.encodeSequence(a), v.encodeSequence(b)\n",
    "            score = aligner.align(a_encoded, b_encoded, backtrace=False) / max(len(terms_corpus[i]), len(terms_corpus[j]))\n",
    "\n",
    "            tick()\n",
    "\n",
    "            if score >= threshold:\n",
    "                yield (i, j, score)\n",
    "            \n",
    "    tick(0)\n",
    "    \n",
    "def compute_jaccard_similarity_scores(model_type, terms, process_opts, threshold=0.7, n_largest=1000):\n",
    "    \n",
    "    def jaccard(s1, s2):\n",
    "        return len(s1 & s2) / len(s1 | s2)\n",
    "    \n",
    "    sets = [ set(x) for x in terms ]\n",
    "    ij_s = itertools.combinations(range(len(terms)), r=2)\n",
    "    \n",
    "    scores = ( (i, j, jaccard(sets[i], sets[j])) for i, j in ij_s)\n",
    "    \n",
    "    return heapq.nlargest(n_largest, (x for x in scores if x[2] >= threshold), key=lambda x: x[2])\n",
    "\n",
    "#    for i, j in itertools.combinations(range(terms), r=2):\n",
    "#        score = len(sets[i] & sets[j]) / len(sets[i] | sets[j])\n",
    "#        if score >= threshold:\n",
    "#            yield (i, j, len(sets[i] & sets[j]) / len(sets[i] | sets[j]))\n",
    "\n",
    "def compute_wmd_word2vec_similarity_scores(model_type, terms, process_opts, threshold=0.7, n_largest=1000):\n",
    "    \n",
    "    if model_type == 'googlenews':\n",
    "        model_path = os.path.join(DATA_FOLDER, 'GoogleNews-vectors-negative300.bin.gz')\n",
    "        if not os.path.exists(model_path):\n",
    "            raise ValueError(\"SKIP: Model file {} not found\".format(model_path))\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "    else:\n",
    "        model = gensim.models.Word2Vec(terms, workers=3, size=100)\n",
    "    \n",
    "    ij_s = itertools.combinations(range(len(terms)), r=2)\n",
    "    \n",
    "    scores = ( (i, j, model.wmdistance(terms[i], terms[j])) for i, j in ij_s)\n",
    "    \n",
    "    return heapq.nlargest(n_largest, (x for x in scores if x[2] >= threshold), key=lambda x: x[2])\n",
    "\n",
    "#    for i, j in itertools.combinations(range(terms), r=2):\n",
    "#        score = len(sets[i] & sets[j]) / len(sets[i] | sets[j])\n",
    "#        if score >= threshold:\n",
    "#            yield (i, j, len(sets[i] & sets[j]) / len(sets[i] | sets[j]))\n",
    "        \n",
    "    \n",
    "def compute_similarity_scores(corpus, process_opts, extract_args):\n",
    "    \n",
    "    global __current_similarity_model\n",
    "    \n",
    "    tick = process_opts.get('tick', utility.noop)\n",
    "    \n",
    "    tick(1)\n",
    "    \n",
    "    n_features = process_opts.get('n_features').value\n",
    "    model_type = process_opts.get('model_type').value\n",
    "    \n",
    "    [engine, model_type] = model_type.split('_')\n",
    "    \n",
    "    terms = [ list(doc) for doc in textacy_utility.extract_corpus_terms(corpus, extract_args) ]\n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary(terms)\n",
    "    bow_corpus = [ dictionary.doc2bow(tokens) for tokens in terms ]\n",
    "    \n",
    "    tick()\n",
    "    \n",
    "    if engine == 'gensim':\n",
    "        \n",
    "        similarity_scores = compute_gensim_similarity_scores(model_type, terms, process_opts, n_features=n_features)\n",
    "        \n",
    "    elif engine == 'sklearn':\n",
    "        \n",
    "        similarity_scores = compute_tfidf_similarity_scores(model_type, terms, process_opts)\n",
    "\n",
    "    elif engine == 'biopython':\n",
    "        \n",
    "        similarity_scores = list(compute_biopython_alignment_score(model_type, terms, process_opts, n_lsi_largest=None))\n",
    "\n",
    "    elif engine == 'alignment':\n",
    "        \n",
    "        similarity_scores = list(compute_python_alignment_score(model_type, terms, process_opts, n_lsi_largest=None))\n",
    "        \n",
    "    elif engine == 'jaccard':\n",
    "        \n",
    "        similarity_scores = list(compute_jaccard_similarity_scores(model_type, terms, process_opts, threshold=0.7, n_largest=1000))\n",
    "        \n",
    "    elif engine == 'wmd':\n",
    "        \n",
    "        similarity_scores = list(compute_wmd_word2vec_similarity_scores(model_type, terms, process_opts, threshold=0.7, n_largest=1000))\n",
    "        \n",
    "    else:\n",
    "        assert False, 'Unknown engine'\n",
    "\n",
    "    tick()\n",
    "        \n",
    "    #total_scores = [ ]\n",
    "    #for i in range(1, len(corpus)-1):\n",
    "    #    lsi_document = lsi_model[bow_corpus[i]]\n",
    "    #    scores = list(enumerate(lsi_index[lsi_document]))\n",
    "    #    total_scores += [ (i, j, w) for (j, w) in scores if i < j ]\n",
    "        \n",
    "    df = pd.DataFrame(similarity_scores, columns=['doc_id1', 'doc_id2', 'score'])\n",
    "    \n",
    "    tick()\n",
    "    \n",
    "    def get_parties(index):\n",
    "        metadata = corpus[index].metadata\n",
    "        return '{} vs {}'.format(metadata['party1'], metadata['party2'])\n",
    "    \n",
    "    df['treaty_id1'] = df.doc_id1.apply(lambda x: corpus[x].metadata['treaty_id'])\n",
    "    df['year1'] = df.doc_id1.apply(lambda x: corpus[x].metadata['signed_year'])\n",
    "    df['parties1'] = df.doc_id1.apply(get_parties)\n",
    "    \n",
    "    df['treaty_id2'] = df.doc_id2.apply(lambda x: corpus[x].metadata['treaty_id'])\n",
    "    df['year2'] = df.doc_id2.apply(lambda x: corpus[x].metadata['signed_year'])\n",
    "    df['parties2'] = df.doc_id2.apply(get_parties)\n",
    "    \n",
    "    df['score%'] = df.score.apply(lambda x: int(x*100))\n",
    "    df['score1K'] = df.score.apply(lambda x: int(x*1000))\n",
    "    \n",
    "    __current_similarity_model = df.sort_values(['score'], ascending=False)\n",
    "    \n",
    "    #current_similarity_model().groupby('score%').size().plot()\n",
    "    tick()\n",
    "    \n",
    "    process_opts.get('gui').output.clear_output()\n",
    "\n",
    "    with process_opts.get('gui').output:\n",
    "        \n",
    "        if process_opts.get('output_type').value == 'table':\n",
    "            display(df.head(1000))\n",
    "        else:\n",
    "            \n",
    "            f, axs = plt.subplots(1,3,figsize=(16,4))\n",
    "            # plt.rc('figure', figsize=(9,4))\n",
    "            \n",
    "            #df.groupby(['score%']).size().plot(kind='line')\n",
    "            plot_histogram(axs[0], list(df['score%'].values), bins=100, figsize=None, subplot=None, title='Score distribution')\n",
    "            \n",
    "            values = [ len(doc) for doc in corpus ]\n",
    "            plot_histogram(axs[1], values, bins=100, figsize=None, subplot=None, title='Corpus doc lengths')\n",
    "            \n",
    "            values = [ len(doc) for doc in terms ]\n",
    "            plot_histogram(axs[2], values, bins=100, figsize=None, subplot=None, title='Reduced doc lengths')\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "    \n",
    "    tick(0)\n",
    "try:\n",
    "    model_options = [\n",
    "        ('bag-of-word', 'gensim_bow'),\n",
    "        ('Jaccard', 'jaccard_simple'),\n",
    "        ('gensim LSI', 'gensim_lsi'),\n",
    "        ('gensim TF-IDF', 'gensim_tfidf'),\n",
    "        ('gensim LDA', 'gensim_lda'),\n",
    "        ('biopython Pairwise (slow)', 'biopython_pairwise-score'),\n",
    "        ('sklearn TF-IDF', 'sklearn_tfidf'),\n",
    "        ('alignment PACKAGE (slow)', 'alignment_GlobalSequenceAligner'),\n",
    "        ('WMD GoogleNews (slow)', 'wmd_googlenews'),\n",
    "        ('WMD Corpus (slow)', 'wmd_corpus'),\n",
    "    ]\n",
    "    output_options = [\n",
    "        ('Network', 'network'),\n",
    "        ('Table', 'table')\n",
    "    ]\n",
    "    n_features = widgets.IntSlider(description='Features', min=2, max=30, value=10)\n",
    "    model_type = widgets.Dropdown(description='Model', options=model_options, value='gensim_lsi')\n",
    "    output_type = widgets.Dropdown(description='Output', options=output_options, value='network')\n",
    "    display(\n",
    "        widgets.HBox([model_type, n_features])\n",
    "    )\n",
    "    _ = generic_process_corpus_gui.process_corpus_gui(\n",
    "        current_corpus_container(),\n",
    "        treaty_repository.current_wti_index(),\n",
    "        compute_similarity_scores,\n",
    "        tagset=DF_TAGSET,\n",
    "        gpe_filename=GPE_FILENAME,\n",
    "        model_type=model_type,\n",
    "        n_features=n_features,\n",
    "        output_type=output_type,\n",
    "        match_score=1,\n",
    "        mismatch_score=-1,\n",
    "        gap_score=-1\n",
    "    )\n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>Explore </span> Similarity Scores <span style='float: right; color: red'>OPTIONAL</span>\n",
    "Explore previously computed similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "model_id": "05bb402e85964426a9bcf5223ca16a1d",
       "version_minor": 0
      },
      "text/plain": "VBox(children=(HBox(children=(IntSlider(value=25, description='Most similar', max=1000, min=10), FloatSlider(v…"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import common.network.plot as network_plot\n",
    "import types\n",
    "\n",
    "def plot_similarity_network(n_top, p_threshold, output_type):\n",
    "\n",
    "    plot_opts = dict(\n",
    "        #x_axis_type=None,\n",
    "        #y_axis_type=None,\n",
    "        #background_fill_color='white',\n",
    "        line_opts=dict(color='black', alpha=0.5),\n",
    "        figsize=(900, 900),\n",
    "        node_opts=dict(color='lightgreen', level='overlay', alpha=1.0),\n",
    "        node_size=20,\n",
    "        node_label='name',\n",
    "        node_label_opts=dict(y_offset=15, x_offset=0),\n",
    "        #edge_label='edge_label', \n",
    "        #edge_label_opts={}\n",
    "    )\n",
    "\n",
    "    layout_opts = {\n",
    "        'algorithm': 'nx_spring_layout',\n",
    "        'args': {} # dict(scale=1.0, K=K, C=C, p=p1)\n",
    "    }\n",
    "    \n",
    "    df = current_similarity_model()\n",
    "\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    df = df[df.score >= (p_threshold / 100.0)].head(n_top)\n",
    "\n",
    "    if output_type == \"network\":\n",
    "        plot_data = network_plot.plot_df(df, source='treaty_id1', target='treaty_id2', weight='score', layout_opts=layout_opts, plot_opts=plot_opts)\n",
    "    else:\n",
    "        display(df)\n",
    "        \n",
    "def display_similarity_plot_gui():\n",
    "    \n",
    "    output_options = [\n",
    "        ('Network', 'network'),\n",
    "        ('Table', 'table')\n",
    "    ]\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        n_top=widgets.IntSlider(description='Most similar', min=10, max=1000, value=25),\n",
    "        p_threshold=widgets.FloatSlider(description='Threshold', min=0.0, max=100.0, value=90.0),\n",
    "        output_type = widgets.Dropdown(description='Output', options=output_options, value='network')\n",
    "    )\n",
    "    \n",
    "    iw = widgets.interactive(plot_similarity_network, n_top=gui.n_top, p_threshold=gui.p_threshold, output_type=gui.output_type)\n",
    "    \n",
    "    display(\n",
    "        widgets.VBox(\n",
    "            [\n",
    "                widgets.HBox([gui.n_top, gui.p_threshold, gui.output_type]),\n",
    "                iw.children[-1]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    iw.update()\n",
    "        \n",
    "display_similarity_plot_gui()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>PREPARE</span> Generate PASSIM input file<span style='color: blue; float: right'>OPTIONAL</span>\n",
    "\n",
    "### Explore Text using PASSIM\n",
    "http://www.ccs.neu.edu/home/dasmith/infect-dl-2014.pdf\n",
    "https://github.com/ViralTexts/vt-passim/wiki/Passim-data-model\n",
    "\n",
    "#### Alternatives?:\n",
    "\n",
    "[tess] Forstall, C., Coffee, N., Buck, T., Roache, K., & Jacobson, S.\n",
    "       (2014). Modeling the scholars: Detecting intertextuality through\n",
    "       enhanced word-level n-gram matching. Digital Scholarship in the\n",
    "       Humanities, 30(4), 503-515.\n",
    "       \n",
    "https://github.com/tesserae/tesserae-v5\n",
    "\n",
    "Detect and align similar passages: https://github.com/dasmiq/passim\n",
    "\n",
    "Install pyarroq for easy bindings to Apache Arrow data (i.e. parquet): ```sudo pip3 install pyarrow```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-06-02 14:29:56,790 : INFO : generic_process_corpus_gui.py.process_corpus_gui() : ...loading term substitution mappings...\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "model_id": "7eff3290215c4a9db20cee5b4a40c7ea",
       "version_minor": 0
      },
      "text/plain": "VBox(children=(IntProgress(value=0, layout=Layout(width='90%'), max=5), HBox(children=(VBox(children=(HBox(chi…"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "model_id": "a0eb631a9c124c1db7a3748f1af0e877",
       "version_minor": 0
      },
      "text/plain": "Text(value='treaties_passim.json', description='Filename:', placeholder='filename')"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import generic_process_corpus_gui\n",
    "import json\n",
    "\n",
    "def corpus_to_passim(corpus, process_opts, extract_args):\n",
    "    \n",
    "    def terms_to_text(terms):\n",
    "        \n",
    "        ts = ' '.join(terms)\n",
    "        \n",
    "        ts = cgi.escape(ts)\n",
    "        \n",
    "        return ts\n",
    "    \n",
    "    def doc_to_json(doc):\n",
    "        \n",
    "        treaty_id     = doc.metadata['treaty_id']\n",
    "        signed_year   = doc.metadata['signed_year']\n",
    "        terms         = [ x for x in textacy_utility.extract_document_terms(doc, extract_args)]\n",
    "        \n",
    "        document_text = terms_to_text(terms)\n",
    "        \n",
    "        doc_data      = dict(id=treaty_id, series=treaty_id, text=document_text, date=str(signed_year))\n",
    "        json_data     = json.dumps(doc_data, ensure_ascii=True, sort_keys=True)\n",
    "        \n",
    "        return json_data\n",
    "               \n",
    "    filename = process_opts.get('filename_widget').value\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "    \n",
    "        for doc in corpus:\n",
    "            terms = [ x for x in textacy_utility.extract_document_terms(doc, extract_args)]\n",
    "            f.write(doc_to_json(doc) + '\\n')\n",
    "\n",
    "try:\n",
    "    \n",
    "    filename_widget = widgets.Text(value='treaties_passim.json', placeholder='filename', description='Filename:', disabled=False)\n",
    "    \n",
    "    _ = generic_process_corpus_gui.process_corpus_gui(\n",
    "        current_corpus_container(), treaty_repository.current_wti_index(), corpus_to_passim, tagset=DF_TAGSET, gpe_filename=GPE_FILENAME, filename_widget=filename_widget)\n",
    "    \n",
    "    display(filename_widget)\n",
    "    \n",
    "    \n",
    "except Exception as ex:\n",
    "    raise\n",
    "    logger.error(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: green;'>EXPLORE</span> Explore PASSIM outputs<span style='color: blue; float: right'>OPTIONAL</span>\n",
    "\n",
    ">>\n",
    "From [ViralTexts Wiki](https://github.com/ViralTexts/vt-passim/wiki/Passim-data-model):\n",
    ">>\n",
    "*By default, passim outputs a set of clusters of reprinted passages. While the passages are grouped into clusters, the concrete output is a flat series of records, which are easier to manage than nested or relational data, especially when not much data is associated with the clusters as such rather than their constituent passages.\n",
    ">>\n",
    "As noted above, passim passes many input fields unchanged into the output. New or changed fields include:\n",
    ">>\n",
    "- cluster is a long integer uniquely identifies the cluster.\n",
    "- size is an integer count of the number of passages in the cluster. It is a convenience field that allows us to sort clusters in descending order of size.\n",
    "uid is a long integer unique key computed from the input id field, which is preserved unchanged. It is used for internal passim computations and is present in the output for debugging purposes.\n",
    "- text is a substring of the input text field that contains the reused passage.\n",
    "- begin is the character offset into the input text where the reused passage begins.\n",
    "- end is the character offset into the input text where the reused passage ends.\n",
    "- pages is an array of pages and bounding boxes on those pages corresponding to the reused passage.\n",
    "- locs is an array of canonical citations corresponding to the reused passage.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_passim_output_table(data_folder, kind):\n",
    "    \n",
    "    source = os.path.join(data_folder, '{}.parquet'.format(kind))\n",
    "    \n",
    "    df = pq.read_table(source).to_pandas()\n",
    "    display(df)\n",
    "    \n",
    "def display_passim_output_gui():\n",
    "    \n",
    "    folders = glob.glob('./output*')\n",
    "    \n",
    "    # kind_options = list(map(lambda x: re.search('.*\\/(\\w+)\\.parquet$', x).group(1), filenames))    \n",
    "    kind_options = ['dfpost', 'pass', 'clusters', 'out', 'extents', 'align', 'pairs']\n",
    "    \n",
    "    gui = types.SimpleNamespace(\n",
    "        folder=widgets.Dropdown(description='Batch', options=folders, value=None),\n",
    "        kind=widgets.Dropdown(description='Kind', options=kind_options, value='pairs'),\n",
    "        display=widgets.Button(description='Display', button_style='Success'),\n",
    "        output=widgets.Output()\n",
    "    )\n",
    "    \n",
    "    display(\n",
    "        widgets.VBox([\n",
    "            widgets.HBox([gui.folder, gui.kind, gui.display]),\n",
    "            gui.output\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    def display_handler(*args):\n",
    "        \n",
    "        gui.output.clear_output()\n",
    "        \n",
    "        folder = gui.folder.value\n",
    "        kind = gui.kind.value\n",
    "        \n",
    "        if folder is None or kind is None:\n",
    "            return\n",
    "        \n",
    "        gui.display.disabled = True\n",
    "        \n",
    "        with gui.output:\n",
    "            \n",
    "            filename = os.path.join(gui.folder.value, '{}.parquet'.format(gui.kind.value))\n",
    "            \n",
    "            if not os.path.isdir(filename):\n",
    "                \n",
    "                logger.info('File %s not found.' % filename)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                dataset = pq.ParquetDataset(filename)\n",
    "\n",
    "                df = dataset.read().to_pandas()\n",
    "\n",
    "                if kind == 'align':\n",
    "                    headers = ['id1', 'id2', 'matches', 'score', 's1', 's2']\n",
    "                    df = df[headers]\n",
    "                    \n",
    "                display(df.head())\n",
    "            \n",
    "        gui.display.disabled = False\n",
    "    \n",
    "    gui.display.on_click(display_handler)\n",
    "    \n",
    "#display_passim_output_table('./output_xxx', 'align')\n",
    "\n",
    "#['dfpost', 'pass', 'clusters', 'out', 'extents', 'align', 'pairs']\n",
    "#data = types.SimpleNamespace(\n",
    "#    dfpost = pq.ParquetDataset('./output_xxx/dfpost.parquet').read().to_pandas(),\n",
    "#    passage = pq.ParquetDataset('./output_xxx/pass.parquet').read().to_pandas(),\n",
    "#    clusters = pq.ParquetDataset('./output_xxx/clusters.parquet').read().to_pandas(),\n",
    "#    out = pq.ParquetDataset('./output_xxx/out.parquet').read().to_pandas(),\n",
    "#    extents = pq.ParquetDataset('./output_xxx/extents.parquet').read().to_pandas(),\n",
    "#    align = pq.ParquetDataset('./output_xxx/align.parquet').read().to_pandas(),\n",
    "#    pairs = pq.ParquetDataset('./output_xxx/pairs.parquet').read().to_pandas()\n",
    "#)\n",
    "\n",
    "display_passim_output_gui()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "filename = './output_xxx/align.parquet'\n",
    "dataset = pq.ParquetDataset(filename)\n",
    "df = dataset.read().to_pandas().sort_values(['score'], ascending=False)\n",
    "\n",
    "gui = types.SimpleNamespace(\n",
    "    left=widgets.Output(layout={'border': '1px solid black'}),\n",
    "    right=widgets.Output(layout={'border': '1px solid black'}),\n",
    "    score=widgets.Text(description='SCORE', disabled=True),\n",
    "    #difference=widgets.HTML(),\n",
    "    left_header=widgets.Label(layout={'border': '1px solid black', 'font-weight': 'bold'}),\n",
    "    right_header=widgets.Label(layout={'border': '1px solid black', 'font-weight': 'bold'}),\n",
    "    index=widgets.IntSlider(description='Index', min=0, max=len(df)-1,value=0),\n",
    "    forward=widgets.Button(description='>>', button_style='Success'),\n",
    "    backward=widgets.Button(description='<<', button_style='Success'),\n",
    ")\n",
    "\n",
    "def on_index_value_change(*args):\n",
    "    \n",
    "    gui.left.clear_output()\n",
    "    gui.right.clear_output()\n",
    "    \n",
    "    row = df.iloc[gui.index.value].to_dict()\n",
    "    \n",
    "    treaty_left = treaty_repository.current_wti_index().treaties.loc[row['id1']]\n",
    "    treaty_right = treaty_repository.current_wti_index().treaties.loc[row['id2']]\n",
    "    \n",
    "    gui.left_header.value = 'Treaty {} between {} and {}. Signed: {} Length: {}'.format(treaty_left.name, treaty_left.party1, treaty_left.party2, treaty_left.signed_year, row['len1'])\n",
    "    gui.right_header.value = 'Treaty {} between {} and {}. Signed: {} Length: {}'.format(treaty_right.name, treaty_right.party1, treaty_right.party2, treaty_right.signed_year, row['len2'])\n",
    "    \n",
    "    gui.score.value = str(int(row['score']))\n",
    "    \n",
    "        #html = difflib.HtmlDiff().make_table(row['s1'].split(), row['s2'].split())\n",
    "        #gui.difference.value = html\n",
    "        #{'uid2': 4219526764423281057, 'gid2': 4219526764423281057, 'tok1': 344, 'ew2': 487,\n",
    "        # 's1': '', 'id2': '100514', 'b2': 3551, 'bw2': 441, 'len2': 3979,\n",
    "        # 's2': '', 'uid1': -3491716573906744210, 'e1': 2844, 'id1': '100075', 'e2': 3924, 'series2': '100514', 'bw1': 300, 'gid1': -3491716573906744210,\n",
    "        # 'series1': '100075', 'tok2': 495, 'ew1': 343, 'score': 502.0, 'b1': 2510, 'len1': 2844, 'matches': 284}        \n",
    "\n",
    "    with gui.left:\n",
    "        print(row['s1'])\n",
    "\n",
    "    with gui.right:\n",
    "        print(row['s2'])\n",
    "\n",
    "def on_stepper_click(sender, *args):\n",
    "    \n",
    "    if sender.description == '<<':\n",
    "        if gui.index.value > 0:\n",
    "            gui.index.value = gui.index.value - 1\n",
    "            \n",
    "    if sender.description == '>>':\n",
    "        if gui.index.value < gui.index.max:\n",
    "            gui.index.value = gui.index.value + 1\n",
    "    \n",
    "gui.index.observe(on_index_value_change, names='value')\n",
    "gui.forward.on_click(on_stepper_click)\n",
    "gui.backward.on_click(on_stepper_click)\n",
    "\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        #widgets.HBox([gui.difference]),\n",
    "        widgets.HBox([gui.index, gui.backward, gui.forward]),\n",
    "        widgets.HBox([gui.score]),\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                gui.left_header,\n",
    "                gui.left\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                gui.right_header,\n",
    "                gui.right\n",
    "            ])\n",
    "        ])\n",
    "    ])\n",
    ")\n",
    "on_index_value_change()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}