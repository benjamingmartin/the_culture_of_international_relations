{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Culture of International Relations - Text Analysis\n",
    "### <span style='color: green'>SETUP </span> Prepare and Setup Notebook <span style='float: right; color: red'>MANDATORY</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, collections, zipfile\n",
    "import re, typing.re\n",
    "import nltk, textacy, spacy \n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "#import bokeh, bokeh.plotting, bokeh.models, \n",
    "import matplotlib.pyplot as plt\n",
    "import common.utility as utility\n",
    "import common.widgets_utility as widgets_utility\n",
    "import common.widgets_config as widgets_config\n",
    "import common.config as config\n",
    "import common.utility as utility\n",
    "import common.treaty_utility as treaty_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import treaty_corpus\n",
    "#import types, glob\n",
    "import textacy.keyterms\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display, set_matplotlib_formats\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "utility.setup_default_pd_display(pd)\n",
    "\n",
    "DATA_FOLDER = '../data'\n",
    "PATTERN = '*.txt'\n",
    "PERIOD_GROUP = 'years_1945-1972'\n",
    "DF_TAGSET = pd.read_csv('../data/tagset.csv', sep='\\t').fillna('')\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "TREATY_TIME_GROUPINGS = WTI_INDEX.get_treaty_time_groupings()\n",
    "\n",
    "%matplotlib inline\n",
    "# set_matplotlib_formats('svg')   \n",
    "#bokeh.plotting.output_notebook()\n",
    "\n",
    "current_corpus_container = lambda: textacy_utility.CorpusContainer.container()\n",
    "current_corpus = lambda: textacy_utility.CorpusContainer.corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green'>PREPARE </span> Load and Prepare Corpus <span style='float: right; color: red'>MANDATORY</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy_corpus_utility as textacy_utility\n",
    "import textacy_corpus_gui\n",
    "try:\n",
    "    container = current_corpus_container()\n",
    "    textacy_corpus_gui.display_corpus_load_gui(DATA_FOLDER, WTI_INDEX, container)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> Display Named Entities<span style='color: green; float: right'>SKIP</span>\n",
    "Spacy NER, note that \"ner\" must be enabled in corpus pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7f7d9bb2b5468182f667b0979122e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Treaty', index=1, layout=Layout(width='80%'), options=(('All Treaties', Nâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Named Entities\n",
    "import gui_utility\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "from spacy import displacy\n",
    "\n",
    "def display_document_entities_gui(corpus, wti_index):\n",
    "    \n",
    "    def display_document_entities(corpus, treaty_id):\n",
    "        \n",
    "        doc = textacy_utility.get_treaty_doc(corpus, treaty_id)\n",
    "        \n",
    "        displacy.render(doc.spacy_doc, style='ent', jupyter=True)\n",
    "\n",
    "    document_options = [('All Treaties', None)] + gui_utility.get_treaty_dropdown_options(wti_index, corpus)\n",
    "            \n",
    "    treaty_ids = widgets.Dropdown(description='Treaty', options=document_options, value=document_options[1][1], layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    itw = widgets.interactive(\n",
    "        display_document_entities,\n",
    "        corpus=widgets.fixed(corpus),\n",
    "        treaty_id=treaty_ids\n",
    "    )\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        treaty_ids,\n",
    "        widgets.VBox([itw.children[-1]], layout=widgets.Layout(margin_top='20px', height='500px',width='100%'))\n",
    "    ]))\n",
    "\n",
    "    itw.update()\n",
    "    \n",
    "try:\n",
    "    corpus = current_corpus()\n",
    "    display_document_entities_gui(corpus, WTI_INDEX)\n",
    "except Exception as ex:\n",
    "    logger.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>MODEL</span> TESTS - IGNORE EVERYTHING BELOW<span style='color: green; float: right'>SKIP</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.parse import corenlp\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url='http://localhost:9001', encoding='utf8', tagtype='ner')\n",
    "    input_tokens = 'Stony Brook University in NY'.split()\n",
    "    tagged_output = corenlp_tagger.tag(input_tokens)\n",
    "    print('Stanford tagger is up and running!')\n",
    "except: # (ConnectionError, ConnectionRefusedError):\n",
    "    logger.error('Server not found! Please start Stanford CoreNLP Server!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "sys.path = list(set(['.', '..']) - set(sys.path)) + sys.path\n",
    "\n",
    "import textacy_corpus_utility as textacy_utility\n",
    "import common.treaty_state as treaty_repository\n",
    "import common.utility as utility\n",
    "import common.config as config\n",
    "\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "from IPython.display import display\n",
    "\n",
    "logger = utility.getLogger('corpus_text_analysis')\n",
    "\n",
    "import pickle\n",
    "import topic_model\n",
    "import topic_model_utility\n",
    "import treaty_corpus\n",
    "\n",
    "DATA_FOLDER = '../data'\n",
    "LANGUAGE = 'en'\n",
    "\n",
    "WTI_INDEX = treaty_repository.load_wti_index(data_folder=DATA_FOLDER)\n",
    "CORPUS_PATH = os.path.join(DATA_FOLDER, \"treaty_text_corpora_20181206_preprocessed.zip\")\n",
    "\n",
    "treaties = WTI_INDEX.get_treaties(language=LANGUAGE)\n",
    "document_stream = treaty_corpus.get_document_stream(CORPUS_PATH, LANGUAGE, treaties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import codecs\n",
    "import time\n",
    "import collections\n",
    "import nltk.tag\n",
    "from nltk.parse import corenlp\n",
    "import nltk.tokenize.stanford as st\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "def extract_entity_phrases(data, classes=[ 'LOCATION', 'PERSON']):\n",
    "\n",
    "    # Extract entities of selected classes, add index to enable merge to phrases\n",
    "    entities = [ (i, word, wclass)\n",
    "        for (i, (word, wclass)) in enumerate(data) if classes is None or wclass in classes ]\n",
    "\n",
    "    # Merge adjacent entities having the same classifier\n",
    "    for i in range(len(entities) - 1, 0, -1):\n",
    "        if entities[i][0] == entities[i - 1][0] + 1 and entities[i][2] == entities[i - 1][2]:\n",
    "            entities[i - 1] = (entities[i - 1][0], entities[i - 1][1] + \" \" + entities[i][1], entities[i - 1][2])\n",
    "            del entities[i]\n",
    "\n",
    "    # Remove index in returned data\n",
    "    return [ (word, wclass) for (i, word, wclass) in entities  ]\n",
    "\n",
    "def create_ner_tagger(options):\n",
    "    corenlp_tagger = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8', tagtype='ner')\n",
    "    return corenlp_tagger\n",
    "\n",
    "def create_tokenizer(options):\n",
    "    corenlp_tokenizer = corenlp.CoreNLPParser(url=options['server_url'], encoding='utf8')\n",
    "    return corenlp_tokenizer\n",
    "\n",
    "def create_statistics(entities):\n",
    "    wc = collections.Counter()\n",
    "    wc.update(entities)\n",
    "    return wc\n",
    "\n",
    "def serialize_content(stats, filename, token_count):\n",
    "    document_name, treaty_id, lang = extract_document_info(filename)\n",
    "    data = [ (document_name, treaty_id, lang, word, wclass, stats[(word, wclass)], token_count) for (word, wclass) in stats  ]\n",
    "    content = '\\n'.join(map(lambda x: ';'.join([str(y) for y in x]), data))\n",
    "    return content\n",
    "\n",
    "def write_content(outfile, content):\n",
    "    if content != '':\n",
    "        outfile.write(content)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "def recognize_entities(options):\n",
    "\n",
    "    corenlp_tokenizer = create_tokenizer(options)\n",
    "    corenlp_tagger = create_ner_tagger(options)\n",
    "    \n",
    "    outfile = os.path.join(options['output_folder'], \"output_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\")\n",
    "    tags = [ 'NUMBER', 'LOCATION', 'DATE', 'MISC', 'ORGANIZATION', 'DURATION', 'SET', 'ORDINAL', 'PERSON' ]\n",
    "    \n",
    "    document_stream = treaty_corpus.get_document_stream(options['source_path'], options['language'], treaties)\n",
    "    for treaty_id, language, filename, content in document_stream:\n",
    "        print('treaty_id')\n",
    "        \n",
    "options = {\n",
    "    \"language\": 'en',\n",
    "    \"source_path\": \"../data/treaty_text_corpora_20181206_preprocessed.zip\",\n",
    "    'server_url': 'http://localhost:9001',\n",
    "    'output_folder': DATA_FOLDER,\n",
    "}\n",
    "\n",
    "recognize_entities(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_current_corpus().textacy_corpus\n",
    "gpe = set([])\n",
    "for doc in corpus:\n",
    "    candidates = [ x for x in doc if len(x) > 1 and x.ent_type_ == 'GPE' and x.is_alpha ]\n",
    "    gpe = gpe.union(set([ x.lower_ for x in candidates]))\n",
    "    gpe = gpe.union(set([ x.lemma_ for x in candidates]))\n",
    "\n",
    "df = pd.DataFrame({ 'word': list(gpe)})\n",
    "df.sort_values('word')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_folder = '../data/'\n",
    "\n",
    "def include_predicate(filename, options):\n",
    "    \n",
    "options = {\n",
    "    \"language\": 'en',\n",
    "    \"source_path\": \"treaty_text_corpora_20181206_preprocessed.zip\",\n",
    "    'server_url': 'http://localhost:9001',\n",
    "    'output_folder': data_folder,\n",
    "}\n",
    "\n",
    "main(options)\n",
    "\n",
    "for zip_source in options[\"zip_sources\"]:\n",
    "    with io.open(outfile, 'w', encoding='utf8') as o:\n",
    "        with zipfile.ZipFile(zip_source) as pope_zip:\n",
    "            for filename in pope_zip.namelist():\n",
    "                with pope_zip.open(filename) as pope_file:\n",
    "                    try:\n",
    "                        text = pope_file.read().decode(\"utf-8\")\n",
    "                        tokens = corenlp_tokenizer.tokenize(text)\n",
    "                        data = corenlp_tagger.tag(tokens)\n",
    "                        entities = extract_entity_phrases(data, tags)  # [ 'LOCATION', 'PERSON', 'ORGANIZATION' ])\n",
    "                        statistics = create_statistics(entities)\n",
    "                        content = serialize_content(statistics, filename, len(tokens))\n",
    "                        write_content(o, content)\n",
    "                    except Exception as ex:\n",
    "                        raise\n",
    "                        print('Failed: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_current_corpus().textacy_corpus\n",
    "gpe = set([])\n",
    "for doc in corpus:\n",
    "    candidates = [ x for x in doc if len(x) > 1 and x.ent_type_ == 'GPE' ]\n",
    "    gpe = gpe.union(set([ x.lower_ for x in candidates]))\n",
    "    gpe = gpe.union(set([ x.lemma_ for x in candidates]))\n",
    "\n",
    "df = pd.DataFrame({ 'word': list(gpe)})\n",
    "df.sort_values('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [get_current_corpus().textacy_corpus[0]]\n",
    "ents = set([])\n",
    "for doc in corpus:\n",
    "    candidates = set([\n",
    "            x.lower_ + ' / ' + ' '.join([ t.ent_type_ for t in x ])\n",
    "        for x in doc.spacy_doc.ents if x.text not in ('', ' ', '\\n', '\\t')])\n",
    "    ents = ents.union(candidates)\n",
    "\n",
    "df_ent = pd.DataFrame({'ent': list(ents)})\n",
    "df_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"FB is hiring a new Vice President of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)\n",
    "# the model didn't recognise \"FB\" as an entity :(\n",
    "\n",
    "ORG = doc.vocab.strings[u'ORG']  # get hash value of entity label\n",
    "fb_ent = Span(doc, 0, 1, label=ORG) # create a Span for the new entity\n",
    "doc.ents = list(doc.ents) + [fb_ent]\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('After', ents)\n",
    "# [(u'FB', 0, 2, 'ORG')] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'as_strings': True,\n",
    "    'named_entities': False,\n",
    "    'ngrams': [1, 2],\n",
    "    'normalize': 'lemma'\n",
    "}\n",
    "kwargs = {\n",
    "    'filter_punct': True,\n",
    "    'filter_stops': True,\n",
    "    'include_pos': ('NOUN', 'PROPN'),\n",
    "    'min_freq': 2\n",
    "}\n",
    "tokenizer_args = {\n",
    "    'args': args,\n",
    "    'kwargs': kwargs,\n",
    "    'extra_stop_words': {},\n",
    "    'mask_gpe': True,\n",
    "    'min_freq': 2,\n",
    "    'max_doc_freq': 0.80    \n",
    "}\n",
    "corpus = get_current_corpus().textacy_corpus\n",
    "fx_terms = lambda: ( textacy_utility.textacy_filter_terms(doc, tokenizer_args) for doc in corpus )\n",
    "terms = fx_terms()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
