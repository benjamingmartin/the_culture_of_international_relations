{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color: green;'>DESCRIBE</span> CO-occurrence Matrix<span style='color: red; float: right'>WORK IN PROGRESS</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#corpus = get_current_corpus().textacy_corpus\n",
    "\n",
    "nlp = textacy.load_spacy('en')\n",
    "text = \"The Horse Raced Past the Barn Fell\"\n",
    "\n",
    "corpus = textacy.Corpus(nlp)\n",
    "corpus.add_text(text)\n",
    "    \n",
    "term_args = dict(\n",
    "    args=dict(\n",
    "        #ngrams=gui.ngrams.value,\n",
    "        #named_entities=gui.named_entities.value,\n",
    "        normalize='lemma',\n",
    "        as_strings=True\n",
    "    ),\n",
    "    kwargs=dict(\n",
    "        filter_nums=True,\n",
    "        drop_determiners=True,\n",
    "        min_freq=1,\n",
    "        include_pos=['NOUN'],\n",
    "        filter_stops=True,\n",
    "        filter_punct=True\n",
    "    ),\n",
    "    extra_stop_words=None\n",
    ")\n",
    "terms_iter = lambda: ( textacy_utility.textacy_filter_terms(doc, term_args) for doc in corpus )\n",
    "\n",
    "#vocab = corpus.spacy_vocab\n",
    "#id2token = { x.lower: x.lower_ for x in vocab }\n",
    "#dictionary.token2id = { x.lower_: x.lower for x in vocab }\n",
    "dictionary = gensim.corpora.Dictionary(terms_iter())\n",
    "_ = dictionary[0]\n",
    "X = gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator(dictionary.id2token, dictionary)\n",
    "X.accumulate(terms_iter(), window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-12-14 14:59:45,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-12-14 14:59:45,962 : INFO : built Dictionary(6 unique tokens: ['of', 'the', 'concept', 'basic', 'word']...) from 1 documents (total 7 corpus positions)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6716205aeebf46ceb814bf47e445955f",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "nlp = textacy.load_spacy('en')\n",
    "text = \"The basic concept of the word association\".lower()\n",
    "\n",
    "corpus = textacy.Corpus(nlp)\n",
    "corpus.add_text(text)\n",
    "fx = lambda: ((x.orth_ for x in doc) for doc in corpus)\n",
    "dictionary = gensim.corpora.Dictionary(fx())\n",
    "_ = dictionary[0]\n",
    "X = gensim.topic_coherence.text_analysis.WordOccurrenceAccumulator(dictionary.id2token, dictionary)\n",
    "X.accumulate(fx(), window_size=2)\n",
    "coo = X._co_occurrences.tocoo()\n",
    "df = pd.DataFrame({'word_id1': coo.row, 'word_id2': coo.col, 'count': coo.data})\n",
    "#df = df[df.word_id1 > df.word_id2]\n",
    "\n",
    "df['word1'] = df.word_id1.apply(lambda x: dictionary[x])\n",
    "df['word2'] = df.word_id2.apply(lambda x: dictionary[x])\n",
    "df['count1'] = df.word_id1.apply(lambda x: X._occurrences[x])\n",
    "df['count2'] = df.word_id2.apply(lambda x: X._occurrences[x])\n",
    "\n",
    "df = df[['word_id1', 'word_id2', 'word1', 'word2', 'count', 'count1', 'count2']].sort_values(['word1'], ascending=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def corpus_subset(corpus, px):\n",
    "    iter1, iter2 = itertools.tee(corpus.get(px))\n",
    "    docs = (d.spacy_doc for d in iter1)\n",
    "    metadatas = (d.metadata for d in iter2)\n",
    "    sub_corpus = textacy.Corpus(lang=corpus.spacy_lang, docs=docs, metadatas=metadatas)\n",
    "    return sub_corpus\n",
    "\n",
    "def fpx(year):\n",
    "    return lambda x: int(x.metadata['signed_year']) == year\n",
    "\n",
    "#data = corpus_subset(corpus, fpx(1947)).word_freqs(weighting='freq', as_strings=True)\n",
    "#df = pd.DataFrame({'key': list(data.keys()),  'weight': list(data.values()) })\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from glove import Corpus\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# See http://www.foldl.me/2014/glove-python/\n",
    "#def compute_GloVe_df(sentences, window=2, dictionary=None):\n",
    "#    \n",
    "#    corpus = Corpus(dictionary=dictionary)\n",
    "#    corpus.fit(sentences, window=window)\n",
    "\n",
    "#    dm = corpus.matrix.todense()\n",
    "#    inverse_dictionary = { i: w for w, i in corpus.dictionary.items() }\n",
    "#    id2token = [ inverse_dictionary[i] for i in range(0,max(inverse_dictionary.keys())+1)]\n",
    "\n",
    "#    df = pd.DataFrame(dm.T, columns=id2token).assign(word=id2token).set_index('word')\n",
    "#    return df\n",
    "\n",
    "#https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.nonzero.html\n",
    "    \n",
    "def _coo_to_sparse_series(A, dense_index=False):\n",
    "    \"\"\" Convert a scipy.sparse.coo_matrix to a SparseSeries.\n",
    "    Use the defaults given in the SparseSeries constructor. \"\"\"\n",
    "    # A = scipy.sparse.triu(A)\n",
    "    s = pd.Series(A.data, pd.MultiIndex.from_arrays((A.row, A.col)))\n",
    "    s = s.sort_index()\n",
    "    s = s.to_sparse()\n",
    "    return s\n",
    "\n",
    "term_args = dict(\n",
    "    args=dict(\n",
    "        ngrams=1,\n",
    "        named_entities=False,\n",
    "        normalize='lemma',\n",
    "        as_strings=True\n",
    "    ),\n",
    "    kwargs=dict(\n",
    "        filter_stops=True,\n",
    "        filter_punct=True,\n",
    "        filter_nums=True,\n",
    "        min_freq=1,\n",
    "        drop_determiners=True,\n",
    "        include_pos=('NOUN', 'PROPN', )\n",
    "    )\n",
    ")\n",
    "\n",
    "stream = (textacy_filter_terms(doc, term_args) for doc in CORPUS)\n",
    "#stream = (' '.join(list(textacy_filter_terms(doc, term_args))) for doc in CORPUS)\n",
    "\n",
    "glove_co_matrix = Corpus() #dictionary=None)\n",
    "\n",
    "docs = (list(textacy_filter_terms(doc, term_args)) for doc in CORPUS)\n",
    "glove_co_matrix.fit(docs, window=5)\n",
    "\n",
    "dictionary = glove_co_matrix.dictionary\n",
    "co_series = _coo_to_sparse_series(glove_co_matrix.matrix)\n",
    "co_df = co_series.to_dense().reset_index().rename(columns={ 'level_0': 'token_id_x', 'level_1': 'token_id_y', 0: 'weight'})\n",
    "id2token = pd.DataFrame({ 'token_id': list(dictionary.values()), 'token': list(dictionary.keys()) }).set_index('token_id')\n",
    "\n",
    "#https://stackoverflow.com/questions/34181494/populate-a-pandas-sparsedataframe-from-a-scipy-sparse-coo-matrix\n",
    "\n",
    "df = pd.merge(co_df, id2token, left_on='token_id_x', right_index=True, how='inner').rename(columns={'token': 'token_x'})\n",
    "df = pd.merge(df,    id2token, left_on='token_id_y', right_index=True, how='inner').rename(columns={'token': 'token_y'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'row': glove_co_matrix.matrix.row, 'col': glove_co_matrix.matrix.col}).groupby(['row', 'col']).size().nlargest()\n",
    "#glove_co_matrix.matrix.tocoo()\n",
    "#df.nlargest(100, ['weight'])\n",
    "#df.groupby(['token_id_x', 'token_id_y']).size().nlargest()\n",
    "#co_series.reset_index().groupby(['level_0', 'level_1']).size().nlargest()\n",
    "dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS AND WORK IN PROGRESS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_current_corpus().textacy_corpus\n",
    "term_args = {\n",
    "    'mask_gpe': True,\n",
    "    'kwargs': {'filter_stops': True, 'filter_punct': True, 'include_pos': ('NOUN', 'PROPN'), 'min_freq': 2},\n",
    "    'extra_stop_words': set(),\n",
    "    'args': {'normalize': 'lemma', 'named_entities': False, 'ngrams': [1], 'as_strings': True},\n",
    "    'min_freq': 2,\n",
    "    'max_doc_freq': 0.8\n",
    "}\n",
    "fx_terms = lambda: textacy_utility.extract_corpus_terms(corpus, term_args)\n",
    "\n",
    "for x in fx_terms():\n",
    "    print(len([y for y in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([ {\n",
    "    'n_topics': int(model.tm_model.num_topics),\n",
    "    'perplexity_score': model.perplexity_score,\n",
    "    'coherence_score': model.coherence_score\n",
    "  } for model in models ])\n",
    "df['n_topics'] = df.n_topics.astype(int)\n",
    "df = df.set_index('n_topics')\n",
    "df['coherence_score'].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame Corpus from spaCy Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Corpus vs WTI Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus_documents = corpus.documents.set_index(['treaty_id', 'language'])\n",
    "treaty_text_languages = wti_index.get_treaty_text_languages().set_index(['treaty_id', 'language'])\n",
    "\n",
    "treaties_in_corpus_not_in_wti = corpus_documents.index.difference(treaty_text_languages.index).get_values()\n",
    "treaties_in_wti_not_in_corpus = treaty_text_languages.index.difference(corpus_documents.index).get_values()\n",
    "\n",
    "print(  'Found in corpus, but not in WTI: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_corpus_not_in_wti ]))\n",
    "\n",
    "print(  'Found in WTI, but not in corpus: ' +\n",
    "        ', '.join([ '{}/{}'.format(x,y) for x,y in treaties_in_wti_not_in_corpus ]))\n",
    "\n",
    "#corpus_documents.loc[corpus_text_not_in_wti]\n",
    "#treaty_text_languages.loc[wti_not_in_corpus]\n",
    "\n",
    "#wti_not_in_corpus\n",
    "\n",
    "# Duplicates:\n",
    "#corpus_documents.index.get_duplicates()\n",
    "#treaty_text_languages.corpus_documents.index.get_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Basic Corpus Statistics\n",
    "See https://www.nltk.org/book/ch01.html\n",
    "\n",
    "* Size of treaties over time\n",
    "* Unique word, unique words per word class\n",
    "* Lexical diversity\n",
    "* Frequency distribution\n",
    "* Average word length, sentence length\n",
    "\n",
    "\n",
    "```python\n",
    " \n",
    "len(texts) / count(docs)\n",
    "0.06230453042623537\n",
    "len(set(text3)) / len(text3)\n",
    "0.06230453042623537\n",
    "def lexical_diversity(text): [1]\n",
    "    return len(set(text)) / len(text) [2]\n",
    "\n",
    "def percentage(count, total): [3]\n",
    "    return 100 * count / total\n",
    "#### Most common words\n",
    "fdist1 = FreqDist(text1)\n",
    "fdist1.most_common(50)\n",
    "#### Word length frequencies\n",
    "fdist = FreqDist(len(w) for w in text1)  [2]\n",
    "print(fdist)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code \n",
    "\n",
    "corpus = None\n",
    "def display_token_toplist_interact(source_folder):\n",
    "    global corpus\n",
    "    progress_widget = None\n",
    "    \n",
    "    def display_token_toplist(source_folder, language, statistics='', remove_stopwords=False):\n",
    "        global corpus\n",
    "\n",
    "        try:\n",
    "\n",
    "            progress_widget.value = 1\n",
    "\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0]).load_mm_corpus()\n",
    "\n",
    "            progress_widget.value = 2\n",
    "            service = MmCorpusStatisticsService(corpus, dictionary=corpus.dictionary, language=language)\n",
    "\n",
    "            print(\"Corpus consists of {} documents, {} words in total and a vocabulary size of {} tokens.\"\\\n",
    "                      .format(len(corpus), corpus.dictionary.num_pos, len(corpus.dictionary)))\n",
    "\n",
    "            progress_widget.value = 3\n",
    "            if statistics == 'word_freqs':\n",
    "                display(service.compute_word_frequencies(remove_stopwords))\n",
    "            elif statistics == 'documents':\n",
    "                display(service.compute_document_stats())\n",
    "            elif statistics == 'word_count':\n",
    "                display(service.compute_word_stats())\n",
    "            else:\n",
    "                print('Unknown: ' + statistics)\n",
    "\n",
    "        except Exception as ex:\n",
    "            logger.error(ex)\n",
    "\n",
    "        progress_widget.value = 5\n",
    "        progress_widget.value = 0\n",
    "        return corpus\n",
    "    \n",
    "    language_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    statistics_widget=widgets.Dropdown(\n",
    "        options={\n",
    "            'Word freqs': 'word_freqs',\n",
    "            'Documents': 'documents',\n",
    "            'Word count': 'word_count'\n",
    "        },\n",
    "        value='word_count',\n",
    "        description='Statistics:', **dict(layout=widgets.Layout(width='260px'))\n",
    "    )\n",
    "    \n",
    "    remove_stopwords_widget=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist'\n",
    "    )\n",
    "    \n",
    "    progress_widget=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    "\n",
    "    wi = widgets.interactive(\n",
    "        display_token_toplist,\n",
    "        source_folder=source_folder,\n",
    "        language=language_widget,\n",
    "        statistics=statistics_widget,\n",
    "        remove_stopwords=remove_stopwords_widget\n",
    "    )\n",
    "\n",
    "    boxes = widgets.HBox(\n",
    "        [\n",
    "            language_widget, statistics_widget, remove_stopwords_widget, progress_widget\n",
    "        ]\n",
    "    )\n",
    "    display(widgets.VBox([boxes, wi.children[-1]]))\n",
    "    wi.update()\n",
    "\n",
    "display_token_toplist_interact('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red'>WORK IN PROGRESS</span> Task: Treaty Keyword Extraction (using TF-IDF weighing)\n",
    "- [ML Wiki.org](http://mlwiki.org/index.php/TF-IDF)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "- Spärck Jones, K. (1972). \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\".\n",
    "- Manning, C.D.; Raghavan, P.; Schutze, H. (2008). \"Scoring, term weighting, and the vector space model\". ([PDF](http://nlp.stanford.edu/IR-book/pdf/06vect.pdf))\n",
    "- https://markroxor.github.io/blog/tfidf-pivoted_norm/\n",
    "$\\frac{tf-idf}{\\sqrt(rowSums( tf-idf^2 ) )}$\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/pivoted-normalized-document-length-1.html\n",
    "\n",
    "Neural Network Methods in Natural Language Processing, Yoav Goldberg:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "from scipy.sparse import csr_matrix\n",
    "%timeit\n",
    "\n",
    "def get_top_tfidf_words(data, n_top=5):\n",
    "    top_list = data.groupby(['treaty_id'])\\\n",
    "        .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "        .reset_index(level=0, drop=True)\n",
    "    return top_list\n",
    "\n",
    "def compute_tfidf_scores(corpus, dictionary, smartirs='ntc'):\n",
    "    #model = gensim.models.logentropy_model.LogEntropyModel(corpus, normalize=True)\n",
    "    model = gensim.models.tfidfmodel.TfidfModel(corpus, dictionary=dictionary, normalize=True) #, smartirs=smartirs)\n",
    "    rows, cols, scores = [], [], []\n",
    "    for r, document in enumerate(corpus): \n",
    "        vector = model[document]\n",
    "        c, v = zip(*vector)\n",
    "        rows += (len(c) * [ int(r) ])\n",
    "        cols += c\n",
    "        scores += v\n",
    "        \n",
    "    return csr_matrix((scores, (rows, cols)))\n",
    "    \n",
    "if True: #'tfidf_cache' not in globals():\n",
    "    tfidf_cache = {\n",
    "    }\n",
    "    \n",
    "def display_tfidf_scores(source_folder, language, period, n_top=5, threshold=0.001):\n",
    "    \n",
    "    global state, tfw, tfidf_cache\n",
    "    \n",
    "    try:\n",
    "        treaties = state.treaties\n",
    "\n",
    "        tfw.progress.value = 0\n",
    "        tfw.progress.value += 1\n",
    "        if language[0] not in tfidf_cache.keys():\n",
    "            corpus = TreatyCorpusSaveLoad(source_folder=source_folder, lang=language[0])\\\n",
    "                .load_mm_corpus(normalize_by_D=True)\n",
    "            document_names = corpus.document_names\n",
    "            dictionary = corpus.dictionary\n",
    "            _ = dictionary[0]\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            A = compute_tfidf_scores(corpus, dictionary)\n",
    "\n",
    "            tfw.progress.value += 1\n",
    "            scores = pd.DataFrame(\n",
    "                [ (i, j, dictionary.id2token[j], A[i, j]) for i, j in zip(*A.nonzero())],\n",
    "                columns=['document_id', 'token_id', 'token', 'score']\n",
    "            )\n",
    "            tfw.progress.value += 1\n",
    "            scores = scores.merge(document_names, how='inner', left_on='document_id', right_index=True)\\\n",
    "                .drop(['document_id', 'token_id', 'document_name'], axis=1)\n",
    "\n",
    "            scores = scores[['treaty_id', 'token', 'score']]\\\n",
    "                .sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "            tfidf_cache[language[0]] = scores\n",
    "\n",
    "        scores = tfidf_cache[language[0]]\n",
    "        if threshold > 0:\n",
    "            scores = scores.loc[scores.score >= threshold]\n",
    "\n",
    "        tfw.progress.value += 1\n",
    "\n",
    "        #scores = get_top_tfidf_words(scores, n_top=5)\n",
    "        #scores = scores.groupby(['treaty_id']).sum() \n",
    "\n",
    "        scores = scores.groupby(['treaty_id'])\\\n",
    "            .apply(lambda x: x.nlargest(n_top, 'score'))\\\n",
    "            .reset_index(level=0, drop=True)\\\n",
    "            .set_index('treaty_id')\n",
    "\n",
    "        if period is not None:\n",
    "            periods = state.treaties[period]\n",
    "            scores = scores.merge(periods.to_frame(), left_index=True, right_index=True, how='inner')\\\n",
    "                .groupby([period, 'token']).score.agg([np.mean])\\\n",
    "                .reset_index().rename(columns={0:'score'}) #.sort_values('token')\n",
    "\n",
    "        #['token'].apply(' '.join)\n",
    "\n",
    "        display(scores)\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "        \n",
    "    tfw.progress.value = 0\n",
    "\n",
    "#if 'tfidf_scores' not in globals():\n",
    "#    tfidf_scores = compute_document_tfidf(corpus, corpus.dictionary, state.treaties)\n",
    "#    tfidf_scores = tfidf_scores.sort_values(['treaty_id', 'score'], ascending=[True, False])\n",
    "\n",
    "tfw = BaseWidgetUtility(\n",
    "    language=widgets.Dropdown(\n",
    "        options={\n",
    "            'English': ('en', 'english'),\n",
    "            'French': ('fr', 'french'),\n",
    "            'German': ('de', 'german'),\n",
    "            'Italian': ('it', 'italian')\n",
    "        },\n",
    "        value=('en', 'english'),\n",
    "        description='Language:', **drop_style\n",
    "    ),\n",
    "    remove_stopwords=widgets.ToggleButton(\n",
    "        description='Remove stopwords', value=True,\n",
    "        tooltip='Do not include stopwords in token toplist', **toggle_style\n",
    "    ),    \n",
    "    n_top=widgets.IntSlider(\n",
    "        value=5, min=1, max=25, step=1,\n",
    "        description='Top #:',\n",
    "        continuous_update=False\n",
    "    ),\n",
    "    threshold=widgets.FloatSlider(\n",
    "        value=0.001, min=0.0, max=0.5, step=0.01,\n",
    "        description='Threshold:',\n",
    "        tooltip='Word having a TF-IDF score below this value is filtered out',\n",
    "        continuous_update=False,\n",
    "        readout_format='.3f',\n",
    "    ), \n",
    "    period=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Period:', **drop_style\n",
    "    ),\n",
    "    output=widgets.Dropdown(\n",
    "        options={\n",
    "            '': None,\n",
    "            'Year': 'signed_year',\n",
    "            'Default division': 'signed_period',\n",
    "            'Alt. division': 'signed_period_alt'\n",
    "        },\n",
    "        value='signed_period',\n",
    "        description='Output:', **drop_style\n",
    "    ),\n",
    "    progress=widgets.IntProgress(min=0, max=5, step=1, value=0) #, layout=widgets.Layout(width='100%')),\n",
    ")\n",
    "\n",
    "itfw = widgets.interactive(\n",
    "    display_tfidf_scores,\n",
    "    source_folder='./data',\n",
    "    language=tfw.language,\n",
    "    n_top=tfw.n_top,\n",
    "    threshold=tfw.threshold,\n",
    "    period=tfw.period\n",
    ")\n",
    "\n",
    "boxes = widgets.HBox(\n",
    "    [\n",
    "        widgets.VBox([tfw.language, tfw.period]),\n",
    "        widgets.VBox([tfw.n_top, tfw.threshold]),\n",
    "        widgets.VBox([tfw.progress, tfw.output])\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(widgets.VBox([boxes, itfw.children[-1]]))\n",
    "itfw.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from beakerx.object import beakerx\n",
    "from beakerx import *\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "model = Cooccurrence(ngram_range=(1, 1))\n",
    "Xc = model.fit_transform(corpus)\n",
    "\n",
    "id2token = { i: x for (i, x) in enumerate(model.get_feature_names()) }\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Xxy = Xc.tocoo()\n",
    "word1 = [ id2token[x] for x in Xxy.row ]\n",
    "word2 = [ id2token[x] for x in Xxy.col ]\n",
    "\n",
    "df = pd.DataFrame({ 'word1': [ id2token[x] for x in Xxy.row ], 'word2': [ id2token[x] for x in Xxy.col ], 'count': Xxy.data })[['word1', 'word2', 'count']] #.set_index(['word1', 'word2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coo_to_sparse_series(A, dense_index=False):\n",
    "    \"\"\" Convert a scipy.sparse.coo_matrix to a SparseSeries.\n",
    "    Use the defaults given in the SparseSeries constructor. \"\"\"\n",
    "    s = Series(A.data, MultiIndex.from_arrays((A.row, A.col)))\n",
    "    s = s.sort_index()\n",
    "    s = s.to_sparse()  # TODO: specify kind?\n",
    "    # ...\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-93b80c7107e8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-93b80c7107e8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://github.com/hans/glove.py/blob/master/glove.py@listify\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/hans/glove.py/blob/master/glove.py\n",
    "#@listify\n",
    "def build_cooccur(vocab, corpus, window_size=10, min_count=None):\n",
    "    \"\"\"\n",
    "    Build a word co-occurrence list for the given corpus.\n",
    "    This function is a tuple generator, where each element (representing\n",
    "    a cooccurrence pair) is of the form\n",
    "        (i_main, i_context, cooccurrence)\n",
    "    where `i_main` is the ID of the main word in the cooccurrence and\n",
    "    `i_context` is the ID of the context word, and `cooccurrence` is the\n",
    "    `X_{ij}` cooccurrence value as described in Pennington et al.\n",
    "    (2014).\n",
    "    If `min_count` is not `None`, cooccurrence pairs where either word\n",
    "    occurs in the corpus fewer than `min_count` times are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    id2word = dict((i, word) for word, (i, _) in vocab.iteritems())\n",
    "\n",
    "    # Collect cooccurrences internally as a sparse matrix for passable\n",
    "    # indexing speed; we'll convert into a list later\n",
    "    cooccurrences = sparse.lil_matrix((vocab_size, vocab_size), dtype=np.float64)\n",
    "\n",
    "    for i, line in enumerate(corpus):\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            logger.info(\"Building cooccurrence matrix: on line %i\", i)\n",
    "\n",
    "        tokens = line.strip().split()\n",
    "        token_ids = [vocab[word][0] for word in tokens]\n",
    "\n",
    "        for center_i, center_id in enumerate(token_ids):\n",
    "            # Collect all word IDs in left window of center word\n",
    "            context_ids = token_ids[max(0, center_i - window_size) : center_i]\n",
    "            contexts_len = len(context_ids)\n",
    "\n",
    "            for left_i, left_id in enumerate(context_ids):\n",
    "                # Distance from center word\n",
    "                distance = contexts_len - left_i\n",
    "\n",
    "                # Weight by inverse of distance between words\n",
    "                increment = 1.0 / float(distance)\n",
    "\n",
    "                # Build co-occurrence matrix symmetrically (pretend we\n",
    "                # are calculating right contexts as well)\n",
    "                cooccurrences[center_id, left_id] += increment\n",
    "                cooccurrences[left_id, center_id] += increment\n",
    "\n",
    "    # Now yield our tuple sequence (dig into the LiL-matrix internals to\n",
    "    # quickly iterate through all nonzero cells)\n",
    "    for i, (row, data) in enumerate(itertools.izip(cooccurrences.rows, cooccurrences.data)):\n",
    "        if min_count is not None and vocab[id2word[i]][1] < min_count:\n",
    "            continue\n",
    "\n",
    "        for data_idx, j in enumerate(row):\n",
    "            if min_count is not None and vocab[id2word[j]][1] < min_count:\n",
    "                continue\n",
    "\n",
    "            yield i, j, data[data_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#' Create a feature co-occurrence matrix\n",
    "#' \n",
    "#' Create a sparse feature co-occurrence matrix, measuring co-occurrences of\n",
    "#' features within a user-defined context. The context can be defined as a\n",
    "#' document or a window within a collection of documents, with an optional\n",
    "#' vector of weights applied to the co-occurrence counts.\n",
    "#' @param x character, \\link{corpus}, \\link{tokens}, or \\link{dfm} object from\n",
    "#'   which to generate the feature co-occurrence matrix\n",
    "#' @param context the context in which to consider term co-occurrence:\n",
    "#'   \\code{\"document\"} for co-occurrence counts within document; \\code{\"window\"}\n",
    "#'   for co-occurrence within a defined window of words, which requires a\n",
    "#'   positive integer value for \\code{window}.  Note: if \\code{x} is a dfm\n",
    "#'   object, then \\code{context} can only be \\code{\"document\"}.\n",
    "#' @param window positive integer value for the size of a window on either side\n",
    "#'   of the target feature, default is 5, meaning 5 words before and after the\n",
    "#'   target feature\n",
    "#' @param count how to count co-occurrences:\n",
    "#'   \\describe{\n",
    "#'   \\item{\\code{\"frequency\"}}{count the number of co-occurrences within the\n",
    "#'   context}\n",
    "#'   \\item{\\code{\"boolean\"}}{count only the co-occurrence or not within the\n",
    "#'   context, irrespective of how many times it occurs.}\n",
    "#'   \\item{\\code{\"weighted\"}}{count a weighted function of counts, typically as\n",
    "#'   a function of distance from the target feature.  Only makes sense for\n",
    "#'   \\code{context = \"window\"}.}\n",
    "#'   }\n",
    "#' @param weights a vector of weights applied to each distance from\n",
    "#'   \\code{1:window}, strictly decreasing by default; can be a custom-defined\n",
    "#'   vector of the same length as \\code{length(weights)}\n",
    "#' @param ordered if \\code{TRUE} the number of times that a term appears before\n",
    "#'   or after the target feature are counted separately. Only makes sense for\n",
    "#'   context = \"window\".\n",
    "#' @param span_sentence if \\code{FALSE}, then word windows will not span\n",
    "#'   sentences\n",
    "#' @param tri if \\code{TRUE} return only upper triangle (including diagonal).\n",
    "#'   Ignored if \\code{ordered = TRUE}\n",
    "#' @param ... not used here\n",
    "#' @author Kenneth Benoit (R), Haiyan Wang (R, C++), Kohei Watanabe (C++)\n",
    "#' @import Matrix\n",
    "#' @export\n",
    "#' @aliases is.fcm\n",
    "#' @details The function \\code{\\link{fcm}} provides a very general\n",
    "#'   implementation of a \"context-feature\" matrix, consisting of a count of\n",
    "#'   feature co-occurrence within a defined context.  This context, following\n",
    "#'   Momtazi et. al. (2010), can be defined as the \\emph{document},\n",
    "#'   \\emph{sentences} within documents, \\emph{syntactic relationships} between\n",
    "#'   features (nouns within a sentence, for instance), or according to a\n",
    "#'   \\emph{window}.  When the context is a window, a weighting function is\n",
    "#'   typically applied that is a function of distance from the target word (see\n",
    "#'   Jurafsky and Martin 2015, Ch. 16) and ordered co-occurrence of the two\n",
    "#'   features is considered (see Church & Hanks 1990).\n",
    "#'   \n",
    "#'   \\link{fcm} provides all of this functionality, returning a \\eqn{V * V}\n",
    "#'   matrix (where \\eqn{V} is the vocabulary size, returned by\n",
    "#'   \\code{\\link{nfeat}}). The \\code{tri = TRUE} option will only return the\n",
    "#'   upper part of the matrix.\n",
    "#'   \n",
    "#'   Unlike some implementations of co-occurrences, \\link{fcm} counts feature\n",
    "#'   co-occurrences with themselves, meaning that the diagonal will not be zero.\n",
    "#'   \n",
    "#'   \\link{fcm} also provides \"boolean\" counting within the context of \"window\",\n",
    "#'   which differs from the counting within \"document\".\n",
    "#'   \n",
    "#'   \\code{is.fcm(x)} returns \\code{TRUE} if and only if its x is an object of \n",
    "#'   type \\link{fcm}.\n",
    "#'   \n",
    "#' @references \n",
    "#'   Momtazi, S., Khudanpur, S., & Klakow, D. (2010). \n",
    "#'   \"\\href{https://www.lsv.uni-saarland.de/fileadmin/publications/SaeedehMomtazi-HLT_NAACL10.pdf}{A\n",
    "#'    comparative study of word co-occurrence for term clustering in language \n",
    "#'   model-based sentence retrieval.}\" \\emph{Human Language Technologies: The \n",
    "#'   2010 Annual Conference of the North American Chapter of the ACL}, Los \n",
    "#'   Angeles, California, June 2010, pp. 325-328.\n",
    "#'   \n",
    "#'   Jurafsky, Daniel and James H. Martin (2018). \"Chapter 6, Vector Semantics.\"\n",
    "#'   from \\emph{Speech and Language Processing}.  Draft of September 23, 2018,\n",
    "#'   from \\url{https://web.stanford.edu/~jurafsky/slp3/}.\n",
    "#'   \n",
    "#'   Church, K. W. & P. Hanks (1990) \n",
    "#'   \"\\href{http://dl.acm.org/citation.cfm?id=89095}{Word association norms,\n",
    "#'   mutual information, and lexicography}\" \\emph{Computational Linguistics},\n",
    "#'   16(1):22–29.\n",
    "#' @examples\n",
    "#' # see http://bit.ly/29b2zOA\n",
    "#' txt <- \"A D A C E A D F E B A C E D\"\n",
    "#' fcm(txt, context = \"window\", window = 2)\n",
    "#' fcm(txt, context = \"window\", count = \"weighted\", window = 3)\n",
    "#' fcm(txt, context = \"window\", count = \"weighted\", window = 3, \n",
    "#'              weights = c(3, 2, 1), ordered = TRUE, tri = FALSE)\n",
    "#' \n",
    "#' # with multiple documents\n",
    "#' txts <- c(\"a a a b b c\", \"a a c e\", \"a c e f g\")\n",
    "#' fcm(txts, context = \"document\", count = \"frequency\")\n",
    "#' fcm(txts, context = \"document\", count = \"boolean\")\n",
    "#' fcm(txts, context = \"window\", window = 2)\n",
    "#' \n",
    "#' \n",
    "#' # from tokens\n",
    "#' txt <- c(\"The quick brown fox jumped over the lazy dog.\",\n",
    "#'          \"The dog jumped and ate the fox.\")\n",
    "#' toks <- tokens(char_tolower(txt), remove_punct = TRUE)\n",
    "#' fcm(toks, context = \"document\")\n",
    "#' fcm(toks, context = \"window\", window = 3)\n",
    "fcm <- function(x, context = c(\"document\", \"window\"), \n",
    "                count = c(\"frequency\", \"boolean\", \"weighted\"),\n",
    "                window = 5L,\n",
    "                weights = 1L,\n",
    "                ordered = FALSE,\n",
    "                span_sentence = TRUE, tri = TRUE, ...) {\n",
    "    UseMethod(\"fcm\")\n",
    "}\n",
    " \n",
    "#' @export\n",
    "fcm.default <- function(x, ...) {\n",
    "    stop(friendly_class_undefined_message(class(x), \"fcm\"))\n",
    "}\n",
    " \n",
    "#' @noRd\n",
    "#' @export\n",
    "fcm.character <- function(x, ...) {\n",
    "    fcm(tokens(x), ...)\n",
    "}\n",
    " \n",
    "#' @noRd\n",
    "#' @export\n",
    "fcm.corpus <- function(x, ...) {\n",
    "    fcm(tokens(x), ...)\n",
    "}\n",
    " \n",
    "#' @noRd\n",
    "#' @import Matrix\n",
    "#' @export\n",
    "fcm.dfm <- function(x, context = c(\"document\", \"window\"), \n",
    "                       count = c(\"frequency\", \"boolean\", \"weighted\"),\n",
    "                       window = 5L,\n",
    "                       weights = 1L,\n",
    "                       ordered = FALSE,\n",
    "                       span_sentence = TRUE, tri = TRUE, ...) {\n",
    "    \n",
    "    context <- match.arg(context)\n",
    "    count <- match.arg(count)\n",
    "    window <- as.integer(window)\n",
    "    x <- as.dfm(x)\n",
    "    margin <- colSums(x)\n",
    "    \n",
    "    if (!nfeat(x)) {\n",
    "        result <- new(\"fcm\", as(make_null_dfm(), \"dgCMatrix\"), count = count,\n",
    "                      context = context, window = window, margin = numeric(),\n",
    "                      weights = weights, tri = tri)\n",
    "        return(result)\n",
    "    }\n",
    "    \n",
    "    if (!span_sentence) \n",
    "        warning(\"spanSentence = FALSE not yet implemented\")\n",
    "    if (context != \"document\") \n",
    "        stop(\"fcm.dfm only works on context = \\\"document\\\"\")\n",
    " \n",
    "    if (count == \"boolean\") {\n",
    "        temp <- x > 1\n",
    "        x <- dfm_weight(x, \"boolean\") \n",
    "    } else if (count == \"frequency\") {\n",
    "        temp <- x\n",
    "        temp@x <- choose(temp@x, 2)\n",
    "    } else {\n",
    "        stop(\"Cannot have weighted counts with context = \\\"document\\\"\")\n",
    "    }\n",
    " \n",
    "    # compute co_occurrence of the diagonal elements\n",
    "    sum_col <- colSums(temp) # apply(temp, MARGIN = 2, sum)\n",
    "    feature <- sum_col >= 1\n",
    "    index_diag <- which(feature)\n",
    "    length_feature <- length(feature)\n",
    "    temp2 <- Matrix::sparseMatrix(i = index_diag,\n",
    "                                  j = index_diag,\n",
    "                                  x = sum_col[feature],\n",
    "                                  dims = c(length_feature , length_feature))\n",
    "    \n",
    "    result <- Matrix::crossprod(x)\n",
    "    diag(result) <- 0\n",
    "    result <- result + temp2\n",
    "    result <- result[rownames(result), colnames(result)]\n",
    "    \n",
    "    # discard the lower diagonal if tri == TRUE\n",
    "    if (tri) result <- Matrix::triu(result)\n",
    " \n",
    "    # create a new feature context matrix\n",
    "    result <- new(\"fcm\", as(result, \"dgCMatrix\"), count = count,\n",
    "                  context = context, window = window, margin = margin,\n",
    "                  weights = weights, tri = tri)\n",
    "    # set the names \n",
    "    names(result@Dimnames) <- c(\"features\", \"features\")\n",
    "    result\n",
    "}\n",
    " \n",
    "    \n",
    "#' @noRd\n",
    "#' @import data.table\n",
    "#' @import Matrix\n",
    "#' @export\n",
    "fcm.tokens <- function(x, context = c(\"document\", \"window\"), \n",
    "                       count = c(\"frequency\", \"boolean\", \"weighted\"),\n",
    "                       window = 5L,\n",
    "                       weights = 1L,\n",
    "                       ordered = FALSE,\n",
    "                       span_sentence = TRUE, tri = TRUE, ...) {\n",
    "    context <- match.arg(context)\n",
    "    count <- match.arg(count)\n",
    "    window <- as.integer(window)\n",
    "    # TODO could add a warning if not roundly coerced to integer\n",
    "    \n",
    "    if (ordered) tri <- FALSE\n",
    "    if (!span_sentence) \n",
    "        warning(\"spanSentence = FALSE not yet implemented\")\n",
    "    \n",
    "    if (context == \"document\")\n",
    "        result <- fcm(dfm(x, tolower = FALSE, verbose = FALSE), count = count, tri = tri)\n",
    "        \n",
    "    if (context == \"window\") { \n",
    "        if (any(window < 1L)) stop(\"The window size is too small.\")\n",
    "        if (count == \"weighted\") {\n",
    "            if (!missing(weights) && length(weights) != window) {\n",
    "                warning (\"weights length is not equal to the window size, weights are assigned by default!\")\n",
    "                weights <- 1\n",
    "            }\n",
    "        }\n",
    "        if (!is.tokens(x)) x <- as.tokens(x)\n",
    "        types <- types(x)\n",
    "        n <- sum(lengths(x)) * window * 2\n",
    "        result <- as(qatd_cpp_fcm(x, length(types), count, window, \n",
    "                                  weights, ordered, tri, n), \"dgCMatrix\")\n",
    "        dimnames(result) <- list(features = types, features = types)\n",
    "    }\n",
    " \n",
    "    # discard the lower diagonal if tri == TRUE\n",
    "    if (tri) result <- Matrix::triu(result)\n",
    "    \n",
    "    # create a new feature context matrix\n",
    "    result <- new(\"fcm\", as(result, \"dgCMatrix\"), count = count,\n",
    "                  context = context, window = window, margin = colSums(dfm(x)),\n",
    "                  weights = weights, tri = tri)\n",
    "    # set the names \n",
    "    names(result@Dimnames) <- c(\"features\", \"features\")\n",
    "    result\n",
    "}     \n",
    " \n",
    "#' @rdname print.dfm\n",
    "#' @export\n",
    "setMethod(\"print\", signature(x = \"fcm\"), \n",
    "          function(x, show.values = NULL, show.settings = FALSE, \n",
    "                   show.summary = TRUE, \n",
    "                   ndoc = quanteda_options(\"print_dfm_max_ndoc\"), \n",
    "                   nfeature = quanteda_options(\"print_dfm_max_nfeat\"), ...) {\n",
    "              if (show.summary) {\n",
    "                  cat(\"Feature co-occurrence matrix of: \",\n",
    "                      format(ndoc(x), big.mark = \",\"), \" by \",\n",
    "                      format(nfeat(x), big.mark = \",\"), \" feature\",\n",
    "                      if (nfeat(x) != 1L) \"s\" else \"\",\n",
    "                      if (is.resampled(x)) paste(\", \", nresample(x), \" resamples\", sep = \"\") else \"\",\n",
    "                      \".\\n\", sep = \"\")\n",
    "              }\n",
    "              print_dfm(x, ndoc, nfeature, show.values, show.settings, ...)\n",
    "          })\n",
    " \n",
    "#' @rdname print.dfm\n",
    "#' @export\n",
    "setMethod(\"show\", signature(object = \"fcm\"), function(object) print(object))\n",
    " \n",
    "#' @rdname print.dfm\n",
    "#' @method head fcm\n",
    "#' @export\n",
    "head.fcm <- function(x, n = 6L, nfeature = 6L, ...) {\n",
    "    head.dfm(x, n, nfeature, ...)\n",
    "}\n",
    " \n",
    "#' @rdname print.dfm\n",
    "#' @method tail fcm\n",
    "#' @export\n",
    "tail.fcm <- function(x, n = 6L, nfeature = 6L, ...) {\n",
    "    head.dfm(x, n, nfeature, ...)\n",
    "}\n",
    " \n",
    "#' @noRd\n",
    "#' @rdname fcm-class\n",
    "#' @export\n",
    "is.fcm <- function(x) {\n",
    "    is(x, \"fcm\")\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abydos\n",
    "from abydos.distance import NeedlemanWunsch, needleman_wunsch\n",
    "from abydos.distance import SmithWaterman, smith_waterman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sim_nw = lambda x, y: 2 * int(x is y) - 1\n",
    "\n",
    "smith_waterman('ROGER'*20, 'ROGER'*20, gap_cost=0, sim_func=_sim_nw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sim_matrix() got an unexpected keyword argument 'gap_cost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b33d28ceb0a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNeedlemanWunsch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sim_matrix() got an unexpected keyword argument 'gap_cost'"
     ]
    }
   ],
   "source": [
    "NeedlemanWunsch.sim_matrix('cat', 'cat', gap_cost=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 655.4,
   "position": {
    "height": "886px",
    "left": "1049px",
    "right": "20px",
    "top": "110px",
    "width": "654px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
